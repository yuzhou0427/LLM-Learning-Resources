# 公开学习链接

## 目录

<br>

## 1. modelscope：
汇聚各领域最先进的机器学习模型，提供模型探索体验、推理、训练、部署和应用的一站式服务。发现、学习、定制和分享心仪的模型：
（常见的LLM、TTS、CV、多模态模型均可以查到）
- https://modelscope.cn/home

<br>

## 2. 多模态：

BLIP-2、InstructBLIP稳居前三！十二大模型，十六份榜单，全面测评「多模态大语言模型」：
- https://cloud.tencent.com/developer/article/2308689


BLIP2：下一代多模态模型的雏形：
- https://zhuanlan.zhihu.com/p/606364639


LLaVA（Large Language and Vision Assistant）大模型：
- https://zhuanlan.zhihu.com/p/624928279


幻觉降低30%！首个多模态大模型幻觉修正工作Woodpecker：
- https://mp.weixin.qq.com/s/f1s-VzPU2F8I5O--GNc2pg


<br>

## 3. LLM大模型知识点：

How to Train Really Large Models on Many GPUs?
- https://lilianweng.github.io/posts/2021-09-25-train-large/
- 中文地址：https://blog.csdn.net/qq_32275289/article/details/124377578


大模型训练和推理：
- https://qiankunli.github.io/2022/02/10/large_model.html#%E5%88%86%E5%B8%83%E5%BC%8F%E6%8E%A8%E7%90%86
> 补充：作者有一系列关于大模型的博客，可以在左边目录栏查看，非常有价值。


大模型训练为什么用A100不用4090：
- https://mp.weixin.qq.com/s/nHCznUDOpXk3G4zfhisf9w
> 结论：大模型的训练用 4090 是不行的，但推理（inference/serving）用 4090 不仅可行，在性价比上还能跟 H100 打个平手。


一图看懂ONNX模型格式：
- https://zhuanlan.zhihu.com/p/425232454


ONNX是什么?怎么用?[简明解读版]
- https://blog.csdn.net/u010087338/article/details/126208513


如何让chatGPT自动优化提示词prompt?一条简单指令轻松搞定：
- https://mp.weixin.qq.com/s/UMLtpF2myDEJYn9AYW-4XQ


使用GPT3.5,LangChain，FAISS和python构建一个本地知识库：
- https://juejin.cn/post/7272185503821430843


大模型推理妙招—投机采样（Speculative Decoding）：
- https://zhuanlan.zhihu.com/p/651359908


LLM（大语言模型）解码时是怎么生成文本的？
- https://zhuanlan.zhihu.com/p/624845975


Transformer改进之相对位置编码(RPE)：
- https://zhuanlan.zhihu.com/p/105001610


roformer：带有旋转位置嵌入的增强型transformer
- https://zhuanlan.zhihu.com/p/574478161


通俗解释困惑度 (Perplexity)-评价语言模型的好坏：
- https://zhuanlan.zhihu.com/p/44107044


详解Transformer （Attention Is All You Need）：
- https://zhuanlan.zhihu.com/p/48508221


Rethink深度学习中的Attention机制：
- https://zhuanlan.zhihu.com/p/125145283


浅谈Transformer的初始化、参数化与标准化：
- https://zhuanlan.zhihu.com/p/400925524


LLM：大模型的正则化：
- https://blog.csdn.net/pipisorry/article/details/130958856


为什么Pre Norm的效果不如Post Norm？
- https://kexue.fm/archives/9009


昇腾大模型|结构组件-1——Layer Norm、RMS Norm、Deep Norm：
- https://zhuanlan.zhihu.com/p/620297938


大模型参数高效微调(PEFT)：
- https://zhuanlan.zhihu.com/p/621700272


Prompt-Tuning——深度解读一种新的微调范式：
- https://blog.csdn.net/qq_36426650/article/details/120607050


大模型的幻觉问题调研: LLM Hallucination Survey：
- https://zhuanlan.zhihu.com/p/642648601


最简单的计算模型(LLM)FLOPs的方法：
- https://zhuanlan.zhihu.com/p/652697200


训练模型算力的单位：FLOPs、FLOPS、Macs 与 估算模型（FC, CNN, LSTM, Transformers&&LLM）的FLOPs：
- https://zhuanlan.zhihu.com/p/649993943


深入理解NLP Subword算法：BPE、WordPiece、ULM：
- https://zhuanlan.zhihu.com/p/86965595


Muti Query Attention 和 Attention with Linear Bias（附源码）：
- https://zhuanlan.zhihu.com/p/634236135


深入理解“预训练”语言模型：
- https://zhuanlan.zhihu.com/p/159620066


Prompt方法综述：
- https://zhuanlan.zhihu.com/p/431788068


谷歌 Soft Prompt Learning：
- https://zhuanlan.zhihu.com/p/514125505


<br>

## 4. GPT系列：

GPT / GPT-2 / GPT-3 / InstructGPT 进化之路：
- https://zhuanlan.zhihu.com/p/609716668


【ChatGPT系列】大模型的涌现能力：
- https://zhuanlan.zhihu.com/p/596612842


ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？
- https://www.zhihu.com/question/596230048


RLHF，对齐了，又没完全对齐？
- https://mp.weixin.qq.com/s/0Cy5o8WKyJuqWHv3gfKDNw


InstructGPT论文解读：
- https://zhuanlan.zhihu.com/p/613688211


<br>

## 5. 知名LLM大模型：

大语言模型（LLM）综述与实用指南（Amazon，2023）：
- https://arthurchiao.art/blog/llm-practical-guide-zh/


LLaMA：开放和高效的基础语言模型：
- https://zhuanlan.zhihu.com/p/609843048


LLaMA2：【LLM系列之LLaMA2】LLaMA 2技术细节详细介绍！
- https://zhuanlan.zhihu.com/p/644440986


OPT：不要光知道Meta复刻GPT-3“背刺”OpenAI，来看看如何“背刺”：
- https://hub.baai.ac.cn/view/16851


FLAN：【LLM系列-04】Finetuned Language Models Are Zero-Shot Learners：
- https://zhuanlan.zhihu.com/p/607657048


Flan-T5: One Model for ALL Tasks：
- https://zhuanlan.zhihu.com/p/580468546


PaLM：基于Pathways的大语言模型：
- https://blog.csdn.net/bqw18744018044/article/details/128809221


T5：NLP Text-to-Text 预训练模型超大规模探索：
- https://zhuanlan.zhihu.com/p/88438851


Alpaca：自驱力超强的羊驼？斯坦福微调LLaMa：
- https://zhuanlan.zhihu.com/p/615279976


ChatGLM：千亿基座的对话模型开启内测：
- https://chatglm.cn/blog


BLOOM：一个176B参数且可开放获取的多语言模型：
- https://zhuanlan.zhihu.com/p/603518061


EcomGPT：指令微调的电商领域大模型
- https://mp.weixin.qq.com/s/pT89cpjrRC7nmChEQmTm6A


<br>

## 6. 对比学习&CV相关：

【重要】【论文复现】SimCSE对比学习: 文本增广是什么牛马，我只需要简单Dropout两下：
- https://blog.csdn.net/weixin_45839693/article/details/116302914
> 补充：讲SimCSE论文，其中提到了sentences embedding中的anisotropy问题，论文验证通过对比学习可以有效改善该问题，并对infoNCE loss给出一个非常好解答；并且通过alignment和uniformity指标来评估质量；


深度学习中的temperature parameter是什么：
- https://zhuanlan.zhihu.com/p/132785733


求通俗易懂解释下nce loss？
- https://www.zhihu.com/question/50043438?sort=created


一文了解NLP中的数据增强方法：
- https://zhuanlan.zhihu.com/p/145521255、https://amitness.com/2020/05/data-augmentation-for-nlp/


序列推荐中的意图对比学习|Intent Contrastive Learning for Sequential Recommendation|www'22：
- https://zhuanlan.zhihu.com/p/527858820


B站MoCo解析：
- https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.999.0.0&vd_source=a72c689033fe302cb8a1c44e0b648cbe


Resnet50详解与实践（基于mindspore)：
- https://zhuanlan.zhihu.com/p/137734361


CV攻城狮入门VIT(vision transformer)之旅——VIT原理详解篇：
- https://juejin.cn/post/7153427278054031391


[论文阅读]DeepViT: Towards Deeper Vision Transformer：
- https://zhuanlan.zhihu.com/p/359601694


<br>

## 7. 多任务、多领域建模：

多任务学习MTL模型：MMoE、PLE：
- https://zhuanlan.zhihu.com/p/425209494


多任务学习之mmoe理论详解与实践：
- https://zhuanlan.zhihu.com/p/597857374


先入为主：将先验知识注入推荐模型（LHUC）：
- https://zhuanlan.zhihu.com/p/442845759


多任务学习MTL模型：多目标Loss优化策略：
- https://zhuanlan.zhihu.com/p/456089764


多任务学习优化（Optimization in Multi-task learning）：
- https://zhuanlan.zhihu.com/p/269492239


论文阅读：Gradient Surgery for Multi-Task Learning：
- https://zhuanlan.zhihu.com/p/258327403


多任务学习——【ICLR 2020】PCGrad：
- https://zhuanlan.zhihu.com/p/395220852


搜广推之多场景学习：
- https://tangshusen.me/2023/07/03/multi-domain/


动态权重：推荐算法的新范式：
- https://zhuanlan.zhihu.com/p/500934745


多目标排序各大公司落地：
- https://zhuanlan.zhihu.com/p/341345727


KDD'22 | 基于显著性正则化的多任务学习：
- https://mp.weixin.qq.com/s/cWVcxayQyJUWi91EzdDhLg


<br>

## 8. 推荐相关：

Deep Cross Network (深度交叉网络, DCN) 介绍与代码分析：
- https://blog.csdn.net/Eric_1993/article/details/105600937


推荐生态中的bias和debias：
- https://zhuanlan.zhihu.com/p/342905546


从Google Visor到Microsoft NNI再到Advisor调参服务接口发展史：
- https://zhuanlan.zhihu.com/p/45883429


FM（Factorization Machines）的理论与实践：
- https://zhuanlan.zhihu.com/p/50426292


<br>

## 9. 重要知识储备：

概率论与数理统计公式：
- https://zhuanlan.zhihu.com/p/32026754


凸函数介绍：
- https://zhuanlan.zhihu.com/p/56876303


拉格朗日算子：
- https://www.cnblogs.com/mo-wang/p/4775548.html
>上文介绍了等式约束条件下拉格朗日算子的求解逻辑，同时也介绍了在不等约束下求解的KKT条件；并求数学逻辑上证明了满足KKT条件下最优解的存在性；


信息熵，KL散度：
- https://zhuanlan.zhihu.com/p/438129018，https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/，https://zhuanlan.zhihu.com/p/45131536
>上文给出了KL散度的基本定义和性质，KL散度又可以称为相对熵 = 交叉熵 - 信息熵，即D（p｜q） = H（p，q） - H（p）；
>第三篇文章给出-log(p(x))一个很好的信息熵解答；同时也说明逻辑回归是最大熵模型的一个特例；


什么是MLE，什么是MAP？
- https://blog.csdn.net/u011508640/article/details/72815981
- https://zhuanlan.zhihu.com/p/26614750
> MLE（maximum likelihood estimation）：对于一组独立同分布的样本【从联合分布转化为多个分布乘积】，在确定模型的情况下， 去预估这组样本整体发生概率最大时，所对应的模型参数；
———— 这种思想，从直观上感觉没问题，但如果以这个目标去设计，如果模型过于复杂很容易出现过拟合现象；


什么是EM算法？
- https://blog.csdn.net/zouxy09/article/details/8537620
> EM（Expectation Maximization），本质是一种在不完整数据或者有丢失数据集（存在隐变量）的，进行最大似然预估的方法；


LR推导及损失函数：
- https://blog.csdn.net/weixin_41725746/article/details/93378662
> 上文中有lr参数的梯度的推导，很实用；
- https://zhuanlan.zhihu.com/p/74874291
> 上文从对数几率的视角解释了LR的建模逻辑；


word2vec：
- https://zhuanlan.zhihu.com/p/56106590
- https://zhuanlan.zhihu.com/p/70208318


L1和L2 详解(范数、损失函数、正则化)：
- https://zhuanlan.zhihu.com/p/137073968


实体关系、实体属性、三元组、SPO三元组及其抽取方案：
- https://zhuanlan.zhihu.com/p/237452918


欠拟合、过拟合及如何防止过拟合：
- https://zhuanlan.zhihu.com/p/72038532


<br>

## 10. 评估相关：

内生性问题及其产生原因：
- https://zhuanlan.zhihu.com/p/110645711


假设检验之——T检验：
- https://www.jianshu.com/p/46d9b111dffc


一文详解t检验：
- https://zhuanlan.zhihu.com/p/138711532


双重差分法（DID）介绍：
- https://zhuanlan.zhihu.com/p/48952513


理解并求解置信区间：
- https://zhuanlan.zhihu.com/p/36206276


快手因果推断与实验设计：
- https://mp.weixin.qq.com/s/svVl1eiVUH6rOYG3p2YiGg


聚类评估指标：
- https://blog.csdn.net/weixin_45488228/article/details/100549820


<br>

## 11. 供应链相关：

TensorFlow 助力网易严选供应链需求预测（系列之四）：
- https://mp.weixin.qq.com/s/2-1taZ5o4uzVcGMe5u4P-A


Safety stock：
- https://en.wikipedia.org/wiki/Safety_stock?spm=ata.13261165.0.0.602572d2HYbJ36#cite_note-9


一般正态曲线函数的积分怎么求？为什么总是1？
- https://blog.csdn.net/Ocean_waver/article/details/105505451


误差函数：
- https://zh.wikipedia.org/wiki/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0



知识图谱：

<br>

## 12. 美团商品知识图谱的构建及应用：
- https://mp.weixin.qq.com/s/ZSn10RAxPKBQBhP6iTmlxQ


本地生活综合性需求图谱的构建及应用：
- https://mp.weixin.qq.com/s/wKZJ3toGlDQM5PKvNj7I7w


<br>

## 13. 其他：

面试智力/概率题梳理：
- https://www.nowcoder.com/discuss/374239272688254976


Linux 6种日志查看方法，不会看日志会被鄙视的：
- https://cloud.tencent.com/developer/article/1579977


pandas 教程：
- https://www.gairuo.com/p/pandas-tutorial


keras 教程：
- https://keras-zh.readthedocs.io/


gensim教程：
- https://radimrehurek.com/gensim/intro.html


from tensorflow.python.eager.context import get_config报错：
- https://blog.csdn.net/Carl_changxin/article/details/117427911


史上最全跨境电商选品参考网站：
- https://zhuanlan.zhihu.com/p/118539860


选品十三讲（全文）：
- https://mp.weixin.qq.com/s/9G3806JnP52gNSLxNGX_3g


电商卖家工具：
- https://www.xiaohus.com/seller-tool


艾媒智库，数据中心：
- https://data.iimedia.cn/page-category.jsp?nodeid=44277903


<br>

## 14. 报告名称：


Gartner｜How to Choose an Approach for Deploying Generative AI　如何选择生成式AI的部署方法

Gartner | Hype Cycle for Artificial Intelligence, 2023 人工智能2023年Gartner技术成熟度曲线

Gartner | Hype Cycle for Data Science and Machine Learning, 2023 2023年数据科学和机器学习的Gartner技术成熟度曲线

Gartner | Hype Cycle for Emerging Technologies, 2023 2023年新兴技术的Gartner技术成熟度曲线

Gartner | AI Design Patterns for Generative AI and Augmented Analytics and BI 用于生成式AI和增强分析以及商业智能的AI设计模式

Gartner | AI Design Patterns for Knowledge Graphs and Generative AI——知识图谱和生成式AI的设计模式

Gartner | AI Design Patterns for Large Language Models 大模型相关的AI设计模式

Gartner | Hype Cycle for Generative AI, 2023 生成式AI的2023年技术成熟度曲线