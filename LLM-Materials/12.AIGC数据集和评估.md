# AIGC数据集和评估

## 目录

- [1 数据集](#1-数据集)
  - [1.1 开源数据集](#11-开源数据集)
    - [1.1.1 文本预训练数据集](#111-文本预训练数据集)
    - [1.1.2 中文指令微调数据集](#112-中文指令微调数据集)
  - [1.2 数据处理](#12-数据处理)
    - [1.2.1 数据清洗](#121-数据清洗)
    - [1.2.2 数据标注](#122-数据标注)
    - [1.2.3 数据扩充](#123-数据扩充)
- [2 评估](#2-评估)
  - [2.1 评价方式](#21-评价方式)
- [参考链接](#参考链接)

<br>


## 1 数据集

### 1.1 开源数据集

大语言模型数据集主要包含两大类，预训练数据和指令微调数据。

数据分类：

![llm-data.jpg](..%2Fassets%2Fassets12%2Fllm-data.jpg)


#### 1.1.1 文本预训练数据集

**常用数据集:**

| 数据集             |规模| 特点                                                                   | 数据类型                                                                                                          | 数据组成   |
|-----------------|--|----------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|--------|
| Wikipedia       |21 GB| 多语言高质量百科全书                                                           | 无监督文本                                                                                                         |        |
| BooksCorpus     |book1: 2.2GB; book3: 37GB| 英文小说集                                                                | 无监督文本                                                                                                         |  |
| Common Crawl    |超过 PB| 网页数据，规模巨大                                                            | 无监督文本                                                                                                         |  |
| ROOT            |1.6TB| 包含 46种自然语言和13种编程语言                                                   | 无监督文本和代码                                                                                                      | ![root-data.png](..%2Fassets%2Fassets12%2Froot-data.png) |
| The Pile        |825GB| 数据来源广泛， 多样性佳。由 22 个不同的高质量子集构成。                                       | 无监督文本、代码（Github7.59%）、问答（Stack Exchange5.13%）、数学推理（DM Mathematics1.24%）、对话（Ubuntu IRC0.88%）、翻译（EuroParl0.73%） | ![pile-data.png](..%2Fassets%2Fassets12%2Fpile-data.png) |
| 悟道              |3TB| 中文数据集:采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，包含教育、科技等50+个行业数据标签。 | 文本、对话、图文对、视频文本对                                                                                               | 数据占比未知 |
| CLUECorpus 2020 |100GB| 中文数据集:通过对Common Crawl的中文部分进行语料清洗，最终得到的高质量中文预训练语料                     | 无监督文本                                                                                                         |  |
| CodeSearchNet 2019           |17GB| 大型代码数据集，包含来自 GitHub 上的开源项目的用 Go、Java、JavaScript、PHP、Python 和 Ruby 编写的相关文档。 | 代码                                                                                                            | ![codesearchnet-2019.png](..%2Fassets%2Fassets12%2Fcodesearchnet-2019.png) |


**样本任务类型:**

以下是常见的任务类型，以及对应经典数据集:

![mission-dataset.png](..%2Fassets%2Fassets12%2Fmission-dataset.png)


#### 1.1.2 中文指令微调数据集


| 机构              | 数据集                                                                       | 规模 | 数据类型 | 数据特点 | case                                                                     |
|-----------------|---------------------------------------------------------------------------|----|------|------|--------------------------------------------------------------------------|
| 斯坦福Alpaca       | [中文指令数据](https://github.com/carbonz0/alpaca-chinese-dataset)              |52k|种子数据包含：问答、摘要、数学推理、代码生成等|原英文版数据用175 个人工编写的任务种子集合作为初始化指令样例，用text-davinci-003生成。| [alpaca-chinese-dataset.json](prompt-demo%2Falpaca-chinese-dataset.json) |
|                 | [Alpaca_GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) |52k|同上|用 GPT-4 生成的，并做了中文翻译| 格式同上                                                                     |
| [BELLE Group](https://huggingface.co/datasets?sort=downloads&search=BELLE%20Group) | School Math                                                               |25万|数学推理|中文数学题数据，包含解题过程| [belle-match-instruction-demo.json](prompt-demo%2Fbelle-match-instruction-demo.json) |
|                 | Multiturn Chat                                                            |80万|对话|用户与助手的多轮对话| [belle-chat-instruction-demo.json](prompt-demo%2Fbelle-chat-instruction-demo.json) |
|                 | Generated Chat                                                            |40万|对话|给定角色的多轮对话| [belle-general-chat-instruction-demo.json](prompt-demo%2Fbelle-general-chat-instruction-demo.json)   |
|                 | train_2M_CN                                                               |200万|同Alpaca|与Alpaca类似生成的多样化指令任务数据| [belle-2m-cn-demo.json](prompt-demo%2Fbelle-2m-cn-demo.json) |
| 智源	             | [COIG](https://huggingface.co/datasets/BAAI/COIG)                                                                  | 191k|翻译、推理、问答、对话、代码生成|包括翻译指令（67798条）、考试指令（63532条）、人类价值观对齐指令（34471条）、反事实修正多轮聊天（13653条）、Leetcode指令（11737条），经过了人工修正，可以商用|人类价值观case|


其他中文指令微调数据：

- [awesome-open-instruct-data-for-chinese](https://github.com/LianjiaTech/BELLE/blob/main/data/awesome_open_instruct_data_for_chinese.md)
- [Awesome Pretrained Chinese NLP Models-中文指令数据集](https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models#%E4%B8%AD%E6%96%87%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E9%9B%86)

注：最近研究表明，代码语料可以提升模型复杂推理的能力（chain-of-thought），因其具有的长距离依赖以及内在精密的逻辑；


### 1.2 数据处理

# 1.2.1 数据清洗

![data-clean.png](..%2Fassets%2Fassets12%2Fdata-clean.png)

|步骤| 方法                                                |
|--|---------------------------------------------------|
|质量过滤| 1）基于高质量的文本训练一个分类器。<br>2）基于启发式的方法，通过设计规则来消除低质量的文本。 |
|去重| 在句子级、文档级和数据集级，利用词重叠率等方法去重。                        |
|隐私重构| 采用基于规则的方法（如关键词）去除个人信息等。                           |
|分词| 使用已有的分词模型或者使用SentencePiece为语料库专门训练等。              |


**具体而言：**

```text
- 1）在第一步的语料清洗中，可以利用Wikipedia等样本作为正例训练一个二分类器筛选高质量语料。不过最近的研究表明，这一筛选方式可能带来偏见。所以现在更推荐使用启发式规则来筛选，比如剔除非目标任务语言、丢弃低perplexity数据、删去标点/符号过多或过长过短的句子、删除具有某些特定词汇（如html标签、链接、脏话、敏感词）的句子；

- 2）第二步是去重。包含大量重复词汇或短语的句子可以删掉；重复率（词/n-grams共现）过高的段落可以删掉；删除训练集中可能与测试集相关度过高的内容。这样可以提高训练集质量，缓解语言模型生成内容重复的问题，避免测试集泄露带来的过拟合问题；

- 3）第三步是通过关键词等方式剔除用户隐私信息（姓名、地址、电话等）；

- 4）最后，三步清洗完毕，就可以上分词、准备训练了。分词方面，并没有什么黑科技。要么直接使用GPT-2等现成的分词器，要么对训练语料构建基于SentencePiece、Byte Pair Encoding等算法的分词方式。
```

**一些细节点：**
大模型的特点，导致在处理预训练语料时，需要注意一些特殊的细节：
- **需要调节不同来源的语料的混合比例**：不能直接基于语料规模。均衡的语料比例有助于提高模型的泛化能力，特定类型的语料可以提升模型特定的能力；
- **语料规模要与模型的参数规模相配合**：经验表明，给定算力，语料的token数与模型的参数个数相当时，模型的表现相对更好。所以不要一味地追求大语料，控制规模、提高质量、训练充分，也很重要；
- **语料质量很重要（再次强调）**：实验表明，大模型训练时，低质量的语料不用都比用了好。过多的重复数据甚至会让训练过程失效（崩溃或陷入无意义的局部最优）；


### 1.2.2 数据标注

|标注用途|具体工作|数据后处理| 人员筛选           | 标准指标规范      |
|--|--|--|----------------|-------------|
|有监督微调 SFT（supervised fine-tuning）|对样本中的 Prompt 编写答案|数据梳理、隐私| 性别、种族、敏感言论一致性等 | 有帮助、真实性、无害性 |
|RM（Reward Model）|对模型的多个输出进行打分或排序|数据更新、转换|                |             |


### 1.2.3 数据扩充

从语言模型中生成指令、输入和输出样本。

|步骤| 方法                                                        |
|--|-----------------------------------------------------------|
|指令生成| 引导的方式，基于已有的种子数据，生成一个多样化的指令集；                              |
|识别任务类型| 要用两种不同的方法来处理分类和非分类任务，需要进行识别；                              |
|生成实例| 给定指令和任务类型，对应使用输入或输出优先的方式生成实例；                             |
|过滤数据| 当一条新指令与现有指令的ROUGE-L重叠度小于0.7时，才会被添加到任务池中。同时用关键词过来掉图像类似的数据。 |


![general-instruction.png](..%2Fassets%2Fassets12%2Fgeneral-instruction.png)


<br>

## 2 评估

### 2.1 评价方式

| 评价方式 |方法| 计算方法和特点                                                                                                            | 图示 |
|------|--|--------------------------------------------------------------------------------------------------------------------|---|
| 人工评价 |打分、相对排序| 人工根据回复的相关性等指标进行打分或排序 <br> <li> 优点：更接近于实际应用中的用户体验；<br> <li> 缺点：耗时耗力。                                                |   |
| 基于统计 |BLEU、ROUGE| BLEU是计算两个句子的n-gram重合度，主要用于翻译、摘要等任务。<br> BLEU计算公式：<br> https://blog.csdn.net/qq_30232405/article/details/104219396 <br> <br> <li> 缺优点：方便、快速。<br> <li> 缺点：不考虑语法、同义词等，在创造性和多样性方面与人类判断的相关性相对较低。 |  |
|      |精确率、 召回率和 F1| 主要用于分类、推理等任务                                                                                                                 | -- |
| 基于模型   |BERTScore| 1. 对生成句和参考句分别用bert提取单词在上下文的特征向量（同一个单词在不同句子的向量不同）；<br>2. 对2个句子的每一个词分别计算内积，得到相似性矩阵；<br>3. 基于矩阵计算最高分（贪心匹配）的加权累加（使用idf），然后归一化得到bertscore的precision，recall和F1；<br><br> <li> 优点：解决了匹配和依赖关系的问题；缺乏训练数据也具有较好的表现。 | ![bertscore.png](..%2Fassets%2Fassets12%2Fbertscore.png) |
|    |GPTScore| 1. 给出任务描述（如生成摘要）；<br> 2. 选择几个要评估的方面（如相关性、流畅性），并给出定义；<br> 3. 将指令、评估方面、例子和待评估的文本输入到GPT-3得到分数；<br> <br> <li> 优点：可定制性、多角度评估、免训练。                                                                                                                 |  |
|    |G-Eval| 1. 将任务描述和评价标准输入到LLM，生成CoT的评估步骤；<br> 2. 将上下文和目标文本，以及生成的CoT输入到GPT-4，生成分数的概率（解决只输出整数分数和低方差问题）；<br> 计算概率加权分数作为最终分数；<br><br><li>优点：与人工的Spearman 相关性较高； | ![g-eval.png](..%2Fassets%2Fassets12%2Fg-eval.png) |



## 参考链接

[BERTScore](https://arxiv.org/abs/1904.09675v3)

[文本生成评价指标](https://zhuanlan.zhihu.com/p/144182853)

[GPTScore: Evaluate as You Desire](https://arxiv.org/abs/2302.04166)

[G-Eval](https://arxiv.org/abs/2303.16634)

[ChatGPT 调研报告](https://aimg8.dlssyht.cn/u/551001/ueditor/file/276/551001/1678415058523571.pdf)

[大语言模型（LLM）的进化树，学习LLM看明白这一张图就够了](https://zhuanlan.zhihu.com/p/627491455)

[ChatGPT数据集之谜](https://mp.weixin.qq.com/s/A0qeI3rX4JGpWiV9Syrmfw)

[30个大语言模型训练相关的数据集分享](https://mp.weixin.qq.com/s/W6Kt3-9FMRtoAZr0EKRPvQ)

[指令微调数据集](https://blog.csdn.net/dzysunshine/article/details/130870398)

[大模型微调项目 / 数据集调研汇总](https://www.cvmart.net/community/detail/7558)

[The Pile数据集概述](https://www.ctfiot.com/100952.html)

[COIG：首个大规模、可商用的中文开源指令数据](https://hub.baai.ac.cn/view/25750)

[详谈大模型训练中的数据收集、处理与模型影响](https://mp.weixin.qq.com/s/bHsb631KA5AaulBHNT5m9w)

[A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)

[BLOOM](https://zhuanlan.zhihu.com/p/603518061)

[Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)

[ChatGPT 标注指南：任务、数据与规范](https://yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/)

[SELF-INSTRUCT指令自动化生成框架](https://mp.weixin.qq.com/s/Lo1f1knFFQWdHNLTNyFKDQ)
	



	
				
			