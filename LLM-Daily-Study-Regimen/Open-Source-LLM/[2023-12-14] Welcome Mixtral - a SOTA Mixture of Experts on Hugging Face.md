# Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face


åœ°å€ï¼šhttps://huggingface.co/blog/mixtral

githubï¼šhttps://github.com/huggingface/blog/blob/main/mixtral.md


<br>

&nbsp;&nbsp;&nbsp;&nbsp;Mixtral 8x7bæ˜¯Mistralä»Šå¤©å‘å¸ƒçš„ä¸€æ¬¾ä»¤äººå…´å¥‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨å¼€æ”¾è·å–æ¨¡å‹æ–¹é¢å–å¾—äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­èƒœè¿‡äº†GPT-3.5ã€‚æˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿé€šè¿‡åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­å…¨é¢é›†æˆMixtralæ¥æ”¯æŒè¿™ä¸€å‘å¸ƒã€‚


<br>

## What is Mixtral 8x7b?

&nbsp;&nbsp;&nbsp;&nbsp;Mixtralä¸Mistral 7Bæ‹¥æœ‰ç›¸ä¼¼çš„æ¶æ„ï¼Œä½†æœ‰ä¸€ä¸ªç‹¬ç‰¹ä¹‹å¤„ï¼šå®ƒå®é™…ä¸Šæ˜¯8ä¸ªâ€œä¸“å®¶â€æ¨¡å‹çš„ç»„åˆï¼Œè¿™å¾—ç›Šäºä¸€ç§ç§°ä¸ºâ€œä¸“å®¶æ··åˆâ€ï¼ˆMixture of Expertsï¼ŒMoEï¼‰çš„æŠ€æœ¯ã€‚å¯¹äºTransformeræ¨¡å‹ï¼Œè¿™ç§æŠ€æœ¯çš„å·¥ä½œåŸç†æ˜¯é€šè¿‡ç”¨ç¨€ç–çš„MoEå±‚æ›¿æ¢ä¸€äº›å‰é¦ˆå±‚ã€‚MoEå±‚åŒ…å«ä¸€ä¸ªè·¯ç”±ç½‘ç»œï¼Œé€‰æ‹©å“ªäº›ä¸“å®¶æœ€æœ‰æ•ˆåœ°å¤„ç†å“ªäº›æ ‡è®°ã€‚åœ¨Mixtralçš„æƒ…å†µä¸‹ï¼Œä¸ºæ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©äº†ä¸¤ä¸ªä¸“å®¶ï¼Œè¿™ä½¿å¾—è¯¥æ¨¡å‹èƒ½å¤Ÿä»¥12Bå‚æ•°å¯†é›†å‹æ¨¡å‹çš„é€Ÿåº¦è§£ç ï¼Œå°½ç®¡å…¶åŒ…å«çš„æœ‰æ•ˆå‚æ•°æ•°é‡æ˜¯å…¶4å€ï¼

> æœ‰å…³MoEçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ç›¸å…³åšå®¢æ–‡ç« ï¼šhttps://huggingface.co/blog/moe


**Mixtralå‘å¸ƒç®€è¦æ‘˜è¦ï¼š**

- å‘å¸ƒåŸºç¡€ç‰ˆå’ŒInstructç‰ˆæœ¬
- æ”¯æŒ32,000æ ‡è®°çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚
- åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­èƒœè¿‡Llama 2 70Bï¼Œä¸GPT3.5ç›¸åŒ¹æ•Œæˆ–è¶…è¶Š
- æ”¯æŒè‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ„å¤§åˆ©è¯­ã€‚
- åœ¨ç¼–ç æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œåœ¨HumanEvalä¸Šè¾¾åˆ°40.2%
- å…·æœ‰å•†ä¸šå®½æ¾çš„Apache 2.0è®¸å¯è¯


&nbsp;&nbsp;&nbsp;&nbsp;Mixtralæ¨¡å‹æœ‰å¤šå¥½å‘¢ï¼Ÿä»¥ä¸‹æ˜¯base modelåœ¨[LLMæ’è¡Œæ¦œ](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ä¸Šä¸å…¶ä»–å¼€æ”¾æ¨¡å‹ç›¸æ¯”çš„æ€§èƒ½æ¦‚è¿°ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼‰ï¼š

![Mixtral-8x7b-basemodel-rank.png](..%2Fassets%2FMixtral-8x7b-basemodel-rank.png)


<br>

&nbsp;&nbsp;&nbsp;&nbsp;å¯¹äºInstructå’ŒChatæ¨¡å‹ï¼Œè¯„ä¼°MT-Benchæˆ–AlpacaEvalç­‰åŸºå‡†æ›´ä¸ºåˆé€‚ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å±•ç¤ºäº†[Mixtral Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)åœ¨ä¸é¡¶çº§é—­æºå’Œå¼€æºæ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒä¸­çš„è¡¨ç°ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼‰ï¼š

| Model                                                                                               | Availability    | Context window (tokens) | MT-Bench score â¬‡ï¸ |
| --------------------------------------------------------------------------------------------------- | --------------- | ----------------------- | ---------------- |
| [GPT-4 Turbo](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)        | Proprietary     | 128k                    | 9.32             |
| [GPT-3.5-turbo-0613](https://platform.openai.com/docs/models/gpt-3-5)                               | Proprietary     | 16k                     | 8.32             |
| [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) | Apache 2.0      | 32k                     | 8.30             |
| [Claude 2.1](https://www.anthropic.com/index/claude-2-1)                                            | Proprietary     | 200k                    | 8.18             |
| [openchat/openchat_3.5](https://huggingface.co/openchat/openchat_3.5)                               | Apache 2.0      | 8k                      | 7.81             |
| [HuggingFaceH4/zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)                 | MIT             | 8k                      | 7.34             |
| [meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)             | Llama 2 license | 4k                      | 6.86             |


ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼ŒMixtral Instructåœ¨MT-Benchä¸Šèƒœè¿‡æ‰€æœ‰å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶æˆä¸ºç¬¬ä¸€ä¸ªä¸GPT-3.5æ€§èƒ½ç›¸åª²ç¾çš„æ¨¡å‹ï¼


### About the name

&nbsp;&nbsp;&nbsp;&nbsp;Mixtral MoEè¢«ç§°ä¸º Mixtral-8x7Bï¼Œä½†å®ƒå¹¶æ²¡æœ‰56Bçš„å‚æ•°ã€‚åœ¨å‘å¸ƒåä¸ä¹…ï¼Œæˆ‘ä»¬å‘ç°ä¸€äº›äººè¢«è¯¯å¯¼ä»¥ä¸ºè¿™ä¸ªæ¨¡å‹çš„è¡Œä¸ºç±»ä¼¼äºåŒ…å«8ä¸ªæ¯ä¸ªæœ‰7Bå‚æ•°çš„æ¨¡å‹çš„é›†æˆæ¨¡å‹ï¼Œä½†MoEæ¨¡å‹çš„å·¥ä½œæ–¹å¼å¹¶éå¦‚æ­¤ã€‚æ¨¡å‹çš„åªæœ‰ä¸€äº›å±‚ï¼ˆå‰é¦ˆå—ï¼‰è¢«å¤åˆ¶ï¼›å…¶ä½™çš„å‚æ•°ä¸7Bæ¨¡å‹ç›¸åŒã€‚æ€»å‚æ•°æ•°ä¸æ˜¯56Bï¼Œè€Œæ˜¯çº¦45Bã€‚ä¸€ä¸ªæ›´å¥½çš„åç§°å¯èƒ½æ˜¯ [Mixtral-45-8e](https://twitter.com/osanseviero/status/1734248798749159874)ï¼Œä»¥æ›´å¥½åœ°ä¼ è¾¾æ¶æ„ã€‚æœ‰å…³MoEå¦‚ä½•å·¥ä½œçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„â€œ[æ·±å…¥è§£æä¸“å®¶æ··åˆ](https://huggingface.co/blog/moe)â€æ–‡ç« ã€‚



### Prompt format

&nbsp;&nbsp;&nbsp;&nbsp;The [base model](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) has no prompt format. Like other base models, it can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. Itâ€™s also a great foundation for fine-tuning your own use case. The [Instruct model](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) has a very simple conversation structure.

```bash
<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2[/INST]
```

è¯¥æ ¼å¼å¿…é¡»å®Œå…¨å¤åˆ¶ä»¥å®ç°æœ‰æ•ˆä½¿ç”¨ã€‚ç¨åæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ `transformers` ä¸­æä¾›çš„èŠå¤©æ¨¡æ¿è½»æ¾å¤åˆ¶ instruct æç¤ºã€‚


## Demo

&nbsp;&nbsp;&nbsp;&nbsp;æ‚¨å¯ä»¥åœ¨ Hugging Face Chat ä¸ Mixtral Instruct æ¨¡å‹èŠå¤©ï¼åœ¨è¿™é‡ŒæŸ¥çœ‹: https://huggingface.co/chat/?model=mistralai/Mixtral-8x7B-Instruct-v0.1.


## Inference

æˆ‘ä»¬æä¾›äº†ä¸¤ç§ä¸»è¦æ–¹å¼æ¥è¿è¡Œ Mixtral æ¨¡å‹çš„æ¨ç†ï¼š

- é€šè¿‡ ğŸ¤— Transformers çš„ pipeline() å‡½æ•°ã€‚
- ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨ç†ï¼ˆText Generation Inferenceï¼‰ï¼Œæ”¯æŒè¿ç»­æ‰¹å¤„ç†ã€å¼ é‡å¹¶è¡Œç­‰é«˜çº§åŠŸèƒ½ï¼Œå®ç°æå¿«çš„æ¨ç†ç»“æœã€‚


å¯¹äºæ¯ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨åŠç²¾åº¦ï¼ˆfloat16ï¼‰æˆ–ä½¿ç”¨é‡åŒ–æƒé‡çš„æƒ…å†µä¸‹è¿è¡Œæ¨¡å‹ã€‚ç”±äº Mixtral æ¨¡å‹åœ¨å¤§å°ä¸Šå¤§è‡´ç›¸å½“äºä¸€ä¸ªå‚æ•°ä¸º 45B çš„ç¨ å¯†æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ä¼°ç®—æ‰€éœ€çš„æœ€å° VRAMï¼š

| Precision | Required VRAM |
| --------- | ------------- |
| float16   | >90 GB        |
| 8-bit     | >45 GB        |
| 4-bit     | >23 GB        |


### Using ğŸ¤— Transformers

ä½¿ç”¨ transformers [release 4.36](https://github.com/huggingface/transformers/releases/tag/v4.36.0)ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Mixtral å¹¶å……åˆ†åˆ©ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ‰€æœ‰å·¥å…·ï¼Œä¾‹å¦‚ï¼š

- è®­ç»ƒå’Œæ¨æ–­è„šæœ¬ä»¥åŠç¤ºä¾‹
- å®‰å…¨æ–‡ä»¶æ ¼å¼ (`safetensors`)
- ä¸è¯¸å¦‚ bitsandbytesï¼ˆ4 ä½é‡åŒ–ï¼‰ã€PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰å’Œ Flash Attention 2 ç­‰å·¥å…·çš„é›†æˆ
- è¿è¡Œæ¨¡å‹ç”Ÿæˆçš„å®ç”¨ç¨‹åºå’Œè¾…åŠ©å·¥å…·
- å¯¼å‡ºæ¨¡å‹ä»¥è¿›è¡Œéƒ¨ç½²çš„æœºåˆ¶

ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ `transformers` ç‰ˆæœ¬ï¼š

```bash
pip install -U "transformers==4.36.0" --upgrade
```

åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Transformers è¿›è¡Œæ¨æ–­å¹¶è¿›è¡Œ 4 ä½é‡åŒ–ã€‚ç”±äºæ¨¡å‹ä½“ç§¯è¾ƒå¤§ï¼Œæ‚¨éœ€è¦è‡³å°‘å…·æœ‰ 30 GB RAM çš„æ˜¾å¡æ¥è¿è¡Œå®ƒã€‚è¿™åŒ…æ‹¬ A100ï¼ˆ80 æˆ– 40GB ç‰ˆæœ¬ï¼‰æˆ– A6000ï¼ˆ48 GBï¼‰ã€‚

```python
from transformers import AutoTokenizer
import transformers
import torch

model = "mistralai/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
)

messages = [{"role": "user", "content": "Explain what a Mixture of Experts is in less than 100 words."}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```

> \<s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A
Mixture of Experts is an ensemble learning method that combines multiple models,
or "experts," to make more accurate predictions. Each expert specializes in a
different subset of the data, and a gating network determines the appropriate
expert to use for a given input. This approach allows the model to adapt to
complex, non-linear relationships in the data and improve overall performance.
> 

### Using Text Generation Inference

**[Text Generation Inference](https://github.com/huggingface/text-generation-inference)** æ˜¯ Hugging Face å¼€å‘çš„ä¸€ä¸ªå¯ç”¨äºç”Ÿäº§çš„æ¨ç†å®¹å™¨ï¼Œæ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„è½»æ¾éƒ¨ç½²ã€‚å®ƒå…·æœ‰è¿ç»­æ‰¹å¤„ç†ã€ä»¤ç‰Œæµå¼ä¼ è¾“ã€åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œå¿«é€Ÿæ¨ç†çš„å¼ é‡å¹¶è¡Œç­‰ç‰¹æ€§ï¼ŒåŒæ—¶æä¾›ç”Ÿäº§å°±ç»ªçš„æ—¥å¿—è®°å½•å’Œè¿½è¸ªã€‚

æ‚¨å¯ä»¥åœ¨ Hugging Face [Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=mistralai%2FMixtral-8x7B-Instruct-v0.1&vendor=aws&region=us-east-1&accelerator=gpu&instance_size=2xlarge&task=text-generation&no_suggested_compute=true&tgi=true&tgi_max_batch_total_tokens=1024000&tgi_max_total_tokens=32000) ä¸Šéƒ¨ç½² Mixtral , å…¶ä½¿ç”¨ Text Generation Inference ä½œä¸ºåç«¯ã€‚è¦éƒ¨ç½² Mixtral æ¨¡å‹ï¼Œè¯·è½¬åˆ° [model page](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) ç„¶åç‚¹å‡» [Deploy -> Inference Endpoints](https://ui.endpoints.huggingface.co/new?repository=meta-llama/Llama-2-7b-hf) .

*Note: æ‚¨å¯èƒ½éœ€è¦é€šè¿‡ç”µå­é‚®ä»¶è¯·æ±‚é…é¢å‡çº§ï¼Œä»¥ä¾¿è®¿é—® A100 GPUï¼Œé‚®ç®±åœ°å€ä¸º **[api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co)**ã€‚*


æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„åšå®¢ä¸­äº†è§£æœ‰å…³[ä½¿ç”¨ Hugging Face æ¨ç†ç«¯ç‚¹éƒ¨ç½² LLMs](https://huggingface.co/blog/inference-endpoints-llm)çš„æ›´å¤šä¿¡æ¯ã€‚ **[blog](https://huggingface.co/blog/inference-endpoints-llm)** åŒ…å«æœ‰å…³æ”¯æŒçš„è¶…å‚æ•°ä»¥åŠå¦‚ä½•ä½¿ç”¨ Python å’Œ Javascript æµå¼ä¼ è¾“å“åº”çš„ä¿¡æ¯ã€‚

æ‚¨è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åœ¨æœ¬åœ°ä½¿ç”¨ Docker åœ¨ 2x A100sï¼ˆ80GBï¼‰ä¸Šè¿è¡Œæ–‡æœ¬ç”Ÿæˆæ¨ç†ï¼š

```bash
docker run --gpus all --shm-size 1g -p 3000:80 -v /data:/data ghcr.io/huggingface/text-generation-inference:1.3.0 \
	--model-id mistralai/Mixtral-8x7B-Instruct-v0.1 \
	--num-shard 2 \
	--max-batch-total-tokens 1024000 \
	--max-total-tokens 32000
```


## Fine-tuning with ğŸ¤— TRL

è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŠ€æœ¯å’Œè®¡ç®—ä¸Šéƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†çœ‹ä¸€ä¸‹ Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­æä¾›çš„å·¥å…·ï¼Œä»¥åœ¨å•ä¸ª A100 GPU ä¸Šé«˜æ•ˆè®­ç»ƒ Mixtralã€‚

ä»¥ä¸‹æ˜¯åœ¨ OpenAssistant çš„ [chat dataset](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25) ä¸Šå¯¹ Mixtral è¿›è¡Œå¾®è°ƒçš„ç¤ºä¾‹å‘½ä»¤ã€‚ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ 4-bit é‡åŒ–å’Œ [QLoRA](https://arxiv.org/abs/2305.14314) æ¥å®šä½æ³¨æ„åŠ›å—ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚ã€‚è¯·æ³¨æ„ï¼Œä¸å¯†é›†çš„ transformer ä¸åŒï¼Œæˆ‘ä»¬ä¸åº”è¯¥å®šä½ MLP å±‚ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç¨€ç–çš„ï¼Œä¸ PEFT ä¸å¤ªäº’åŠ¨ã€‚

First, install the nightly version of ğŸ¤— TRL and clone the repo to access the [training script](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py):

```bash
pip install -U transformers
pip install git+https://github.com/huggingface/trl
git clone https://github.com/huggingface/trl
cd trl
```

Then you can run the script:

```bash
accelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml --num_processes=1 \
	examples/scripts/sft.py \
	--model_name mistralai/Mixtral-8x7B-v0.1 \
	--dataset_name trl-lib/ultrachat_200k_chatml \
	--batch_size 2 \
	--gradient_accumulation_steps 1 \
	--learning_rate 2e-4 \
	--save_steps 200_000 \
	--use_peft \
	--peft_lora_r 16 --peft_lora_alpha 32 \
	--target_modules q_proj k_proj v_proj o_proj \
	--load_in_4bit
```

è¿™éœ€è¦å¤§çº¦ 48 å°æ—¶åœ¨å•ä¸ª A100 ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å¯ä»¥é€šè¿‡è°ƒæ•´ `--num_processes` åˆ°ä½ å¯ç”¨çš„ GPU æ•°é‡æ¥è½»æ¾å¹¶è¡ŒåŒ–ã€‚


