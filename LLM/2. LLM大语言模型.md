# LLM大语言模型

ChatGLM、LLaMA、OPT、FLAN、Alpaca、PaLM、Baichuan、Qwen等；



## 开源模型整理：

| 模型 |含义|模型结构|模型大小| 训练数据                                    |参数结构|PE|
|-|-|-|-|-----------------------------------------|-|-|
|LLaMA等小模型：|-|-|-| -                                       |--|--|
|LLaMA|通过比通常情况下使用更多的token进行训练，在各种推理预算下达到最佳性能，由此产生的模型被称为LLaMA|decoder-only|7B、13B、30B、65B| 1.4T tokens                             ||旋转位置嵌入（RoPE）|
|LLaMA-2|-同上|decoder-only|7B、13B、34B、70B| 1.96T tokens                            ||旋转位置嵌入（RoPE）|
|Alpaca|引入了self-instruction框架，调用GPT3模型生成一系列instruction来对LLaMA进行微调，提高指令遵循能力来自我迭代进化，指令遵循语言模型叫Alpaca|decoder-only|7B| 微调52K指令                                 ||-|
|BELLE|Be Everyone’s Large Language Engine，对LLaMA，BLOOMZ等开源大模型进行微调|decoder-only|LLaMA -7B/13B、BLOOMZ-7B| 20万、60万、100万、200万样本的指令数据                ||-|
|Baichuan|是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，支持中英双语，上下文窗口长度为 4096；在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果|decoder-only|7B、13B| 1.2T tokens【7B】；1.4T tokens【13B】        ||旋转位置嵌入（RoPE）【7B】、ALiBi【13B】|
|Baichuan-2|-|decoder-only|7B、13B| 2.6T tokens                             ||-|
|Qwen|-|未披露|开源7B，闭源上T| 2.2T tokens                             ||-|
|ChatGLM-2|ChatGLM GLM-130B上中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）、RLHF等技术实现人类意图对齐；不同于 BERT、GPT-3 以及 T5 的架构，是一个包含多目标函数的自回归预训练模型|prefix LM|6B、12B、32B、66B、130B| 1.4T tokens                             ||旋转位置嵌入（RoPE）|
|100B以上大模型：|-|-|-| -                                       |-|-|
|GLM|采用自回归空白填充方式（auto-regressive blank infilling），随机对tokens中连续的spans机进行掩盖，以autoregressive blank infilling objective目标，通过调整span的长度和数量，让模型分别训练NLU、LM长文本生成，seq2seq等多个目标|prefix LM|130B| 1.2T英文tokens + 1.3T 中文tokens            ||旋转位置嵌入（RoPE）|
|BLOOM|BLOOM目标不仅是公开发布一个能够和近期开发的系统相媲美的大规模多语言的语言模型，而且还记录其开发中的协调过程；【在BLOOM之前几乎没有开源的大模型】|decoder-only|176B| 1.61T tokens                            ||ALiBi|
|OPT|Open Pre-trained Transformer Language Models，一个完全开放的预训练Transformer语言模型|decoder-only|175B| 0.18T tokens                            ||Learned|
|PaLM|Pathways Language Model|decoder-only|8B、62B、540B| 0.78T tokens                            ||旋转位置嵌入（RoPE）|
|T5|Text-To-Text transfer Transformer，提出统一框架，将所有NLP任务转化为text-to-text任务；适合NLU和“有条件”的文本生成任务，例如文本总结，respose生成等|encoder-decoder|3B、11B|                                         ||Relative PE|
|基于指令微调【Instruction Tuning】：|-|-|-| -                                       |-|-|
|FLAN|在137B LaMDA-PT的预训练LM上，将60个NLP任务用自然语言指令的方式描述并把它们混合在一起，进行指令微调（instruction tuning）；这个模型，我们称之为FLAN（Finetuned Language Net）|decoder-only|137B| <li> 1）预训练 2.49T tokens；<li> 2）微调60*30K指令； |-|-|
|FLAN-T5|通过在超大规模的任务上进行微调，让语言模型具备了极强的泛化性能，做到单个模型就可以在1800多个NLP任务上都能有很好的表现|encoder-decoder|3B、11B|-|-|-|
|COT|对于某些问题，即使给出一个示范（one-shot或者few-shot），LM也无法很好的解答，但是如果我们一步一步地引导，那么模型就能够得到正确答案，这种一步一步引导的prompting就称为Chain of Thought prompting|-|100B以上|-|-|-|




