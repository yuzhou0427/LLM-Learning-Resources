# LLM大语言模型

ChatGLM、LLaMA、OPT、FLAN、Alpaca、PaLM、Baichuan、Qwen等；



## 开源模型整理：

| 模型                          | 含义                                                                                                                                                              | 模型结构            | 模型大小                    | 训练数据                                       | 参数结构 | PE                          | attention，FNN                                                             |norm|MSL|激活函数| 备注其他                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|:-|:-|:-|:-|:-|:-|:----------------------------|:--------------------------------------------------------------------------|:-|:-|:-|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| LLaMA等小模型：                  | -                                                                                                                                                               | -               | -                       | -                                          | --   | --                          |                                                                           ||||                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| LLaMA                       | 通过比通常情况下使用更多的token进行训练，在各种推理预算下达到最佳性能，由此产生的模型被称为LLaMA                                                                                                           | decoder-only    | 7B、13B、30B、65B          | 1.4T tokens                                |      | 旋转位置嵌入（RoPE）                | 因果多头注意力算子                                                                 |RMSNorm|2048|SwiGLU|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| LLaMA-2                     | -同上                                                                                                                                                             | decoder-only    | 7B、13B、34B、70B          | 1.96T tokens                               |      | 旋转位置嵌入（RoPE）                | <li> 1）分组查询注意力(GQA)；<li> 2）Faster Transformer推理加速；<li> 3）PagedAttention；  |RMSNorm|4096|SwiGLU| LLaMA-2-chat：训练 LLaMA-2-chat：Llama 2 使用公开的在线数据进行预训练。 然后通过使用监督微调创建 Llama-2-chat 的初始版本。 接下来，Llama-2-chat 使用人类反馈强化学习 (RLHF) 进行迭代细化，其中包括拒绝采样和近端策略优化 (PPO)。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Alpaca                      | 引入了self-instruction框架，调用GPT3模型生成一系列instruction来对LLaMA进行微调，提高指令遵循能力来自我迭代进化，指令遵循语言模型叫Alpaca                                                                       | decoder-only    | 7B                      | 微调52K指令                                    |      | -                           | -                                                                         |-|2048|-| <li> **中文指令数据**：原英文版数据用175 个人工编写的任务种子集合作为初始化指令样例，用text-davinci-003生成。[中文指令数据](https://github.com/carbonz0/alpaca-chinese-dataset)是由原英文版数据集用机器翻译和self-instruct生成。 <li> **训练**：使用了完全分片数据并行（Fully Sharded Data Parallel） 和混合精度（mixed precision） 等训练等技术。                                                                                                                                                                                                                                                                                                                                                                                                               |
| BELLE                       | Be Everyone’s Large Language Engine，对LLaMA，BLOOMZ等开源大模型进行微调                                                                                                     | decoder-only    | LLaMA -7B/13B、BLOOMZ-7B | 20万、60万、100万、200万样本的指令数据                   |      | -                           | -                                                                         |-|2048|-|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Baichuan                    | 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，支持中英双语，上下文窗口长度为 4096；在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果                                                 | decoder-only    | 7B、13B                  | 1.2T tokens【7B】；1.4T tokens【13B】           |      | 旋转位置嵌入（RoPE）【7B】、ALiBi【13B】 | Flash-Attention                                                           |RMSNorm|4096|SwiGLU| **数据处理：**<br> <li>原始数据包括开源的中英文数据和自行抓取的中文互联网数据，以及部分高质量知识性数据；<li>参考相关数据工作，频率和质量是数据处理环节重点考虑的两个维度。我们基于启发式规则和质量模型打分，对原始数据集进行篇章和句子粒度的过滤。在全量数据上，利用局部敏感哈希方法，对篇章和句子粒度做滤重；<li>经过不断的调整和多轮测试，最终确认了一个在下游任务上表现最好的中英文配比；<li>使用了一个基于自动学习的数据权重策略，对不同类别的数据进行配比；<br> **分词SentencePiece**：<br>参考学术界方案使用 SentencePiece 中的 Byte-Pair Encoding (BPE) 作为分词算法，并且进行了以下的优化：<br> <li>目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用2000万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。 <li> 对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。 <li> 对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖 <br> **ALiBi**：<br> <li> ALiBi 线性偏置技术，相对于 Rotary Embedding 计算量更小，对推理性能有显著提升。 |
| Baichuan-2                  | -                                                                                                                                                               | decoder-only    | 7B、13B                  | 2.6T tokens                                |      | -                           |                                                                           ||||                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Qwen                        | -                                                                                                                                                               | 未披露             | 开源7B，闭源上T               | 2.2T tokens                                |      | -                           |                                                                           ||8K||                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ChatGLM-2                   | ChatGLM GLM-130B上中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）、RLHF等技术实现人类意图对齐；不同于 BERT、GPT-3 以及 T5 的架构，是一个包含多目标函数的自回归预训练模型                                         | prefix LM       | 6B、12B、32B、66B、130B     | 1.4T tokens                                |      | 旋转位置嵌入（RoPE）                | <li> 1）**Flash Attention** 扩充上下文长度；<li> 2）**Muilt-Query Attention**，更高效推理；||32K||<li> Muilt-Query Attention：<br> [Multi-Query Attention](https://arxiv.org/abs/1911.02150)，提高了生成速度，同时也降低了生成过程中KV Cache的显存占用，此外，ChatGLM2-6B采用Causal Mask进行对话训练，连续对话时可复用前面轮次的KV Cache，进一步优化了显存占用。因此，使用6GB显存的显卡进行INT4量化的推理时，初代的ChatGLM-6B模型最多能够生成1119个字符就会提示显存耗尽，而ChatGLM2-6B能够生成至少8192个字符。<br> <li> Flash Attention：<br> 基于[FlashAttention](https://github.com/Dao-AILab/flash-attention)技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的2K扩展到了32K，并在对话阶段使用8K的上下文长度训练。|
| 100B以上大模型：                  | -                                                                                                                                                               | -               | -                       | -                                          | -    | -                           |
| GLM                         | 采用自回归空白填充方式（auto-regressive blank infilling），随机对tokens中连续的spans机进行掩盖，以autoregressive blank infilling objective目标，通过调整span的长度和数量，让模型分别训练NLU、LM长文本生成，seq2seq等多个目标 | prefix LM       | 130B                    | 1.2T英文tokens + 1.3T 中文tokens               |      | 旋转位置嵌入（RoPE）                |
| BLOOM                       | BLOOM目标不仅是公开发布一个能够和近期开发的系统相媲美的大规模多语言的语言模型，而且还记录其开发中的协调过程；【在BLOOM之前几乎没有开源的大模型】                                                                                   | decoder-only    | 176B                    | 1.61T tokens                               |      | ALiBi                       |
| OPT                         | Open Pre-trained Transformer Language Models，一个完全开放的预训练Transformer语言模型                                                                                          | decoder-only    | 175B                    | 0.18T tokens                               |      | Learned                     |
| PaLM                        | Pathways Language Model                                                                                                                                         | decoder-only    | 8B、62B、540B             | 0.78T tokens                               |      | 旋转位置嵌入（RoPE）                |
| T5                          | Text-To-Text transfer Transformer，提出统一框架，将所有NLP任务转化为text-to-text任务；适合NLU和“有条件”的文本生成任务，例如文本总结，respose生成等                                                         | encoder-decoder | 3B、11B                  |                                            |      | Relative PE                 |
| 基于指令微调【Instruction Tuning】： | -                                                                                                                                                               | -               | -                       | -                                          | -    | -                           |
| FLAN                        | 在137B LaMDA-PT的预训练LM上，将60个NLP任务用自然语言指令的方式描述并把它们混合在一起，进行指令微调（instruction tuning）；这个模型，我们称之为FLAN（Finetuned Language Net）                                          | decoder-only    | 137B                    | <li> 1）预训练 2.49T tokens；<li> 2）微调60*30K指令； | -    | -                           |
| FLAN-T5                     | 通过在超大规模的任务上进行微调，让语言模型具备了极强的泛化性能，做到单个模型就可以在1800多个NLP任务上都能有很好的表现                                                                                                  | encoder-decoder | 3B、11B                  | -                                          | -    | -                           |
| COT                         | 对于某些问题，即使给出一个示范（one-shot或者few-shot），LM也无法很好的解答，但是如果我们一步一步地引导，那么模型就能够得到正确答案，这种一步一步引导的prompting就称为Chain of Thought prompting                                      | -               | 100B以上                  | -                                          | -    | -                           |




