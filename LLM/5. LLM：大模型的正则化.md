# LLM：大模型的正则化

主流大模型使用的Normalization主要有三类，分别是Layer Norm，RMS Norm，以及Deep Norm。

![norm.jpg](..%2Fassets%2Fassets5%2Fnorm.jpg)

> 上图中PE就是Position Embedding，MCL为 Max Sentence Length；

> 从上图可以发现，很多模型把全连接层Bias项也去掉了，一个直观的猜测是全连接层的bias项，储存到的是关于数据的一种先验分布信息，而把这种先验分布信息直接储存在模型中，反而可能会导致模型的迁移能力下降。所以它们把每一层的bias项也都去掉了。

<br>

## 目录
- [Post-Norm & Pre-Norm](#1post-norm-和-pre-norm)
- [RMS-Norm](#2rms-norm)
- [Deep-Norm](#3deep-norm)
- [Layer-Norm](#4layer-normalization)


<br>

## 1、Post-Norm 和 Pre-Norm：

![prenorm-postnorm.png](..%2Fassets%2Fassets5%2Fprenorm-postnorm.png)

&nbsp;&nbsp;&nbsp;&nbsp;传统transformer中，在self- attention add之后做layer normalizaiton的方式，称为post-norm，整体block结构为 self- attention + ADD + LN；与之对应的在attention计算之前做layer normalization的，称之为pre-norm，此时整block体结构为 LN + self-attention + ADD，另外pre-norm在最后的输出层之前还会做一个normalization；


1. pre-norm在attention计算前进行归一化，减少输入部分的方差，将数据转化到同一分布，提升transformer内部训练的稳定性；另外由于ADD在norm之后，即在梯度计算时存在一条通路，为此可以减少梯度弥散和梯度爆炸风险，整体更易于训练，提升模型收敛速度；但由于这部分参数没有做归一化，模型的训练效果会相对弱一点，降低了模型的表达能力和鲁棒性；


2. post-norm是在attention和ADD之后，即在残差之后做的归一化，为此输出部分被约束到0，1分布上，对参数的正则化更强，整体模型的收敛效果更好，训练完成后模型表达能力提升，鲁棒性更强；但同时失去了残差“易于训练”的优点，导致收敛速度慢一些；另外由于输入端没有做归一化，transformer训练内部容易不稳定；
`对于postLN训练较慢，可以考虑如下方法：1）warm-up；2） Layer-wise Adaptive Rate Scaling；3）Weight Standardization；4）Group Normalization；`


&nbsp;&nbsp;&nbsp;&nbsp;目前比较明确的结论是：大模型同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm；原因参考：https://kexue.fm/archives/9009
<br>
&nbsp;&nbsp;&nbsp;&nbsp;总结起来，在大模型深度层次很多的情况下，pre-norm层数的增加，起到的效果和增加宽度较为接近，实际上带来的效果帮助不大，是一个伪“深度”；而post-norm则是实实在在的增加深度；那为什么大模型还是pre-norm为主？**主要原因，还是大模型训练成本太高了，需要一些训练提速的办法**；

<br>

## 2、RMS Norm:

&nbsp;&nbsp;&nbsp;&nbsp;**RMS-Norm**即对于layer normlization去掉center部分，剩下的就是RMSNorm（root mean square layer normlization）；即在归一化时，**仅对方差进行了约束，从而减少数据变化范围，来提升训练速度**；有研究表明<font color="#dd0000">RMSNorm和LN效果差不多</font>， 同时由于去掉center部分，减少了训练参数beta，使得<font color="#dd0000">模型训练时间得到明显提升</font>；
	
    计算公式为：

![rmsnorm.png](..%2Fassets%2Fassets5%2Frmsnorm.png)

> 这里的ai与Layer Norm中的x等价，作者认为这种模式在简化了Layer Norm的同时，可以在各个模型上减少约 7%∼64% 的计算时间；

<br>

## 3、Deep Norm：

**DeepNorm**，实际为 Weight Standardlization + post-norm：

1. 其中Weight Standardlization是指在进行attention 和FNN计算时，**对FNN、V、OUT三部分参数进行Xvier归一化，对Q和K不进行处理【因为不影响输出量级】**；由此可以减少参数的量级来减少输出结果的变化范围，进而减小梯度变化范围，从而改善梯度弥散和梯度爆炸，提升训练速度和稳定性；
2. 另一方面，post-norm具有训练收敛效果好，模型表达能力更强，鲁棒性强的优势；为此DeepNorm即包含了post-norm的良好性能，又包含了pre-norm的训练稳定性；

Deep Norm是对Post-LN的的改进，具体的：

![deepnorm.png](..%2Fassets%2Fassets5%2Fdeepnorm.png)

这里N和M分别指编码和解码器的层数；

论文中，作者认为 Post-LN 的不稳定性部分来自于梯度消失以及太大的模型更新，同时，有以下几个理论分析：
- 定义了“预期模型更新”的概念表示 模型更新的规模量级；
- 证明了 WQ和 WK不会改变注意力输出大小数量级的界限，因而 β 并没有缩小这部分参数；
- 模型倾向于累积每个子层的更新，从而导致模型更新量呈爆炸式增长，从而使早期优化变得不稳定；
- 使用Deep Norm 的 "预期模型更新"，在参数 α,β 取值适当的时候，以常数为界。

作者通过实验证实了Deep Norm在训练深层transformer模型的时候具备近乎恒定的更新规模，成功训练了1000层transformer的模型，认为Deep Norm在具备 Post-LN 的良好性能 的同时又有 Pre-LN 的稳定训练。


    代码实现：https://github.com/microsoft/to

<br>

## 4、Layer Normalization：

&nbsp;&nbsp;&nbsp;&nbsp;LayerNorm是大模型也是transformer结构中最常用的归一化操作，简而言之，它的作用是对特征张量按照某一维度或某几个维度进行0均值，1方差的归一化操作，计算公式为：

![layer-norm.jpg](..%2Fassets%2Fassets5%2Flayer-norm.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;这里的x 可以理解为张量中具体某一维度的所有元素，比如对于 shape 为 (2,2,4) 的张量 input，若指定归一化的操作为第三个维度，则会对第三个维度中的四个张量（2,2,1），各进行上述的一次计算：

&nbsp;&nbsp;&nbsp;&nbsp;这里结合PyTorch的nn.LayerNorm算子来看比较明白：

```python
nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)
```

- `normalized_shape`：归一化的维度，int（最后一维）list（list里面的维度），还是以（2,2,4）为例，如果输入是int，则必须是4，如果是list，则可以是[4], [2,4], [2,2,4]，即最后一维，倒数两维，和所有维度；
- `eps`：加在分母方差上的偏置项，防止分母为0；
- `elementwise_affine`：是否使用可学习的参数 gamma和 beta ，前者开始为1，后者为0，设置该变量为True，则二者均可学习随着训练过程而变化。


<br>

## 参考来源：
CSDN：LLM大模型的正则化、https://zhuanlan.zhihu.com/p/620297938、https://kexue.fm/archives/9009