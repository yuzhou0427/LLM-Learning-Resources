# transformer 始化、参数化、标准化

## 目录

- [1 transformer优缺点](#1-transformer优缺点)
- [2 transformer-初始化参数化标准化](#2-transformer-初始化参数化标准化)
  - [2.1 采样分布](#21-采样分布)
  - [2.2 稳定二阶距](#22-稳定二阶距)
  - [2.3 激活函数](#23-激活函数)
  - [2.4 直接标准化](#24-直接标准化)
  - [2.5 nkt参数化](#25-nkt参数化)
  - [2.6 attention-dot为什么除根号d](#26-attention-dot为什么除根号d)
  - [2.7 残差连接](#27-残差连接)


<br>

## 1 transformer优缺点：

- **优点：**
  - （1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位；
  - （2）Transformer的设计**最大的带来性能提升的关键是将任意两个单词的距离是1**，这对解决NLP中棘手的长期依赖问题是非常有效的；
  - （3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向；
  - （4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。

- **缺点：**
  - （1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型**丧失了捕捉局部特征的能力**，RNN + CNN + Transformer的结合可能会带来更好的效果；
  - （2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷；


## 2 transformer 初始化、参数化、标准化：

### 2.1 采样分布：

&nbsp;&nbsp;&nbsp;&nbsp;初始化自然是随机采样的的，所以这里先介绍一下常用的采样分布。一般情况下，我们都是从指定均值和方差的随机分布中进行采样来初始化。其中常用的随机分布有三个：正态分布（Normal）、均匀分布（Uniform）和截尾正态分布（Truncated Normal）。

![transformer-1.png](..%2Fassets%2Fassets11%2Ftransformer-1.png)


### 2.2 稳定二阶距：

&nbsp;&nbsp;&nbsp;&nbsp;在一般的教程中，推导初始化方法的思想是尽量让输入输出具有同样的均值和方差，通常会假设输入是均值为0、方差为1的随机向量，然后试图让输出的均值为0、方差为1。不过，笔者认为这其实是没有必要的，而且对于某些非负的激活函数来说，根本就做不到均值为0。**<font color="#dd0000">事实上，只要每层的输入输出的二阶（原点）矩能保持不变，那么在反向传播的时候，模型每层的梯度也都保持在原点的一定范围中，不会爆炸也不会消失，所以这个模型基本上就可以稳定训练</font>**。


![transformer-2.png](..%2Fassets%2Fassets11%2Ftransformer-2.png)

&nbsp;&nbsp;&nbsp;&nbsp;综上可以看出，对于无激活函数的FCN，若输入分布均值为0，方差为1，Xavier参数为均值为0，方差为1/m随机分布；则输出分布也为均值0，方差为1；


### 2.3 激活函数：

**Relu激活函数：**

&nbsp;&nbsp;&nbsp;&nbsp;当激活函数为relu时，可以假设有一半y被置为0：此时便得到He初始化，均值为0，方差为2/m随机分布，

![transformer-3.png](..%2Fassets%2Fassets11%2Ftransformer-3.png)


**其他激活函数：**

&nbsp;&nbsp;&nbsp;&nbsp;激活函数是elu,gelu等，那么分析起来就没那么简单了；而如果激活函数是tanh,sigmoid的话，那么根本找不到任何初始化能使得二阶矩为1；这种情况下如果还想保持二阶矩不变的话，那么可以考虑的方案是“微调激活函数的定义”。

![transformer-4.png](..%2Fassets%2Fassets11%2Ftransformer-4.png)

![transformer-5.png](..%2Fassets%2Fassets11%2Ftransformer-5.png)


### 2.4 直接标准化：


&nbsp;&nbsp;&nbsp;&nbsp;当然，相比这种简单的“微调”，更直接的处理方法是各种Normalization方法，如Batch Normalization、Instance Normalization、Layer Normalization等，这类方法直接计算当前数据的均值方差来将输出结果标准化，而不用事先估计积分，有时候我们也称其为“归一化”。这三种标准化方法大体上都是类似的，除了Batch Normalization多了一步滑动平均预测用的均值方差外，它们只不过是标准化的维度不一样，比如NLP尤其是Transformer模型用得比较多就是Layer Normalization：

![transformer-6.png](..%2Fassets%2Fassets11%2Ftransformer-6.png)


&nbsp;&nbsp;&nbsp;&nbsp;其他就不再重复描述了。关于这类方法起作用的原理，有兴趣的读者可以参考笔者之前的[《BN究竟起了什么作用？一个闭门造车的分析》](https://kexue.fm/archives/6992)。

&nbsp;&nbsp;&nbsp;&nbsp;这里笔者发现了一个有意思的现象：Normalization一般都包含了减均值（center）和除以标准差（scale）两个部分，但近来的一些工作逐渐尝试去掉center这一步，甚至有些工作的结果显示去掉center这一步后性能还略有提升。

&nbsp;&nbsp;&nbsp;&nbsp;比如2019年的论文[《Root Mean Square Layer Normalization》](https://arxiv.org/abs/1910.07467)比较了去掉center后的Layer Normalization，文章称之为RMS Norm，形式如下：

![transformer-7.png](..%2Fassets%2Fassets11%2Ftransformer-7.png)


&nbsp;&nbsp;&nbsp;&nbsp;可以看出，RMS Norm也就是L2 Normalization的简单变体而已，但这篇论文总的结果显示：RMS Norm比Layer Normalization更快，效果也基本一致。

&nbsp;&nbsp;&nbsp;&nbsp;除了这篇文章外，RMS Norm还被Google用在了T5中，并且在另外的一篇文章[《Do Transformer Modifications Transfer Across Implementations and Applications?》](https://arxiv.org/abs/2102.11972)中做了比较充分的对比实验，显示出RMS Norm的优越性。这样看来，未来RMS Norm很可能将会取代Layer Normalization而成为Transformer的标配。

&nbsp;&nbsp;&nbsp;&nbsp;无独有偶，同样是2019年的论文[《Analyzing and Improving the Image Quality of StyleGAN》](https://arxiv.org/abs/1912.04958)提出了StyleGAN的改进版StyleGAN2，里边发现所用的Instance Normalization会导致部分生成图片出现“水珠”，他们最终去掉了Instance Normalization并换用了一个叫“Weight demodulation”的东西，但他们同时发现如果去掉Instance Normalization的center操作能改善这个现象。这也为Normalization中的center操作可能会带来负面效果提供了佐证。

&nbsp;&nbsp;&nbsp;&nbsp;一个直观的猜测是，**center操作，类似于全连接层的bias项，储存到的是关于数据的一种先验分布信息，而把这种先验分布信息直接储存在模型中，反而可能会导致模型的迁移能力下降。所以T5不仅去掉了Layer Normalization的center操作，它把每一层的bias项也都去掉了【不同任务的先验分布是不同的，为此可以去除bias项】**。


### 2.5 NKT参数化：

&nbsp;&nbsp;&nbsp;&nbsp;回到全连接层的Xavier初始化，它说我们要用“均值为0、方差为1/m的随机分布”初始化。不过，除了直接用这种方式的初始化外，我们还可以有另外一种参数化的方式：用“均值为0、方差为1的随机分布”来初始化，但是将输出结果除以根号m，即模型变为：

![transformer-8.png](..%2Fassets%2Fassets11%2Ftransformer-8.png)

&nbsp;&nbsp;&nbsp;&nbsp;这在高斯过程中被称为“NTK参数化”，很显然，利用NTK参数化，我们可以将所有参数都用标准方差初始化，但依然保持二阶矩不变，甚至前面介绍的“微调激活函数”，也可以看成是NTK参数化的一种。一个很自然的问题是：NTK参数化跟直接用Xavier初始化相比，有什么好处吗？

![transformer-9.png](..%2Fassets%2Fassets11%2Ftransformer-9.png)


### 2.6 attention dot为什么除根号d：

![transformer-10.png](..%2Fassets%2Fassets11%2Ftransformer-10.png)


### 2.7 残差连接：

![transformer-11.png](..%2Fassets%2Fassets11%2Ftransformer-11.png)

![transformer-12.png](..%2Fassets%2Fassets11%2Ftransformer-12.png)

![transformer-13.png](..%2Fassets%2Fassets11%2Ftransformer-13.png)



    参考来源：https://zhuanlan.zhihu.com/p/48508221、https://zhuanlan.zhihu.com/p/400925524