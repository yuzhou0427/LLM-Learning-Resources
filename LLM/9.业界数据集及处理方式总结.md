# 业界数据集及处理方式总结

## 目录

- [结论](#结论)
- [通用领域](#通用领域)


<br>

## 结论

**数据处理：**
- 多样性：可使用 ROUGE 指标过滤掉生成的指令与已有指令重合的指令；
- 高质量：可计算困惑度 Perplexity（PPL），Perplexity 低的通常是不流畅的，可以将低于一定阈值的去掉。也可以用质量模型打分；
- 数据清洗工具CleanTool：包括数据去重，低质量数据删除等功能，(可选使用GPU Turbo Speed Up)
- 规则筛选：根据具体场景进行设置，比如筛选掉有害公司形象的样本。

**数据量：**

| 领域 | 阶段     | 数据量 |
|----|--------|-----|
|通用领域| 预训练    |万亿tokens|
|通用领域| 微调     |百万-千万级条|
|垂直领域| 全参数微调  |百万-千万级条|
|垂直领域| LoRA微调 |几十万-百万级条，以几十万为主|


**数据类型比例：**
- <font color="#dd0000">base 开源:专业</font>：能找到的一般在5:1到1:1；
- <font color="#dd0000">chat 单轮:多轮</font>：如果是单轮占主导的真实场景，单轮:多轮比例能找到的在8:1到3:1，如果是多轮占主导的，多轮比例可以再高很多。


**prompt：**
1. *对齐*：微调模型尽量prompt和基座模型对齐；
2. *描述清晰明确*：加入尽可能多的限定词，注意句子结构完整性，尽量具有描述性；
3. *顺序*：在prompt开始时告诉模型希望它执行的任务，然后再给出上下文信息和示例，可以在prompt末尾重复指令；
4. *分隔符*：使用分隔符清晰地表示输入的不同部分；
5. *约束兜底*：提示模型做兜底，避免胡编乱造；
6. *结构化输出*：要求结构化的输出。


<br>

## 通用领域：

### 数据集：

| 模型                                                                                                                                            | 训练数据                  | 数据处理                    | 训练数据量            | 模型参数量                  | 词表大小 | 模型（微调）                                                                                                                                         | 训练数据（微调）                                                                                                                                                                                                                                                                                                              | 数据处理（微调）                        |训练数据量（微调）|模型参数量（微调）|词表大小（微调）|
|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------|-------------------------|------------------|------------------------|-----|------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|-|-|-|
| [Chinese-LLaMA](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) | 中文语料                  |                         | 20GB             | 7B、13B                 | 49953 | [Chinese-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) |                                                                                                                                                                                                                                                                                                                       | 采样+规则筛选数据标注使用了人工标注的方式，保证了数据的质量。 |~2M条(7B模型)；~3M条(13B模型)|7B、13B|49953|
| Bloom                                                                                                                                         | 46种自然语言和13种编程语言，包含中文。 | 做了质量筛选、去重和隐私处理，尽量避免性别歧视 | 1.5TB            | 560M、1.1B、1.7B、3B、7.1B、176B | 250880 | BLOOMZ-MT                                                                                                                                      | xP3mt，包含46种自然语言，13种NLP任务：编程、close QA、提取、多选QA、情感分析、总结、主题分类、翻译、消歧等。                                                                                                                                                                                                                                                     | 人工标注的数据集的收集                     |80M条|7.1B、176B|250880|
| /                                                                                                                                             |                       |                         |                  |                        |     | belle                                                                                                                                          | 参考[Alpaca](https://github.com/tatsu-lab/stanford_alpaca) Self-instruct生成的中文数据集，目前开放[多轮对话数据0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)、[数学题0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)、[情景多轮对话0.4M](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M) |                                 |7B模型尝试了用0.2M、0.6M、1M、2M条；13B模型用2M条|7B、13B|32000(BELLE-LLaMA)；79458(BELLE-LLaMA-EXT)；250880(BELLE-BLOOM)|
| Baichuan-7B，Baichuan-13B-Base                                                                                                                 | 开源的中英文数据和自行抓取的中文互联网数据，以及部分高质量知识性数据              | 基于启发式规则和质量模型打分，进行篇章和句子粒度的过滤。在全量数据上，利用局部敏感哈希方法，对篇章和句子粒度做滤重。经过不断的调整和测试，确认最好的中英文配比。使用一个基于自动学习的数据权重策略，对不同类别的数据进行配比。                   | 1.2T/1.4T tokens | 7B、13B                   | 64000    | Baichuan-13B-Chat                                                                                                                              | 未公开|                                 | |13B|64000|
| ChatGLM-6B                                                                                                                                          | 中英双语，中英文比例为1:1              |                    | 1T tokens        | 6B                     | 130528    | ChatGLM-6B                                                                                                                                     | 未公开                                                                                                                                                                                                                                                                                                                   |
| ChatGLM2-6B                                                                                                                                          | 中英              |                    | 1.4T tokens      | 6B                       | 130528    | ChatGLM2-6B                                                                                                                                    | 未公开                                                                                                                                                                                                                                                                                                                   |


[Chinese-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) 训练数据：

| 数据集         | 数据量   |
|-------------|-------|
| 中英翻译数据      | 500K  |
| pCLUE数据     | 300K  |
| Alpaca数据（英） | 50K |
| Alpaca数据（中） | 50K |
| Self-instruct数据       | 1~2M |


总结：
```预训练模型的数据量基本在万亿tokens，非特定领域的微调数据量至少在百万级条；```

如何进行数据清洗？

- 多样性：例如在 Self-Instruct 论文中，会使用 ROUGE 指标，过滤掉生成的指令与已有指令重合的指令。也可以用数据清洗工具CleanTool(可选使用GPU Turbo Speed Up)，包括数据去重，低质量数据删除等功能，未来将继续不断完善。

- 高质量：使用 ChatGPT 生成数据，自然训练出来的模型就是模仿 ChatGPT 的回复风格。然而，**ChatGPT（指 GPT3.5）自身的缺点包括浓浓的机翻味道、文绉绉的、不够活泼可爱，其次中文生成不够流畅**。一种思路是使用 PPL 等指标筛选出生成的指令和回复，计算困惑度 Perplexity。Perplexity 低的通常是不流畅的，可以将低于一定阈值的去掉。


### 数据预处理：

| 模型              | 数据格式 | prompt格式                                                                                                                                                                                                                                                                                                                                                                           |
|-----------------|------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Chinese-LLaMA-Alpaca          | 参考Alpaca 指令数据格式：[Alpaca-prompt-demo.json](prompt-demo%2FAlpaca-prompt-demo.json) | [来源](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/inference_hf.py) <br>"Below is an instruction that describes a task. "<br>"Write a response that appropriately completes the request.\n\n"<br>"### Instruction:\n\n{instruction}\n\n### Response:\n\n"                                                                                                  |
| belle         |  | [来源](https://github.com/LianjiaTech/BELLE/blob/6eb905a8df8051000210e30708d8ec1e5b54bff5/train/src/sample_generator.py#L24) <br>Human:{input} <br>Assistant:{}                                                                                                                                                                                                                      |
| baichuan    |   | 默认格式：<br> A chat between a curious user and an artificial intelligence assistant.<br> The assistant gives helpful, detailed, and polite answers to the user's questions.<br> Human: {input}<br> Assistant:<br> 也可采用其他格式，但微调与推理时应该保持一致。                                                                                                                                             |
| ChatGLM    |   | [来源](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/222498523332e6f27d430a9ac5fe10b593196f18/src/glmtuner/chat/stream_chat.py#L33) <br>例如：<br> """[Round 0]<br> 问：你好，你能帮我解答一个问题吗？<br> 答：当然，请问有什么问题？<br> [Round 1]<br> 问：我想了解人工智能的未来发展方向，你有什么想法吗？<br> 答：人工智能在未来的发展方向可能包括更强大的机器学习算法，更先进的自然语言处理技术，以及更加智能的机器人。<br> [Round 2]<br> 问：听起来很不错。人工智能可能在哪些方面面临挑战呢？<br> 答："""<br> 来源 |


### 开源数据集各类型及其比例参考:

| 开源                                                                           | 数据比例                                                |
|------------------------------------------------------------------------------|-----------------------------------------------------|
| [Firefly](https://github.com/yangjianxin1/Firefly)                           | ![firefly.png](..%2Fassets%2Fassets9%2Ffirefly.png) |
| [BELLE](https://huggingface.co/datasets/BELLE-2/train_3.5M_CN_With_Category) | ![belle.png](..%2Fassets%2Fassets9%2Fbelle.png)     |
| [COIG](https://huggingface.co/datasets/BAAI/COIG)                            | 翻译24%、考试23%、人类价值调整12%、反事实纠正5%和 Leetcode 指令语料库4%；    |
| [MOSS](https://github.com/OpenLMLab/MOSS/tree/main/SFT_data)                                                                     | 头脑风暴9%，复杂指令9%，编程18%，角色扮演23%，写作32%，无害7%；             |


### 多轮对话开源数据集资源:

[RefGPT](https://github.com/sufengniu/RefGPT)：开源了5万条中文事实性多轮对话，同时训练了一个RefGPT模型，专门根据reference生成多轮对话（可能会在未来公开）

[moss-003-sft-data](https://github.com/OpenLMLab/MOSS/tree/main/SFT_data): moss-moon-003-sft约含110万条对话数据，包括多轮对话，覆盖有用性、忠实性、无害性三个层面，更加符合真实用户意图分布。


<br>

## 专业领域:

### 数据量级与比例:

| 领域         | 模型                                                              |开源|专业|开源：专业|单轮|多轮|单轮：多轮|
|-------------|-----------------------------------------------------------------|-|-|-|-|-|-|
|医疗| [SoulChat](https://github.com/scutcyr/SoulChat)（心理咨询）           |0|1.2M| |150k|1.05M|1:7|
|医疗| [本草（原名华驼）](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) |0|9k| |8k|1k|8:1|
|医疗| [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor)（非中文）      |Alpaca 52k|115k|2:1| | | |
|法律| [LexiLaw](https://github.com/CSHaitao/LexiLaw)                  |BELLE-1.5M通用数据|几百k| | | | |
|法律| [Lawyer LLama](https://github.com/AndrewZhe/lawyer-llama)  |Alpaca-GPT452k 中文，52k 英文|21k|5:1|16k|5k(2或3轮)|~3:1|
|金融| bloombergGPT  |3450亿token|3630亿token|1:1| | | |



**为什么有的模型不用开源数据？**
<br>

猜测这些领域的数据模型已经见过一些/该垂直领域与通用领域差距没有那么大，不容易造成遗忘。


## 数据生成方式（GPT生成）:

参考LaWGPT：对于法律文书数据、司法考试数据：

1. 初级数据生成：根据 [Stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process) 和 [self-instruct](https://github.com/yizhongw/self-instruct) 方式生成对话问答数据self-instruct 流程：

| 1、通常使用100～200个种子任务，每个任务都有一条人工编写的指令和实例，例如：                                                                                                                                                    |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [self-instruct.json](prompt-demo%2Fself-instruct.json)                                                                                                                                       |
| 2、用模型 text-davinci-003或者 gpt-3.5-turbo生成指令和对应的回答，代码可参照：[Chinese-LLaMA-Alpaca/crawl_prompt.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py)，[BELLE/data](https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M) |
|[self-instruct-prompt.py](prompt-demo%2Fself-instruct-prompt.py)|
|3、过滤：<br> 为了数据的多样性，新生成的指令只有与种子池中的指令的 ROUGE-L 小于0.7时才会添加进入种子池；<br> 排除一些无法被语言模型处理的指令，比如涉及图像、图片、图形的指令；<br> 在给指令生成实例时，会过滤掉输入相同但是输出不同的实例。|


2. 知识引导的数据生成：通过 Knowledge-based Self-Instruct 方式基于中文法律结构化知识生成数据。

3. 引入 ChatGPT 清洗数据，辅助构造高质量数据集。


<br>

## 怎么写好prompt:

**基本要素:**

&nbsp;&nbsp;&nbsp;&nbsp;输入的 prompt 格式不同对于大模型的结果也有不同的影响，要把结果往更好的方向引导，在业界实现中，往往需要 prompt 的格式包含以下几个要素：

| 要素 | 解释   |重要性|
|----|------|-|
| 角色 | 给大模型设定一个角色扮演，让它进入专家模式，对于结果生成的内容有质量上的引导 |选填|
| 问题 | 你的主要问题、相关背景等 |必填|
| 目标 | 你的需求，需要大模型完成、输出什么内容  |必填|
| 要求 | 对于你的目标，希望大模型回答时需要注意什么、回答的格式等  |选填|


> 输入：你是一名架子鼓老师（角色），你需要为你的培训班招生，准备一节试听课，听课对象为8-12岁的孩子以及他们的家长（问题），请你写出这节试听课的提纲和课程内容（目标）。请注意，课程要求时长45分钟，过程中要穿插互动环节（要求）。


**技巧：**

1. **描述清晰明确；**
```
   - **加入尽可能多的限定词**，告知模型能干什么、不能干什么，具体说明上下文、目的、结果、长度、格式、样式、风格等。
   - **注意句子结构完整性**，主谓宾能不省略的就不省略，实践中有时候发现GPT3.5中省略一个主语对结果影响还是较大的。
   - **尽量具有描述性**：比如说明模型希望具有的能力和角色，可以使用类比进行描述。
```

2. **顺序很重要；**
```
   - 一般而言，**在prompt开始时告诉模型希望它执行的任务，然后再给出上下文信息和示例**，可以帮助产生更高质量的输出。（GPT3.5之前作用更明显） 此外，由于recency bias的影响，prompt末尾的信息可能比开头的信息对输出的影响更大，因此**值得尝试在prompt末尾重复指令**。
```

3. **使用分隔符清晰地表示输入的不同部分；**


4. **做好任务假设的边界条件处理（避免胡编乱造）；**
```
   鉴于很多任务是开放性的，所以任务描述的假设不一定成立，这个时候就需要考虑潜在的边界条件以及处理逻辑。又或者如果模型无法完成任务时，需要考虑为模型提供替代方案。
   e.g. prompt要求“从给定文本中梳理出有序指令”，那么你就需要考虑实际文本中如果不包含有序指令该如何处理。
   e.g. prompt要求ChatGPT“结合文本回答问题”时，你可以加上“如果答案不存在，则回复‘未找到’”来避免ChatGPT在没找到答案时胡编乱造。
```
   
5. **要求结构化的输出；**


6. **"Few-shot"or"one-shot" prompting（少量示例）；**


7. **微调垂直领域大模型尽量prompt和基座模型对齐，因为大模型会根据输入数据生成特征向量，而基座模型则会将该向量转换为实际值。只有两个模型之间相对位置正确才能获得良好的效果。**


<br>

## 如何让chatGPT自动优化提示词prompt?一条简单指令轻松搞定

 > https://mp.weixin.qq.com/s/UMLtpF2myDEJYn9AYW-4XQ
```text
请阅读以下所有说明，并且在你理解后说：“我们开始吧！”

我希望你成为我的提示生成者。你的目标是帮助我制作最适合我的需求的提示。这个提示将由你，ChatGPT，来使用。你将按照以下步骤进行操作：
你的第一个回答将是询问我应该关于什么写一个提示。我会提供我的答案，但我们需要通过不断迭代的方式来改进它，继续进行下一步。

根据我的输入，你将生成3个部分。

重写的提示（提供你改写的提示。它应该清晰、简洁，并且容易理解）
建议（提供3个关于在提示中包含哪些细节以改进它的建议）
问题（询问3个与我提供的额外信息有关的最相关的问题，以改进提示）

在这些部分的末尾，提醒我可选的选项，它们是：

选项1：阅读输出并提供更多信息或回答一个或多个问题
选项2：输入“使用这个提示”，然后我将把它作为查询提交给你
选项3：输入“重新开始”以从头开始这个过程
选项4：输入“退出”以结束脚本并回到常规的ChatGPT会话

如果我输入“选项2”、“2”或“使用这个提示”，那么我们就完成了，你应该使用重写的提示作为提示来生成我的请求。
如果我输入“选项3”、“3”或“重新开始”，那么忘记最新的重写提示，重新开始这个过程。
如果我输入“选项4”、“4”或“退出”，那么结束这个过程，回到你的一般操作模式。

我们将继续这个迭代的过程，我会向你提供额外的信息，你会在重写的提示部分更新提示，直到它完成为止。

```

<br>

## 如何避免回答和问题不相关:

对于垂直类知识库，有时候用户的提问可能并不能匹配到相关答案。在对答案精确率要求较高的场景，为了避免ChatGPT一本正经的胡说八道，可以考虑两个方法：

1. 在向量匹配计算余弦相似度时有一定的阈值，如果问题和知识库的段落相似度太低则直接给用户固定的答复，例如：很抱歉，您的问题我暂时无法回答，请问您需要转人工客服吗？
2. 更严格的场景下，可以利用LIamaIndex最新的 evaluation工具 ，在问题+答案+上下文均匹配时，该API才会返回YES。这可以避免ChatGPT产生“幻觉”答案等情况。

<br>

## 如何让模型稳定地输出json:

[TypeChat](https://news.cnblogs.com/n/746388/)


<br>

## 参考资料:

[解读Lawyer LLaMA，延申专业领域大模型微调：数据集构建，模型训练](https://zhuanlan.zhihu.com/p/634861170)

[Lawyer LLaMA Technical Report](https://arxiv.org/pdf/2305.15062.pdf)

[如何制造一个垂直领域大模型](https://zhuanlan.zhihu.com/p/644450744#:~:text=%E6%9E%84%E5%BB%BA%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E6%B3%95%201%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%2B%E7%9F%A5%E8%AF%86%E5%BA%93%20%EF%BC%8C%E8%BF%99%E6%98%AF%E7%9B%AE%E5%89%8D%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%8A%9E%E6%B3%95%EF%BC%8C%E6%9E%84%E5%BB%BA%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%8C%E5%88%A9%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84In%20Context%20Learning%EF%BC%88%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%EF%BC%89%E7%9A%84%E8%83%BD%E5%8A%9B%EF%BC%8C%E9%80%9A%E8%BF%87%E6%A3%80%E7%B4%A2%E5%9C%A8%E9%97%AE%E7%AD%94%E4%B8%AD%E5%A2%9E%E5%BC%BA%E7%BB%99%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%8C%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E5%87%86%E7%A1%AE%E5%9B%9E%E7%AD%94%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E5%92%8C%E4%BC%81%E4%B8%9A%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9B%E4%BD%86%E6%98%AF%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%AF%B9%E5%87%86%E7%A1%AE%E6%A3%80%E7%B4%A2%E8%83%BD%E5%8A%9B%E8%A6%81%E6%B1%82%E5%BE%88%E9%AB%98%EF%BC%8C%E5%8F%A6%E5%A4%96%E5%A6%82%E6%9E%9C%E6%A8%A1%E5%9E%8B%E6%9C%AC%E8%BA%AB%E4%B8%8D%E5%85%B7%E5%A4%87%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%EF%BC%8C%E5%8D%B3%E4%BD%BF%E6%9C%89%E5%87%86%E7%A1%AE%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%8C%E4%B9%9F%E9%9A%BE%E4%BB%A5%E7%BB%99%E5%87%BA%E6%AD%A3%E7%A1%AE%E7%AD%94%E6%A1%88%E3%80%82%202%20PEFT,%28%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%9A%84%E5%BE%AE%E8%B0%83%29%20%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%80%E4%BA%9B%E5%BC%80%E6%BA%90%E7%9A%84%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E9%80%9A%E8%BF%87P-Tuning%E6%88%96%E8%80%85LoRA%E7%AD%89%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%BE%AE%E8%B0%83%EF%BC%8C%E4%BD%BF%E5%85%B6%E9%80%82%E5%BA%94%E9%A2%86%E5%9F%9F%E9%97%AE%E9%A2%98%EF%BC%8C%E6%AF%94%E5%A6%82%E4%B8%80%E4%BA%9B%E6%B3%95%E5%BE%8B%E5%92%8C%E5%8C%BB%E7%96%97%E7%9A%84%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E3%80%82%20...%203%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83%20%EF%BC%8C%E8%BF%99%E6%98%AF%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A7%8D%E6%AF%94%E8%BE%83%E6%B5%81%E8%A1%8C%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%8C%E5%9C%A8%E6%9F%90%E4%B8%AA%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%EF%BC%8C%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83%E8%AE%AD%E7%BB%83%EF%BC%8C%E4%BD%BF%E5%85%B6%E5%AD%A6%E4%BC%9A%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E3%80%82%20...%20%E6%9B%B4%E5%A4%9A%E9%A1%B9%E7%9B%AE)

[垂直领域大模型的一些思考及开源模型汇总](https://zhuanlan.zhihu.com/p/642611747)

[垂直领域大语言模型汇总](https://zhuanlan.zhihu.com/p/637313379)

[开源语言模型百宝袋](https://github.com/createmomo/Open-Source-Language-Model-Pocket)