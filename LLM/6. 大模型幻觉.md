# 大模型幻觉

&nbsp;&nbsp;&nbsp;&nbsp;**当LLM幻觉的原因在于训练数据、文本中存在的偏见以及LLM的固有局限性**；通过解决这些基本因素，我们可以努力提高LLM的准确性和可靠性，尽量保证它们生成的信息不仅连贯，而且是准确的。

## 目录

- [1 幻觉定义](#1-幻觉定义)
- [2 LLM幻觉原因](#2-llm幻觉原因)
  - [2.1 数据层面](#21-数据层面)
  - [2.2 模型层面](#22-模型层面)
- [3 幻觉的评估](#3-幻觉的评估)
  - [3.1 ChatGPT GPT4生成不真实回复的评估机理](#31-chatgptgpt4生成不真实回复的评估机理)
  - [3.2 Retrieval-Augment-LLM评估](#32-retrieval-augment-llm评估)
- [4 LLM幻觉的滚雪球现象](#4-llm幻觉的滚雪球现象)
- [5 LLM幻觉的原子粒度评估](#5-llm幻觉的原子粒度评估)
- [6 幻觉检测修正](#6-幻觉检测修正)
  - [6.1 事实性增强的语言模型](#61-事实性增强的语言模型)
  - [6.2 基于外部知识和自动反馈提升事实性](#62-基于外部知识和自动反馈提升事实性)
  - [6.3 零资源黑盒LLM幻觉检测](#63-零资源黑盒llm幻觉检测)
  - [6.4 零资源黑盒事实错误纠正](#64-零资源黑盒事实错误纠正)
  - [6.5 工具增强的LLM自动纠正](#65-工具增强的llm自动纠正)
  - [6.6 通过推理时干预诱导LLM生成符合事实的答案](#66-通过推理时干预诱导llm生成符合事实的答案)

<br>


## 1 幻觉定义：

> **定义：**
> 当模型生成的文本不遵循原文（Faithfulness）或者不符合事实（Factualness），我们就可以认为模型出现了幻觉的问题。

什么是Faithfulness and Factualness：
- **Faithfulness**：是否遵循input content；
- **Factualness**：是否符合世界知识；


在***传统任务***里，幻觉大都是指的是Faithfulness：
```
Intrinsic Hallucination（信息冲突）: LMs在生成回复时，与输入信息产生了冲突，例如摘要问题里，abstract和document的信息不一致。
Extrinsic Hallucination（无中生有）: LMs在生成回复时，输出一些并没有体现在输入中的额外信息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic Hallucination有可能是真的信息，只是需要外部信息源进行认证）
```

而面向***LLMs***，我们通常考虑的幻觉则是Factualness：
```
因为我们应用LLM的形式是open-domain Chat，而不是局限于特定任务，所以数据源可以看做任意的世界知识。LLMs如果生成了不在input source里的额外信息，但是符合事实的，这种情况也可能是对我们有帮助的。

```

<br>

## 2 LLM幻觉原因：

### 2.1 数据层面：

在数据工程层面可能出现一些问题，导致幻觉问题：
- **数据质量**：训练数据收集过程中，众包/爬虫检索的数据可能包含虚假信息，从而让模型记忆了错误的知识；
- **数据重复**：过多的重复信息也可能导致模型的知识记忆出现bias，从而导致幻觉：

潜在的研究方向：
```
Building High-quality Training Corpus is essential.
Data verification/ Data filter/ Data selection.
```

### 2.2 模型层面：

即使有了高质量训练数据，LLMs仍然可能表现出幻觉现象。
- **模型结构**：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可能存在这一问题；
- **解码算法**：研究表明，*如果使用不确定性较高的采样算法（e.g.，top-p）会诱导LMs出现更严重的幻觉问题*。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative samples）；
- **暴露偏差**：训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long-form response的时候；
- **参数知识**：LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题。

<br>

## 3 幻觉的评估：

- **人工评估**：目前为止最靠谱的，此外还可以依靠LLM打分（比如利用GPT4，但是GPT4也存在着严重的幻觉问题，即使经过retrival-augment，检索回来的信息也有可能是错误的）
- **TruthfulQA**：一个很重要的用于评估LLM是否能够生成符合事实的答案的QA基准，被后续的LLM工作，如GPT4采用评估。包含了817个作者手写的问题，这些问题是精心设计，往往是模型或者人类都很容易回答错误的陈述。
- **模型打分的幻觉评估**： 
  - Chatgpt倾向于在回复中生成无法被验证的内容（幻觉），占比约11.4%；
  - 当前强大的LLM，如Chatgpt，都很难精准检测出文本中出现的幻觉问题；
  - 通过提供外部知识和增加推理步数，能够提升LLM检测幻觉的能力；


### 3.1 ChatGPT/GPT4生成不真实回复的评估、机理：

对LLM生成的不符合事实的错误的类型进行分类和统计：【https://arxiv.org/abs/2304.10513 】

![幻觉评估.png](..%2Fassets%2Fassets6%2F幻觉评估.png)

![幻觉评估2.png](..%2Fassets%2Fassets6%2F幻觉评估2.png)


定义了三个能力“知识记忆、知识调用、知识推理”，可能导致幻觉的：

![幻觉评估2.png](..%2Fassets%2Fassets6%2F幻觉评估3.png)


通过实验，**<font color="#dd0000">对大模型生成可靠回复给出了一些建议</font>**：
- 提供更多的背景知识（检索）
- 提供更细粒度的背景知识（分析）
- 把问题进行分解（CoT）

早期的工作，包括了对ChatGPT幻觉现象的评估：【https://arxiv.org/abs/2302.04023】
- ChatGPT有能力识别虚假信息，并能够在无法识别时回复不知道；
- ChatGPT仍然会被TruthfulQA内的问题误导；
- ChatGPT同样可能出现intrinstic（信息冲突）/extrinsic（无中生有） hallucination的case；


### 3.2 Retrieval-augment LLM评估：

Retrieval有助于显著减少LLM的幻觉现象；
<br>
来自OSU的两个工作，主要是研究了给定retrieval reference的情况下，LLM的遵循能力。

- 前一个工作首先定义了自动评估一个reference能否支撑generation的任务，然后研究了LLMs（prompt/finetuned）的完成这一任务的能力：【https://arxiv.org/abs/2305.06311】
  - automatic attribution evaluation的效果都不好
  - 小的finetuned模型可以超过大的zero-shot模型
  - 模型容量和评估效果并不完全正相关
  - 使用额外的其他任务训练，可以提升automatic attribution evaluation的能力（QA/NLI/FC）


- 后一个工作研究了给定reference情况下对LLM生成结果的影响：【https://arxiv.org/abs/2305.13300】
  - 使用一个5步走的框架进行knowledge elicitation
    - parametric memory：模型内部的知识
    - counter-memeory：与模型内部知识相反的内容
  - **<font color="#dd0000">当只有一个单一的知识源时</font>**：
    - 简单的通过entity替换的counter-memory无法诱骗模型，但是让LLM自己生成
  - **<font color="#dd0000">当有多个知识来源时</font>**：
    - LLM倾向于相信更流行的知识；
    - LLM对于知识的顺序很敏感，倾向于相信先出现的知识；
    - LLM相信更长的知识；
    - LLM随大流，相信占据大多数的知识


<br>

## 4 LLM幻觉的滚雪球现象：

【https://arxiv.org/abs/2305.13534】 本文作者认为：LLM出现幻觉现象，很多情况下并不是因为它们缺少对应的知识，而仅仅是在调用知识的过程中出错。他们发现：LLMs如果在回复一开始做出了错误的判断，那么它们随后会给出错误的解释（幻觉）。他们称这一现象为幻觉的滚雪球现象。但是，当仅给定错误的解释时，LLMs往往可以成功判断它是不正确的。这证明LLMs其实具备相应的知识，只是被早期的错误断言给误导了。
<br>
作者也探索了一些缓解幻觉滚雪球问题的方法：
- **更好的Prompt（CoT）**：有效，但是CoT也存在着思维链内部的幻觉滚雪球问题。
- **解码算法**：作者尝试了不同的解码算法，但并没有显著效果。
- **训练策略**：构造更多的CoT训练数据，以及构造允许模型自我回溯（self-correct）的训练数据。


<br>

## 5 LLM幻觉的原子粒度评估：

【https://arxiv.org/abs/2305.14251】
定义原子事实：仅包含一个信息的短句。
<br>
将LLM生成的内容（本文选用的内容是人物简介）分解为多个原子事实（instructGPT分解，人类校对），评估生成事实的精确度；

人工标注常用LLMs的原子事实精确度后，发现：
- 所有LLM都会出现较为严重幻觉问题，特别是原子粒度上；
- **检索增强的LLM（perplexity AI）能够缓解事实错误**；
- 在生成和罕见实体相关的回复时，LLM更容易犯错；
- 模型在生成回复的后期，更容易出现幻觉；

作者也研究了基于LLM自动评估原子事实精确度的可能性（不可能每次对新模型评估都靠人工标注）：
- 检索增强能显著提升评估的效果；
- 更大的LLM能够更好地评估。


<br>

## 6 幻觉检测&修正：

### 6.1 事实性增强的语言模型：

【https://arxiv.org/abs/2206.04624】
本文首先基于FEVER数据集构建一个FactualPrompt数据集，包含了符合/不符合事实的提示，用于诱导LLM生成符合/不符合事实的下文。

**评价指标：**

- <font color="#dd0000">幻觉实体率</font>：
```text
对比生成内容和Golden Knowledge中实体的重叠率；
```

- <font color="#dd0000">蕴含率</font>：
```text
使用额外的NLI模型，评估生成内容有多少是被golden Knowledge蕴含的；
```

- <font color="#dd0000">生成质量评估：除了幻觉现象外，还要确保生成的效果</font>：
```text
  - 流畅度：平均困惑度；
  - 多样性：distinct n-grams；
  - 重复度；
```

**从多个维度评估LLM生成的事实问题：**

- <font color="#dd0000">模型容量</font>：
```text
更大的模型生成结果的事实性越好；
```

- <font color="#dd0000">提示类型</font>：
```text
不符合事实的提示更可能诱导LLM生成不符合事实的内容；
```

- <font color="#dd0000">解码算法</font>：
```text
带有随机性的解码算法（如Top-P）显著比贪心解码生成的内容更不符合事实；
```

    基于这个发现作者还提出了一个非常简单的top-p解码算法优化，在生成的diversity和factuality中寻求trade-off：
    p随时间步衰减（后期生成的内容更可能不符合事实），每次生成一个新句子（通过检测是否生成了句号）重新初始化p，并且p的衰减可以定一个下界；

<br>

作者还提出了一种继续预训练策略来提升事实性：
- 使用更权威的数据，如Wiki来训练；
- 给每个句子加上Wiki Document的名称：作者认为这能给句子提供额外的事实信息，比如解释句子里的代词（有点玄学）。


### 6.2 基于外部知识和自动反馈提升事实性：

通过外挂知识库和LLM自我审视，来提升LLM生成的事实性/效果；主要包含以下部分：
- <font color="#dd0000">LLM Agent</font>
- <font color="#dd0000">外部知识库：互联网、wiki百科等；</font>
- 动作执行器：
  - <font color="#dd0000">知识检索</font>：BM-25/Dense，通过给定prompt检索知识；
  - <font color="#dd0000">Prompt引擎</font>：基于用户输入/知识/历史信息/反馈信息等，构造prompt，融合信息生成新的回复；
- 策略选择器：
  - 基于规则（是否通过utility模块）
  - 可学习（T5-based）
- 效用检验模块：
  - 打分器：用分数评估回复质量；
  - 反馈器：用自然语言给出回复评估；

本质上，**该系统是一个检索知识->生成回复->评估的反复迭代/决策的过程**，能够通过外部知识库和检查模块，显著提升生成的各维度效果，包括事实性。


### 6.3 零资源黑盒LLM幻觉检测：

作者认为传统的幻觉检测方法在当今LLM时代有如下的缺陷【https://arxiv.org/abs/2303.08896】：
- **基于不确定度指标**：这一类方法通过衡量LLM回复的熵/概率，来判断LLM对回复是否自信，越不自信越可能是编造的内容。但是该方法对闭源模型（如OpenAI）不友好。
- **基于事实验证指标**：这一类方法需要外挂知识库，但是现在缺少涵盖所有世界知识的高质量知识库。

作者提出了SelfCheckGPT方法，核心的假设是：**如果大模型非常肯定一个事实，那么它随机采样多次生成的回复，将对该事实有着近似的陈述（self-consistency）**。如果多次采样，LLM都生成不同的陈述，那么很有可能是出现了幻觉。具体地，评估多个采样陈述是否一致，可以通过：1）BERTScore；2）QA-based；3）n-gram metric进行实现。


### 6.4 零资源黑盒事实错误纠正：

提出了一个五步的零资源事实错误纠正流水线【https://arxiv.org/abs/2305.07982】：
- Claim Answer抽取：从陈述中抽取关键信息；
- Question Generation：针对每个关键信息，生成一个问题；
- Question Answer：针对每个问题，将外部证据作为额外输入，进行回答；
- QA-to-claim：将QA-pair转回陈述；
- Correction scoring：额外打分器判断新的陈述是否合理。


### 6.5 工具增强的LLM自动纠正：

【https://arxiv.org/abs/2305.11738】
MSRA的工作，本文允许LLM在自我检查答案正确性的过程中调用外部工具，比如知识库、搜索引擎和维基百科，从而缓解事实性和幻觉问题。


### 6.6 通过推理时干预诱导LLM生成符合事实的答案：

【https://arxiv.org/abs/2306.03341】
本文中作者提出了一种推理时干预的策略（ITI）提升LLM生成答案的事实性。
- 作者假设：LLMs know more than they say，LLM内部存在着隐藏的、可解释的结构，这些结构和事实性息息相关，因此可以通过干预：
- 作者探索了LLM的生成回复准确率（直接回答问题）和Probe准确率（用一个linear classifier基于中间状态选择回答）的关系，发现LLM很多情况下知道知识，但无法正确生成回复。

- ITI方法选择和事实知识紧密相关的head，进行干预，让激活值移动到truthful相关的方向，实验表明能够有效提升回复的事实性。



`完整参考来源：https://zhuanlan.zhihu.com/p/642648601`