# 模型微调总结

## 目录

- [1 总结背景](#1-总结背景)
- [2 微调思路](#2-微调思路)
- [3 数据处理](#3-数据处理)
  - [3.1 数据量级](#31-数据量级)
  - [3.2 数据生成](#32-数据生成)
- [4 基座模型](#4-基座模型)
- [5 模型训练](#5-模型训练)
  - [5.1 prompt构造](#51-prompt构造)
  - [5.2 训练方法](#52-训练方法)
  - [5.3 业界结论](#53-业界结论)
- [6 模型推理](#6-模型推理)
- [相关解释](#相关解释)


<br>


## 1. 总结背景

&nbsp;&nbsp;&nbsp;&nbsp;本次经验总结基于企微问答模型微调经验产生。主要聚焦于微调环节，并结合预训练环节进行对比。

![tuning.png](..%2Fassets%2Fassets8%2Ftuning.png)


## 2. 微调思路

在预训练模型具备通用能力的情况下，
- **核心思想**：以防止模型能力在灾难性遗忘为主，以强化领域应用为次之。
- **主要思路**：分析场景特点，构造适配场景特点的通用型数据，切忌点状优化。


&nbsp;&nbsp;&nbsp;&nbsp;根据bad case点状优化的缺点：构造的数据越来越偏移，prompt越来越模版化，使得模型微调数据分布与原数据差异过大，导致模型逐渐模版化记忆，通用技能灾难性遗忘。
灾难性遗忘的主要特点：
- 1）适配prompt下，模型回复符合预期。但修改prompt后，回答混乱。
- 2）适配prompt下，增添prompt要求细节，模型执行能力较弱，自动“忽略”这些细节。
- 3）通用场景应用时，仍进行业务场景回复。丧失场景判别能力。
- 4）训练数据存在内容回答质量优秀，但新内容回答质量参差不齐，差异巨大。


![tuning-2.png](..%2Fassets%2Fassets8%2Ftuning-2.png)

<br>

## 3. 数据处理

### 3.1 数据量级

**结论:**
1. 不同训练方式/任务类型与数据量级强绑定。一般的，预训练 > 全参数SFT > LoRA。训练数据总量有如下规律：

| 领域 | 阶段     | 数据量 |
|----|--------|-----|
|通用领域| 预训练    |万亿tokens（GB或TB存储量级）|
|通用领域| 微调     |百万-千万级条|
|垂直领域| 全参数微调  |百万-千万级条|
|垂直领域| LoRA微调 |几十万-百万级条，以几十万为主|

2. 低参数量级情况下（少于1M），全参数微调与LoRA效果基本一致；
3. <font color="#dd0000">训练数据比例保证开源+垂直领域数据，比例需要根据效果确定。一般可以设置开源：垂直 = 2:1 / 3:1；</font>


&nbsp;&nbsp;&nbsp;&nbsp;企微模型训练实验中，比较不同数据量级进行实验，发现1:3为较好的数据比例：

|  | 开源：领域 = 1：0.5 | 开源：领域 = 1：3 | 开源：领域 = 1：4 |
|----|---------------|-------------|-------------|
|GPT评分| 4.61          | 4.66        | 4.56        |

<br>

**业界举例：**


&nbsp;&nbsp;&nbsp;&nbsp;在业界垂直领域训练中，90%以上的项目都会混合开源数据，数据比例及量级受模型本身效果实验影响，但关于训练方式与数据量级基本符合结论总结。

| 领域         | 模型                                                              |开源|专业|开源：专业|单轮|多轮|单轮：多轮|
|-------------|-----------------------------------------------------------------|-|-|-|-|-|-|
|医疗| [SoulChat](https://github.com/scutcyr/SoulChat)（心理咨询）           |0|1.2M| |150k|1.05M|1:7|
|医疗| [本草（原名华驼）](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) |0|9k| |8k|1k|8:1|
|医疗| [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor)（非中文）      |Alpaca 52k|115k|2:1| | | |
|法律| [LexiLaw](https://github.com/CSHaitao/LexiLaw)                  |BELLE-1.5M通用数据|几百k| | | | |
|法律| [Lawyer LLama](https://github.com/AndrewZhe/lawyer-llama)  |Alpaca-GPT452k 中文，52k 英文|21k|5:1|16k|5k(2或3轮)|~3:1|
|金融| bloombergGPT  |3450亿token|3630亿token|1:1| | | |


&nbsp;&nbsp;&nbsp;&nbsp;**当基线模型足够大时，少量的样本微调数据就可以达到很好的效果（几千或者几万条）**。该结论与以上结论并不违背，该结论认为的<font color="#dd0000">**较好的基线模型应该是100B左右的大模型</font>**。而参数量较小的模型因为基础能力较弱因此需要更多数据进行微调。


### 3.2 数据生成

1. 数据生产思路围绕场景下模型需要的主要能力进行。
2. 生产模式使用种子数据 + GPT生成的方式。确保数据的清洗与质量监督。
3. 可以借助开源数据集增强模型部分能力。

| 能力需求 | 生产任务名称 | 细节阐述 | 作用       |
|----|--|--|----------|
|单轮问答| 商家真实QA | QA真实数据 | 明确真实业务场景问答模式 |
|单轮问答| Doc2QA | 将真实QA问题集汇集成短文，使用GPT对于短文内容生成QA | 扩充query形式与问法 |
|多轮对话| Doc2Conv | 将真实QA问题集汇集成短文，给定主题使用GPT对于短文内容生成多轮对话 | 强化业务相关的多轮对话能力 |
|语义理解| query相似度判断 | 使用开源数据AFQMC，支付宝真实query相似度打分数据 | 强化query语意理解能力       |
|语义理解| 短文总结 | 使用开源数据MLQA，Guanoco，对短文段落进行信息总结 |          |


**业界方式:**

目前开源项目大多使用[Stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)的数据生产方式:


| 1、通常使用100～200个种子任务，每个任务都有一条人工编写的指令和实例，例如：                                                                                                                                                    |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [self-instruct.json](prompt-demo%2Fself-instruct.json)                                                                                                                                       |
| 2、用模型 text-davinci-003或者 gpt-3.5-turbo生成指令和对应的回答，代码可参照：[Chinese-LLaMA-Alpaca/crawl_prompt.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py)，[BELLE/data](https://github.com/LianjiaTech/BELLE/tree/main/data/1.5M) |
|[self-instruct-prompt.py](prompt-demo%2Fself-instruct-prompt.py)|
|3、过滤：<br> 为了数据的多样性，新生成的指令只有与种子池中的指令的 ROUGE-L 小于0.7时才会添加进入种子池；<br> 排除一些无法被语言模型处理的指令，比如涉及图像、图片、图形的指令；<br> 在给指令生成实例时，会过滤掉输入相同但是输出不同的实例。|


<br>

## 4. 基座模型

1. <font color="#dd0000">chatglm泛化性过强</font>，容易基于参考答案自由发挥，并夹带幻觉。
2. <font color="#dd0000">belle-bloom较为稳定，prompt格式的适应能力较强</font>，比较适合作为通用基线模型。
3. <font color="#dd0000">baichuan在正确的prompt格式上能够发挥出较好性能</font>，但prompt格式敏感，效果波动极大。
4. 更换与基座模型不同的prompt会对模型表现产生较大影响。
   - 测试样本：真实商家QA，300例。
   - 测试信息：背景介绍 + 问题 + 参考答案 --> 模型回答


| 模型 | 企微专属prompt：BLEU | 适配基座prompt：BLEU | 备注         |
|----|-----------------|---------------|------------|
|bloom-7b| 0.0715          |               |            |
|belle-7b| 0.0698          |               |  |
|belle-bloom-7b| 0.2056          | 0.2437            |  |
|chatglm-6b| 0.2182          |             | 泛化能力强，生成结果不可控 |
|phoenix-7b| 0.0314          |             |  |
|baichuan-7b| 0               | 0.2384            | prompt格式敏感 |


## 5. 模型训练

### 5.1 Prompt构造

1. 微调过程的**Prompt格式需要与开源模型格式保持一致**，能够最大程度降低通用能力的遗忘。
2. 训练过程中**Prompt的背景描述尽量简洁、泛化，以Input，output内容为主**。避免训练过程中模型学习Prompt中的任务模版，造成通用能力遗忘。

&nbsp;&nbsp;&nbsp;&nbsp;目前的数据预处理与prompt处理流程一般遵循Alpaca、Vicuna项目流程。企微模型遵循belle项目流程（Alpaca），保证prompt格式与belle模型一致。


| 模型              | 数据格式 | prompt格式                                                                                                                                                                                                                                                                                                                                                                           |
|-----------------|------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Chinese-LLaMA-Alpaca          | 参考Alpaca 指令数据格式：[Alpaca-prompt-demo.json](prompt-demo%2FAlpaca-prompt-demo.json) | [来源](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/inference_hf.py) <br>"Below is an instruction that describes a task. "<br>"Write a response that appropriately completes the request.\n\n"<br>"### Instruction:\n\n{instruction}\n\n### Response:\n\n"                                                                                                  |
| belle         |  | [来源](https://github.com/LianjiaTech/BELLE/blob/6eb905a8df8051000210e30708d8ec1e5b54bff5/train/src/sample_generator.py#L24) <br>Human:{input} <br>Assistant:{}                                                                                                                                                                                                                      |
| baichuan    |   | 默认格式：<br> A chat between a curious user and an artificial intelligence assistant.<br> The assistant gives helpful, detailed, and polite answers to the user's questions.<br> Human: {input}<br> Assistant:<br> 也可采用其他格式，但微调与推理时应该保持一致。                                                                                                                                             |
| ChatGLM    |   | [来源](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/222498523332e6f27d430a9ac5fe10b593196f18/src/glmtuner/chat/stream_chat.py#L33) <br>例如：<br> """[Round 0]<br> 问：你好，你能帮我解答一个问题吗？<br> 答：当然，请问有什么问题？<br> [Round 1]<br> 问：我想了解人工智能的未来发展方向，你有什么想法吗？<br> 答：人工智能在未来的发展方向可能包括更强大的机器学习算法，更先进的自然语言处理技术，以及更加智能的机器人。<br> [Round 2]<br> 问：听起来很不错。人工智能可能在哪些方面面临挑战呢？<br> 答："""<br> 来源 |



### 5.2 训练方法

**相关结论：**

1. <font color="#dd0000">在100K数据量级情况下，LoRA ≈ 全参数SFT > qLoRA</font>。

2. 继续在开源LoRA参数上增量训练（原参数继续微调）效果优于在开源微调模型上二次LoRA微调（合并参数后微调），但是开源LoRA参数较难找。

3. 7B模型已经具备一定程度的推理能力。少量COT数据可以增强模型推理能力。但是部分case存在错误。COT模型建议10B以上。


**效果对比：**

&nbsp;&nbsp;&nbsp;&nbsp;使用120K数据量级，同为belle-bloom-7B为base model，进行全量SFT微调及LoRA微调，并未体现出全量SFT效果优势。

| 方式 | 训练时间 | GPT评分 |
|----|------|--------------|
|全参数SFT| 18h  |4.46|
|LoRA| 5h   |4.51|
|qLoRA| 4.5h |4.02|


**Lora对比：**

&nbsp;&nbsp;&nbsp;&nbsp;以Chinese-Vicuna-7b为base model进行：

    - 1）LoRA增量微调（原参数继续微调）；
    - 2）LoRA二次微调（合并参数后微调 ）。结果显示为LoRA增量微调效果更优。

| 输入格式               | 方法           | BLUE评分 | gpt评分|
|--------------------|--------------|--------|-|
| Chinese-Vicuna指令格式 | 二次微调：合并参数后微调 | 0.5811 |4.4468|
|                | 增量微调：原参数继续微调 | 0.7198 |4.6666|


**COT对比：**

&nbsp;&nbsp;&nbsp;&nbsp;构建COT推理数据，使用相同prompt情况下，回答结果BLEU评分更高。但COT数据到达一定量级后，效果不再增长。

| 样本格式                          | 评测结果（BLUE） |
|-------------------------------|------------|
| input->output                 | 0.77       |
| input->cot->output            | 0.78       |
| input->cot->output(COT数量减少一半) | 0.78       |


![tuning-3.png](..%2Fassets%2Fassets8%2Ftuning-3.png)


### 5.3 业界结论

``` text
1. 增加训练数据量可以持续提高LoRA模型的有效性，但是在不同类型的任务上表现有所不同。
    1. 在Extract, Classification, Closed QA, Open QA和Summarization任务上，增加数据能持续带来效果的提升，还未达到瓶颈。
    2. 在Translation, Rewrite, 和Brainstorming任务上，几十万的数据量就能获得较好的效果。
    3. 在Math, Code, 和COT任务上，模型效果较差，而且增加数据量已经无法带来效果的提升。

2. 全参数微调>LoRA。但是算力不足，建议在已经完成了指令学习的模型上针对特定任务使用少量指令数据集做lora微调（即二次LoRA微调）。

```

**LLaMA微调记录：**

| 模型参数量 | 微调方法 | 微调数据量 | Average Score | Additional Param. | Training Time (Hour/epoch) |
|-----|---|-------|---------------|-------------------|----------------------------|
|13B|LoRA| 2M    | 0.648         | 28M               | 8                          |
|7B|LoRA| 4M    | 0.624         | 17.9M             | 11                         |
|7B|LoRA| 2M    | 0.609         | 17.9M             | 7                          |
|7B|LoRA| 0.6M  | 0.589         | 17.9M             | 5                          |
|7B|FT| 2M    | 0.710         | -                 | 31                         |
|7B|FT| 0.6M  | 0.686         | -                 | 17                         |

> 13B模型，lora微调新增参数量约28M；
> 7B模型，lora微调新增参数量约17.9M；


<br>

## 6. 模型推理

1. 7B模型在A30的推理耗时约为A100的2倍。显存并非速度瓶颈。推理加速需要模型量化。
2. <font color="#dd0000">**7B模型需要2-4张A30进行部署</font>**。


| 模型大小 | 部署方式 |Memory-Usage|占用率| prompt平均长度(字) | 平均耗时(s) | 输出平均长度(字) | 推理速度(char/s) |
|------|------------|-|-|---------------|---------|-----------|--------------|
| 7B   |单张A100|36092MiB / 81251MiB|44.42%| 563.84        | 2.01    | 113.29    | 56.44        |
| 7B   |4张A30|35234MiB / 97032 MiB|36.31%| 563.84        | 5.47    | 140.74    | 25.74        |



## 相关解释

1. 当基线模型足够大时，少量的样本微调数据就可以达到很好的效果（几千或者几万条）
   - LIMA: Less Is More for Alignment

2. 增加训练数据量可以持续提高LoRA模型的有效性，但是在不同类型的任务上表现有所不同
   - A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model

3. FT
   - 全参数微调

