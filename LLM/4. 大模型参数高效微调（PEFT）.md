# 大模型参数高效微调（PEFT）

## 目录

- [总结](#总结)
- [prefix/prompt tuning](#第一类增加前缀tokenprefixprompt-tuning)
- [adapter tuning](#第二类添加新网络结构adapter-tuning)
- [lora](#第三类增加低秩矩阵lora)

<br>

## 总结：

| 微调方法 | prefix/promp tuning                                                                                                                    | adapter（适配器）                                         | Lora                                                       |
|------|----------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|------------------------------------------------------------|
| 方式   | 不同任务增加额外隐式前缀token，微调时只训练这部分参数；                                                                                                         | 对block中attention和FNN部分增加一个较小的神经网络adapter，微调时只训练适配器参数；| 对attention中kqvo矩阵，通过学习小参数低秩矩阵来近似模型权重矩阵W的参数更新，微调时只优化低秩矩阵参数； |
| 模型结构 | <li> 1）prefix tuning：所有层都增加前缀tokens，参数训练为小参数矩阵，接大MLP映射；<br> <li> 2）prompt tuning：只训练embedding层前缀tokens，采用prompt ensembling集成多种prompts； | <li> 1）存在并行和串行两种adapter；<br> <li> 2）adapter采用瓶颈形式，即两层神经网络（向下投影矩阵 + 非线性变化 + 向上投影矩阵），再连接一个残差层；                                                   | <li> 1）对kqvo参数梯度更新矩阵，拆分层两个低秩矩阵A和B相乘；<br> <li> 2）推理时，最终kqvo矩阵为W + AB（W为原kqvo矩阵，A和B时低秩矩阵）；<br> <li> 3）矩阵A可以随机初始化，矩阵B初始化为0矩阵；                                                         |
| 影响   | 对模型embedding 部分产生较浅影响                                                                                                                  | 对深层的attention部分进行影响                                                   | 对更深一步qkvo参数矩阵进行影响                                                         |
| 效果   | 相对design prompt 效果提升                                                                                                                   | 优于prefix/prompt tuning                                                   | 优于adapter                                                         |
| 缺点   | <li> 1）训练难度大，同时效果提升有限；<br> <li> 2）另外随着前缀tokens增加，模型性能受输入长度限制，推理性能可能会下降；                                                                | <li> 1）改变模型结构， 推理时带来额外的计算时间成本；<br> <li> 2）多个任务需要训练多个模型，线上部署成本高；                                                   |                                                          |
| 优点   | <li> 1）相比design prompt，将显示prompt转化为隐式关系，提升模型对prompt的理解，提升模型性能；<br> <li> 2）不同任务可以同时训练同一个模型，线上也仅需部署单个模型；                                 | <li> 1）进行深层次的微调影响，微调后模型性能相比prompt tuning有一定提升；                                                   | <li> 1）存在较为严格数理逻辑证明，lora效果显著，甚至接近FFT效果；<br> <li> 2）低秩矩阵r<<dim，微调参数量很少，显著降低显存资源，可以进行大规模微调训练；<br> <li> 3）线上推理，由于并行计算，不带来额外计算时间成本；<br> <li> 4）不影响基模型结构，线上部署时，不同任务之间只用切换A、B矩阵便可，切换成本很低；                                                         |
| 补充   | prompt的两种形式之一：1）prompt/prefix tuning即soft prompt；2）design prompt即discrete prompt；                                                      |                                                    | 训练更多的低秩权重矩阵，比单一类型的高秩矩阵效果要好；例如r=2，在kqvo上都是用，比r=4，仅在qk上效果更好；                                                         |

<br>

&nbsp;&nbsp;&nbsp;&nbsp;当前以 ChatGPT 为代表的预训练语言模型（PLM）规模变得越来越大，在消费级硬件上进行全量微调（Full Fine-Tuning）变得不可行。此外，为每个下游任务单独存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同。**参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）方法**被提出来解决这两个问题，**PEFT 可以使 PLM 高效适应各种下游应用任务，而无需微调预训练模型的所有参数**。 微调大规模 PLM 所需的资源成本通常高得令人望而却步。 在这方面，PEFT 方法仅微调少量或额外的模型参数，固定大部分预训练参数，大大降低了计算和存储成本，同时**最先进的 PEFT 技术也能实现了与全量微调相当的性能**。

<br>
<br>

> ***FFT（fullfine-tine）***:全量微调所需的计算成本和存储成本相当高，而PEFT只需要对少量参数微调，固定大部分预训练参数，大大降级了计算成本和存储成本；

PEFT方法可以分为三类，不同的方法对 PLM 的不同部分进行下游任务的适配：
- **Prefix/Prompt-Tuning**：在模型的输入或隐层添加 k个额外可训练的前缀 tokens（这些前缀是连续的伪 tokens，不对应真实的 tokens），只训练这些前缀参数；
- **Adapter-Tuning**：将较小的神经网络层或模块插入预训练模型的每一层，这些新插入的神经模块称为 adapter（适配器），下游任务微调时也只训练这些适配器参数；
- **LoRA**：通过学习小参数的低秩矩阵来近似模型权重矩阵 W的参数更新，训练时只优化低秩矩阵参数；

![peft.jpg](..%2Fassets%2Fassets4%2Fpeft.jpg)

<br>

## 第一类：增加前缀token（prefix/prompt-tuning）

代表方法prefix-tuning，prompt-tuning；两者的相同点和差异点：

1）**相同点：**
  - 都是基于LLM上的微调，且固定预训练（PLM）模型上所有参数，通过添加特定前缀tokens的方式，只微调特定token的参数；不同的下游任务，微调独有的tokens；

2）**差异点：**
  - *消融实验*：prefix-tuning通过各种消融实验，确定在模型embedding层+ 后续所有层都添加前缀tokens方式，且对比了在不同位置添加tokens效果，发现前缀效果最好；而prompt-tuning并有这方面的消融实验对比；
  - *前缀tokens位置*：prefix-tuning添加在所有层，认为这样可以修改模型更深层的表示；prompt-tuning则只添加在embedding层；
  - *前缀tokens训练方式*：prompt-tuning直接训练prompt embedding参数；prefix-tuning则认为直接优化效果并不稳定，通过一个较小的参数矩阵，接一个较大的mlp来进行重参数化训练；
  - *Ensemble思想*：prompt-tuning中提出了prompt ensembling的方式来集成预训练模型的多种prompts；通过在一个任务上训练N个prompts，相当于构建了N个模型，当其中语言模型部分是共享的；为此在推理时，只用进行一次batch为N的正向计算；最后结果采用major voting的方式来得到prompt ensembling整体的预测结果；而prefix-tuning，并没有进行ensembling集成的思想；

<br>

## 第二类：添加新网络结构（adapter-tuning）

代表方式就是adapter tuning；

&nbsp;&nbsp;&nbsp;&nbsp;与prefix/prompt-tuning 这种通过增加前缀token embedding 的方式来以少量参数进行适配下游任务不同；adapter tuning通过在模型网络结构中增加新的网络层或模块来适配下游任务；在微调时，固定预训练（MLP）模型中的全部参数，只训练新增模块的参数；

&nbsp;&nbsp;&nbsp;&nbsp;adapter-tuning有两种形式，一种series adapter串行，一种 parallel adapter并行；其中adapter采用瓶颈的结构，即两层神经网络（向下投影矩阵 + 非线性变化 + 向上投影矩阵），再连接一个残差层；

&nbsp;&nbsp;&nbsp;&nbsp;在串行结构中，adpater接在attention和feed forward部分之后；而在并行结构中，adapter和attention，feed forward部分并行；


## 第三类：增加低秩矩阵（LoRA）

    对于PEFT中前面两类方法prefix/prompt-tuning和adapter tuning，前者优化难度较大，且随着前缀tokens的增加，性能并非线性增加有可能下降，原因在于受到模型输入长度的限制，前缀部分保留必然导致在推理时下游任务输入长度减少；而adapter-tuning改变了模型的结构，带来额外的计算，会导致推理的时间变长；

### LoRA原理：

![peft-lora.png](..%2Fassets%2Fassets4%2Fpeft-lora.png)

&nbsp;&nbsp;&nbsp;&nbsp;总结：transformer等深度网络，其内部神经网络为满秩，但其具备较低的“内在维度”【即可以通过两个低秩矩阵分解得到】，并且这个“内在维度”同样是可学习的；由此为启发，FFT的微调，本质就是原始参数W+W0，其中W0就是梯度更新参数；LoRA假设W0同样可以分解为两个低秩矩阵A和B的矩阵乘法；那在训练过程中，我们就可以固定原始参数W不变，训练两个低秩矩阵A和B便可；


### LoRA优点：

1. 全量微调一般化：通过调整lora参数r，来增加可训练参数数量，lora可以大致收敛到fft的效果；相当于全量微调的能力；
2. 推理不引入额外时间：由于都是并行化计算，所以并不带来额外的推理时间开销；同时在进行下游任务切换的时候，只用从原来AB切换到A‘B’，整体只有一个内存切换的时间开销；
3. 减少内存和存储资源消耗：由于r<<dim，为此占用的显存大幅度降度，这样便可以在gpu上进行大规模的微调训练；同时带来的另一个好处，可以在部署时以更低的成本进行任务切换，只需要切换lora的参数，而不是全量参数；为此可制定很多特定的模型，将这些模型参数存在显存中，在线上可以实时的切换；


### LoRA实验结果：

1. lora随着训练参数量增加（增加r），实验效果可以和FFT持平甚至超过；当该方法在prefix/prompt tuning上不成立，因为受input长度限制；
2. 训练更多的低秩权重矩阵，比单一类型的高秩矩阵效果要好；例如r=2，在kqvo上都是用，比r=4，仅在qk上效果更好；
3. 实验发现在r=4，在kqvo上训练效果最好；这说明W0有一个很小的内在维度？；此时增加r并不一定可以适配，效果会下降；
4. 通过分析W0和W的关系，发现低秩适配矩阵可能会放大特定下游任务的重要特征，而这些特征在一般的预训练模型中没有得到强调。