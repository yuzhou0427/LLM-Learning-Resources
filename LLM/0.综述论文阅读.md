# 0.综述论文阅读

\[ 论文：https://arxiv.org/pdf/2303.18223.pdf \]

## 目录

- [Scaling Laws for LLMs](#Scaling Laws for LLMs)



## Scaling Laws for LLMs：
- 1）KM scaling law：
model size (N), dataset size (D), and the amount of training compute (C), for neural language models. Given a compute budget c, they empirically presented three basic formulas for the scaling law：

![km-scaling-law](assets/km-scaling-law.png)

where L(·) denotes the cross entropy loss in nats. The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768M to 1.5B non-embedding parameters) and training compute,under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors.

- 2）Chinchilla scaling law：
As another representative study,Hoffmann et al. (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the compute-optimal training for LLMs. They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below:
￼
￼
where E = 1.69,A = 406.4,B = 410.7, α = 0.34 andβ = 0.28. By optimizing the loss L(N,D) under the constraint C ≈ 6ND, they showed that the optimal allocation of compute budget to model size and data size can be derived as follows:
￼
￼
where a= α/(α + β) ,b= β/(α + β) and G is a scaling coefficient that can be computed by A, B, α and β. As analyzed, given an increase in compute budget, the KM scaling law favors a larger budget allocation in model size than the data size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales, i.e., having similar values for a and b in Equation.

Tokenization：
word-based tokenization is the predominant approach, which is more aligned with human’s language cognition. However, word- based tokenization can yield different segmentation results for the same input in some languages (e.g., Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the “out- of-vocabulary” issue。
【基于单词的分词，在一些语言（例如中文）里面会产生大量的低频词，这些词无法得到有效训练；同时对于vocabulary之外的词无法处理，即所谓oov；像英文中很多词，仅仅是“时态”、“关系”的变化，存在词表过大，训练冗余】

1）Byte-Pair Encoding (BPE) tokenization：
BPE一种通用的数据压缩算法，后来被应用于NLP中的分词[。它从一组基本符号（例如字母和边界字符）开始，然后迭代地将语料库中频繁出现的两个连续标记的对组合为新的标记（称为合并）。对于每次合并，选择标准基于两个相邻标记的共现频率：选择出现频率最高的对。合并过程持续进行，直到达到预定义的大小。此外，通过将字节视为合并的基本符号，还可以使用字节级BPE来提高多语言语料库（例如包含非ASCII字符的文本）的分词质量。采用这种分词方法的代表性语言模型包括GPT-2、BART和LLaMA。

2）WordPiece tokenization：
WordPiece是Google内部的一种子词分词算法。它最初由Google在开发语音搜索系统时提出[183]。随后，它在2016年被用于神经机器翻译系统[184]，并在2018年成为BERT的词分词器[23]。WordPiece与BPE有非常相似的思想，通过迭代地合并连续的标记，但在合并的选择标准上略有不同。为了进行合并，它首先训练一个语言模型，并使用它对所有可能的标记对进行评分。然后，在每次合并时，它选择导致训练数据似然性最大增加的标记对。由于Google尚未发布WordPiece算法的官方实现，HuggingFace在其在线NLP课程中提供了一种更直观的选择度量方法：通过将共现计数除以训练语料库中两个标记在该对中的出现计数的乘积来评分标记对。

3）Unigram tokenization：
与BPE和WordPiece不同，Unigram分词从一个足够大的可能子字符串或子标记集合开始，然后迭代地从当前词汇表中删除标记，直到达到预期的词汇表大小。作为选择标准，它通过假设从当前词汇表中删除某个标记，计算训练语料库似然性的增加量。这一步是基于训练过的unigram语言模型进行的。为了估计unigram语言模型，它采用期望最大化（EM）算法：在每次迭代中，首先基于旧语言模型找到当前最佳的单词分词方式，然后重新估计unigram的概率以更新语言模型。在这个过程中，使用动态规划算法（即Viterbi算法）来高效地找到给定语言模型的单词的最佳分解方式。采用这种分词方法的代表性模型包括T5和mBART。

尽管利用现有的分词器（例如，OPT 和GPT-3 利用了GPT-2 的分词器）是方便的，但使用专为预训练语料库设计的分词器可能会带来很大的好处，特别是对于包含多个领域、语言和格式的语料库。因此，最近的语言模型通常使用SentencePiece库为预训练语料库专门训练定制的分词器，其中包括字节级BPE和Unigram分词。需要注意的是，BPE中的规范化技术（例如NFKC ）可能会降低分词性能。当扩展现有的语言模型（即持续预训练或指导微调）时，我们还应该注意定制分词器可能带来的潜在副作用。例如，LLaMA基于主要由英文文本组成的预训练语料库训练BPE分词器，得到的词汇表可能在处理非英文数据方面能力较弱，例如生成中文文本时可能需要更长的推理延迟时间。

subword（BPE 和 wordpiece方法）和传统word-base tokenization优势：
传统词表示方法无法很好的处理未知或罕见的词汇（OOV 问题）； 
传统词 tokenization 方法不利于模型学习词缀之间的关系，例如模型学到的“old”, “older”, and “oldest”之间的关系无法泛化到“smart”, “smarter”, and “smartest”； 
Character embedding 作为 OOV 的解决方法粒度太细； 
Subword 粒度在词与字符之间，能够较好的平衡 OOV 问题； 

BPE 优点：
可以很有效地平衡词典大小和编码步骤数（将语料编码所需要的 token 数量）。
随着合并的次数增加，词表大小通常先增加后减小。迭代次数太小，大部分还是字母，没什么意义；迭代次数多，又重新变回了原来那几个词。所以词表大小要取一个中间值； 

BPE缺点：
对于同一个句子, 例如 Hello world，可能会有不同的 Subword 序列。不同的 Subword 序列会产生完全不同的 id 序列表示，这种歧义可能在解码阶段无法解决； 
在训练任务中，如果能对不同的 Subword 进行训练的话，将增加模型的健壮性，能够容忍更多的噪声，而 BPE 的贪心算法无法对随机分布进行学习； 
《当我的语料库非常大时，无法直接在大语料库上直接训练，需要做一个拆分，到多个子语料库上，那不同子语料库上训练得到的结果，对于同一个word，可能结果会不一样，这可能在解码阶段会有影响，但个人任务影响不大》 
BPE适用范围：
BPE 一般适用在欧美语言拉丁语系中，因为欧美语言大多是字符形式，涉及前缀、后缀的单词比较多。而中文的汉字一般不用 BPE 进行编码，因为中文是字无法进行拆分。对中文的处理通常只有分词和分字两种。理论上分词效果更好，更好的区别语义。分字效率高、简洁，因为常用的字不过 3000 字，词表更加简短。
 
￼
￼
￼
￼


￼
￼

Normalization Methods：
训练不稳定是预训练LLM面临的一个挑战性问题。为了缓解这个问题，归一化是稳定神经网络训练的一种广泛采用的策略。在传统的Transformer模型中，使用了LayerNorm 。最近，还提出了几种高级的归一化技术作为LayerNorm的替代方案，例如RMSNorm和DeepNorm。

1）LayerNorm：
在早期的研究中，BatchNorm 是一种常用的归一化方法。然而，它很难处理可变长度和小批量数据的序列数据。因此，引入了LayerNorm 来进行逐层归一化。具体而言，计算每层激活值的平均值和方差，以重新调整和重新缩放激活值；
—— 处理batchnorm，无法处理可变长度序列，及batchsize偏小时，不稳定的情况；

2）RMSNorm：
为了提高LayerNorm（LN）的训练速度，RMSNorm 提出了一种方法，通过仅使用激活值的均方根（RMS）对激活值进行重新缩放，而不是使用均值和方差。相关研究已经证明了它在Transformer 上的训练速度和性能上的优势。采用RMSNorm的代表性模型包括Gopher 和Chinchilla 。
—— 提升LN训练速度，只进行scale操作，不进行ceter操作，提升训练速度，同时在transformer上性能不存在劣势；

3）DeepNorm：
DeepNorm是由微软提出的，用于稳定深度Transformer的训练的方法。通过使用DeepNorm作为残差连接，Transformer可以扩展到1,000层，这显示了其在稳定性和性能方面的优势。GLM-130B 采用了DeepNorm。

4）Normalization Position：
Post-LN：
在传统的Transformer 中使用了Post-LN，它位于残差块之间。然而，现有的研究发现，由于接近输出层的梯度较大，使用Post-LN的Transformer的训练往往不稳定。因此，除了与其他策略结合（例如，在GLM-130B 中将Post-LN与Pre-LN结合使用）之外，现有的LLM中很少使用Post-LN；
—— 由于transformer输入，和最终输出部分没有进行normalization，导致这两部分存在训练不稳定的情况；
Pre-LN：
与Post-LN不同，Pre-LN 在每个子层之前应用，并在最终预测之前放置了额外的LN。与Post-LN相比，使用Pre-LN的Transformer在训练中更加稳定。然而，与使用Post-LN的变种相比，它的性能较差。尽管性能有所下降，但由于训练的稳定性，大多数LLM仍然采用Pre-LN。然而，有一个例外情况是，在训练超过100B参数的模型时，Pre-LN在GLM中被发现不稳定。
—— 相比post-LN，对normalization的位置进行了调整，使得在transformer部分的训练更加稳定，但相应的由于没有对残差部分进行normalization，对输出部分的正则化约束没Post-LN好，为此性能会差一些；
Sandwich-LN：
基于Pre-LN，Sandwich-LN 在残差连接之前添加了额外的LN，以避免Transformer层输出中的数值爆炸问题。然而，已经发现Sandwich-LN有时无法稳定LLM的训练，并可能导致训练的崩溃。

 Activation Functions：
为了获得良好的性能，前馈网络中的激活函数也需要适当设置。在现有的LLM中，广泛使用GeLU激活函数。特别是在最新的LLM（如PaLM和LaMDA）中，也使用了GLU激活的变种，特别是SwiGLU和GeGLU变种，在实践中通常能够实现更好的性能。然而，与GeLU相比，它们在前馈网络中需要额外的参数（约50%）。


Position Embeddings：
由于Transformer中的自注意力模块是置换等变的，因此使用位置编码（PE）来注入绝对或相对位置信息以建模序列。

1）Absolute position embedding（绝对位置编码）：
在传统的Transformer中，使用绝对位置编码。在编码器和解码器的底部，将绝对位置编码添加到输入编码中。在传统的Transformer中，提出了两种绝对位置编码的变体，即正弦和学习的位置编码，其中后者在现有的预训练语言模型中通常被使用。
—— 以transformer中PE为代表，缺点是限制了模型输入长度，当超过预设值时，PE表示会失效；

2）Relative position embedding（相对位置编码）：
Unlike absolute position embeddings, relative positional embeddings are generated according to the offsets between keys and queries [219]. A popular variant of relative PE was introduced in Transformer-XL [220, 221]. The calculation of attention scores between keys and queries has been modified to introduce learnable embeddings corresponding to relative positions. T5 [73] further simplified relative positional em- beddings, which was subsequently adopted by Gopher [59]. Specifically, it adds learnable scalars to the attention scores, where the scalars are calculated based on the distances between the positions of the query and the key. Compared with the absolute PE, Transformers with relative position embedding can generalize to sequences longer than those sequences for training, i.e., extrapolation [210].
—— 基于query和key的相对位置，学习相对标量r；基于相对位置编码的transformer，在推理时可以生成比训练数据更长的序列，即所谓的外推！
—— 说明下：相对位置编码有一个窗口的限制，当超过窗口时，attention对相对位置的敏感性下降， 此时相对位置可以不做区分，当成固定便可以。

3）Rotary Position Embedding（旋转位置编码）：
旋转位置编码（RoPE）基于每个token的绝对位置设置特定的旋转矩阵。可以使用相对位置信息计算query和key之间的分数。由于其出色的性能和长期衰减特性，RoPE被广泛应用于最新的语言模型，例如PaLM 和LLaMA 。基于RoPE，xPos 进一步改善了Transformer的平移不变性和长度外推。在旋转角度向量的每个维度上，xPos添加了一个特殊的指数衰减，当旋转角度较大时，衰减较小。它可以缓解训练过程中随着距离增加而出现的不稳定现象。 —— 对于超出长度的position，不在固定embed，而是有一个长期的衰减；进一步增长了序列长度；同时可以缓解训练过程中由于距离增加带来的不稳定性。
—— 但是，由于每一层都需要做旋转计算，降低训练和推理速度；

4）ALiBi（线性偏置）：
ALiBi [210]被提出来改善Transformer的外推能力。与相对位置编码类似，它根据key和query之间的距离对注意力分数进行偏置。与T5 等相对位置编码方法不同，ALiBi中的惩罚分数是预定义的，没有任何可训练参数。实证结果表明，与几种流行的位置编码方法（如正弦PE 、RoPE 和T5偏置 ）相比，ALiBi在训练序列之外的更长序列上具有更好的外推性能。此外，已经证明ALiBi还可以改善BLOOM 中的训练稳定性。

—— 为了进一步提升，transformer的外推能力；
—— 说明：ALiBi相对其他位置编码，性能更好，并不是说在训练阶段效果更好，而是在推理时，当序列长度超过训练时长度，外推上的性能更好！！！
—— 另外，由于ALiBi，摒弃了训练参数，整体训练效率会提升，提升训练的稳定性。

——去除PE，采用在attention计算时加入线性偏置技术，在计算query*key值之后，加入一个偏置常量（非训练变量），来达到注入位置信息的效果；该常量为事先计算好，且不同head不同；相对于 Rotary Embedding 计算量更小，对推理性能有显著提升；通过「相对位置信息」就能在一定程度上缓解「绝对位置信息」造成的训练和推理过程中长度编码不一致的问题。 ￼
￼


Architecture：

1）Encoder-decoder Architecture：
The vanilla Transformer model is built on the encoder-decoder architecture, which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder adopts stacked multi-head self-attention layers to encode the input sequence for generating its latent representations, while the decoder performs cross-attention on these representations and autoregressively generates the target sequence. Encoder-decoder PLMs (e.g., T5 and BART ) have shown effectiveness on a variety of NLP tasks. So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, e.g., Flan-T5.
【encoder-decoder架构包括encoder和decoder两种blocks，常见的LLM 包括 T5、BART、Flan-T5】

2）Causal Decoder Architecture：
The causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT-series models are developed based on the causal-decoder architecture. In particular, GPT-3 has successfully demonstrated the effectiveness of this architecture, also showing an amazing in-context learning capability of LLMs. Interestingly, GPT-1 and GPT2 do not exhibit such superior abilities as those in GPT-3, and it seems that scaling plays an important role in increasing the model capacity of this model architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as OPT , BLOOM , and Gopher . Note that both the causal decoder and prefix decoder discussed next belong to decoder-only architectures. When mentioning “decoder only architecture”, it mainly refers to the causal decoder architecture in existing literature, unless specified.
【GPT-3在decoder-only架构上取得显著成功，展示了amazing in-context learning能力，常见LLM 包括GPT系列、OPT、BLOOM、LLaMA等】

3）Prefix Decoder Architecture：
The prefix decoder architecture (a.k.a., non-causal decoder ) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Instead of pre-training from scratch, a practical suggestion is to continually train causal decoders and then convert them into prefix decoders for accelerating convergence, e.g., U-PaLM is derived from PaLM . Existing representative LLMs based on prefix decoders include GLM130B and U-PaLM .
【Prefix LM 通过参数共享的方式对prefix tokens进行full attention，对generated tokens进行causal attention；常见的LLM包括GLM、U-LaLM】

【对于这三种类型的架构，我们还可以考虑通过混合专家（MoE）扩展它们，其中每个输入的神经网络权重的子集被稀疏激活，例如Switch Transformer 和GLaM ；研究表明，通过增加专家的数量或总参数大小，可以观察到显著的性能提升 】
———— Switch Transformer ，在attention计算之后，增加一个Router模块，对token结果路由到不同FNN专家上；

4）Emergent Architectures：
传统的Transformer架构通常受到二次计算复杂性的困扰。因此，在训练和推理长输入时，效率已成为一个重要问题。为了提高效率，一些研究致力于设计用于语言建模的新架构，包括参数化状态空间模型（例如S4 、GSS 和H3 ）、类似Hyena 的长卷积和类似Transformer的架构，其中包含递归更新机制（例如RWKV 和RetNet ）；这些新架构的关键优点有两个。首先，这些模型可以像RNN一样递归生成输出，意味着它们在解码过程中只需要参考单个先前状态。这样做可以提高解码过程的效率，因为它消除了传统Transformer中需要重新访问所有先前状态的需求。其次，这些模型具有像Transformer一样并行编码整个句子的能力。这与传统的逐词编码句子的RNN形成对比。因此，它们可以利用GPU的并行性，结合诸如Parallel Scan 、FFT 和Chunkwise Recurren等技术。这些技术使得具有这些新架构的模型能够以高度并行和高效的方式进行训练；
【当前transformer，在解码过程中，每个tokens都需要重新访问先前所有的状态；而RNN在解码时只用参考前一个状态，但其没有transformer一样并行编码整个句子能力；】
为此transformer虽然训练时候并行化计算性能很高，但解码效率相对就比较低！！
￼

￼

综述论文建议在构建LLM时采用如下结构：
1）pre RMSNorm，同时不作用在embedding层；
2）SwiGLU or GeGLU；
3）RoPE or ALiBi；


Attention：

1）Full attention：
the attention mechanism is conducted in a pairwise way, considering the relations between all token pairs in a sequence；

2）Sparse attention：
A crucial challenge of full attention is the quadratic computational complexity, which becomes a burden when dealing with long sequences；locally banded sparse attention has been adopted in GPT3, Instead of the whole sequence, each query can only attend to a subset of tokens based on the positions；

3）Multi-query attention：
Multi-query attention refers to the attention variant where different heads share the same linear transformation matrices on the keys and values； It can significantly save computation costs； Representative models with multi-query attention include PaLM and StarCoder； 
【不同head里的query，共享keys和values】

4）grouped-query attention (GQA) ：
To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA) has been explored. In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices. Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model；
【以上attention，都是在原始Full attention做近似处理，在模型性能损失 和 attention计算效率提升之间做trade-off，对attention模块就行调整】

￼


5）Flash attention：
FlashAttention proposes to optimize the speed and memory consumption of attention modules on GPUs from an IO-aware perspective. There exist different levels of memory on modern GPUs, e.g., SRAM with a fast IO and HBM with a relatively slow IO. FlashAttention organizes the input into blocks and introduces necessary recomputation, both to make better use of the fast memory SRAM. Implemented as a fused kernel in CUDA, FlashAttention has been integrated into PyTorch, DeepSpeed, and Megatron-LM. The updated version FlashAttention-2 further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2× speedup when compared to the original FlashAttention；
【Flash Attention是在GPU上，对计算速度和内存利用上最优化，充分利用GPU中SRAM 的fast IO计算能力；而Flash Attention 2则是在GPU线程块和warps做进一步优化】
【目前Flash attention已经集成到DeepSpeed、Megatron-LM等Resource中】

6）Paged attention：
PagedAttention partitions each sequence into subsequences, and the corresponding KV caches of these subsequences are allocated into non-contiguous physical blocks. The paging technique increases the GPU utilization and enables efficient memory sharing in parallel sampling；
【PagedAttention将每个序列划分为子序列，并将这些子序列的相应KV缓存分配到非连续的物理块中；分页技术提高了GPU利用率，并在并行采样中实现了高效的内存共享】

【Flash attention 和 Paged attention都是从如何更高效的利用过GPU资源，来提升推理性能，前者基于GPU 不同memory level的IO计算能力，后者基于提升cache利用率】


Pretrain-task【预训练目标】：

￼

1）Language Modeling：
The language modeling task (LM) is the most commonly used objective to pre-train decoder-only LLMs；
【LM在实现上相对较比简单，为此被广泛应用在LLM的预训练中】

2）prefix language modeling task：
which is designed for pre-training models with the prefix decoder architecture. The tokens within a randomly selected prefix would not be used in computing the loss of prefix language modeling. With the same amount of tokens seen during pretraining, prefix language modeling performs slightly worse than language modeling, since fewer tokens in the sequence are involved for model pre-training；
【prefix LM采用auto-regressive blank infilling目标，包括short-span的NLU目标，以及documents level和setence level两个长spans的文本生成目标；本质为多目标训练，训练的难度以及样本构造的难度都很大】【在相同训练tokens情况下，prefix LM效果没有decoder-only好，原因在于用于目标loss训练的tokens较少】

3）Denoising Auto Encoding（DAE）：
In addition to conventional LM, the denoising autoencoding task (DAE) has also been widely used to pre-train language modelsfor DAE task are corrupted text with randomly replaced spans. Then, the language models are trained to recover the replaced tokens；
【去噪自编码任务：就是掩码恢复任务】【然而，相比于语言模型任务，去噪自编码任务在实现上似乎更加复杂（要针对性的构造训练样本）；因此它并没有被广泛用于预训练大型语言模型。目前将去噪自编码作为预训练目标的现有语言模型包括T5 和GLM-130B ；这些模型主要是以自回归的方式训练，以恢复被替换的部分】

4）Mixture-of-Denoisers (MoD)：
MoD regards both LM and DAE objectives as different types of denoising tasks；混合去噪器（MoD），也被称为UL2损失，被引入作为预训练语言模型的统一目标。MoD将LM和DAE目标视为不同类型的去噪任务，即S-去噪器（LM）、R-去噪器（DAE，短跨度和低损坏）和X-去噪器（DAE，长跨度或高损坏）。在这三个去噪任务中，S-去噪器类似于传统的LM目标，而R-去噪器和X-去噪器类似于DAE目标，但在跨度长度和文本损坏比例上有所不同。对于以不同特殊标记（即{[R]，[S]，[X]}）开头的输入句子，模型将使用相应的去噪器进行优化。MoD已经应用于最新的PaLM 2模型中。


Decoding Strategy【解码算法】：
【a specific decoding strategy to generate the appropriate output from the LLMs】【生成返回的文本】
￼
【有研究发现，Random Sampling相比greedy search更容易产生幻觉问题，例如Top-p/Top-k（过高的p和k，会导致diversity过高，进而降低factuality）；为此我们给出一个p/k衰减的算法，例如每生成一个新的句子，重新初始化p进行衰减，并且p的衰减可以定一个下界】

1）greedy search：
predicts the most likely token at each step based on the previously generated tokens；
【贪心算法，每次都选择概率最大的】
【缺点：在每一步选择具有最高概率的标记可能会导致忽视一个整体概率更高但局部估计较低的句子】
【贪婪搜索在文本生成任务（例如机器翻译和文本摘要）中可以实现令人满意的结果，其中输出高度依赖于输入】
【然而，在开放式生成任务（例如故事生成和对话）中，贪婪搜索有时倾向于生成尴尬和重复的句子】
【一种改进策略，就是根据概率分布随机选择下一个标记（Random Sampling），以增强生成过程中的随机性和多样性】

2）Beam search：
Beam search retains the sentences with the n (beam size) highest probabilities at each step during the decoding process, and finally selects the generated response with the top probability. Typically, the beam size is configured within the range of 3 to 6. However, opting for a larger beam size might result in a decline in performance；
【在解码过程中保留每一步中具有前 n（束宽度）最高概率的句子，并最终选择具有最高概率的生成回复；通常beam size配置在3到6的范围内，然而选择更大的beam size可能会导致性能下降】
【beam search偏向于较短的句子】

3）Length penalty【长度惩罚】：
Since beam search favours shorter sentences, imposing length penalty (a.k.a., length normalization) is a commonly used technique to overcome this issue, which normalizes the sentence probability according to the sentence length (divided by an exponential power α of the length)；
【由于beam search偏向于较短的句子，为了克服这个问题生成更长的句子，常用的技术是引入长度惩罚（也称为长度归一化），根据句子长度对句子概率进行归一化处理（除以长度的指数幂α）】

4）previously generated tokens or n-grams penalty：
some researchers propose to penalize the generation of previously generated tokens or n-grams to alleviate the issue of repetitive generation. In addition, diverse beam search can be leveraged to produce a set of diverse outputs based on the same input.
【一些研究者提出对先前生成的标记或 n-gram 进行惩罚，以减轻重复生成的问题；此外多样化束搜索可以基于相同的输入产生一组多样化的输出】

5）Improvement for Random Sampling：
1、adjust the temperature：
To modulate the randomness of sampling, a practical method is to adjust the temperature coefficient of the softmax function for computing the probability of the j-th token over the vocabulary；
【就是调整temperature取值，来调整softmax返回的结果；temperature设置越小，使得softmax返回区分度越大，降低diversity提升factuality】
2、Top-k sampling：
Different from temperature sampling, top-k sampling directly truncates the tokens with lower probability and only samples from the tokens with the top k highest probabilities；
【就在top-k中做Random Sampling】
3、Top-p sampling：
Since top-k sampling does not consider the overall possibility distribution, a constant value of k may be not be suitable for different contexts. Therefore, top-p sampling (a.k.a., nucleus sampling) is proposed by sampling from the smallest set having a cumulative probability above (or equal to) p . In practice, the smallest set can be constructed by gradually adding tokens from the vocabulary sorted in descending order of generative probability, until their cumulative value exceeds p；
【top-p，及在所有候选word中，找到累计概率超过p的最小word集合，在这当中做Random Sampling】
【有研究发现，Random Sampling相比greedy search更容易产生幻觉问题，例如Top-p/Top-k（过高的p和k，会导致diversity过高，进而降低factuality）；为此我们给出一个p/k衰减的算法，例如每生成一个新的句子，重新初始化p进行衰减，并且p的衰减可以定一个下界】

—————— 「以上时常见的解码算法，包括greedy search、beam search、length penalty、Random Sampling（及相关变中）」


6）Efficient Decoding Strategies（高效的解码策略）：
考虑到LLM中自回归生成的本质，随着序列长度的增加，生成过程所需的时间逐渐增加。因此一些研究工作探索了加速解码过程的方法；

投机采样（Speculative decoding）：
首先利用紧凑但高效的模型（例如n-gram模型或小型PLM）生成短片段，然后利用LLM验证和纠正这些草稿；这种方法可以显著提高2倍到3倍的速度，而不会影响生成质量；
【先生成小的片段，再对这些片段进行验证和纠正】

SpecInfer进一步提出了一种基于学习的方法，将几个小模型组合起来，以增加覆盖的可能性。此外还提出了基于标记的早停技术，使得可以在较低的Transformer层生成一个标记，而不是经过所有层次；这可以获得更大的加速，但以牺牲生成质量为代价；


常见模型的解码算法配置：
￼
OpenAI API supports several basic decoding strategies, including greedy search (by setting temperature to 0), beam search (with the setting best_of), temperature sampling (with the setting temperature), nucleus sampling (with the setting top_p). It also introduce parameters presence_penalty and frequency_penalty to control the repetition degree of generation. According to the OpenAI’s document, their APIs would produce different outputs even if the input and the hyper-parameters are the same. Setting temperature to 0 can yield more deterministic outputs, albeit with a slight chance of variability。


Architecture Choice：

1）By pre-training with the LM objective, it seems that causal decoder architecture can achieve a superior zeroshot and few-shot generalization capacity. Existing research has shown that without multi-task fine-tuning, the causal decoder has better zero-shot performance than other architectures. The success of GPT-3 has demonstrates that the large causal decoder model can be a good fewshot learner. In addition, instruction tuning and alignment tuning discussed in Section 5 have been proven to further enhance the capability of large causal decoder models.
【LM目标，可以使模型获得更好的zero-shot 和 few-shot 的in-context learning能力】【同时采用instruction tuning和alignment tuning 可以使deocder-only的模型在人类意图理解、helpful、honesty、harmless等能力有明显的提升】

2）Scaling law has been widely observed in causal decoders. By scaling the model size, the dataset size, and the total computation, the performance of causal decoders can be substantially improved. Thus, it has become an important strategy to increase the model capacity of the causal decoder via scaling. However, more detailed investigation on encoder-decoder models is still lacking, and more efforts are needed to investigate the performance of encoder-decoder models at a large scale. 
【当前已经有足够的结果支持decoder-only的架构，在增加模型大小、训练数据集大小，增加算力，可以带来效果的显著提升；而这方面的研究在encoder-decoder上暂时还没有足够的结论支持】

-- 如果只是做某些特定性任务，那encoder-decoder、perfix LM架构是完全可以了，尤其是在一些涉及到NLU相关的任务上面，性能是在decoder-only之上的；


Long Context：

One of the drawbacks of Transformer-based LMs is the limited context length due to the quadratic computational costs in both time and memory. Meanwhile, there is an increasing demand for long context windows in applications such as PDF processing and story writing. A variant of ChatGPT with 16K tokens as the context window has recently been released, which is much longer than the initial 4K tokens. Additionally, the context window of GPT-4 has been extended to 32K tokens. Next, we discuss two important factors related to long context modeling.
【输入context 的长度由于影响模型推理的性能（2次关系）以及占用更多内存，导致LM context一直受限制；但真实世界中一直都有对超长文本输入的需求；目前chatgpt文本输入长度为16k，gpt-4为32k】

Extrapolation：In real-world applications, it is possible for LLMs to process long input texts that exceed the maximum length of the training corpus. The ability of encoding longer texts is often referred to as extrapolation capability. Several position embedding methods, such as RoPE and T5 bias, have been empirically validated to possess certain extrapolation capabilities. Specifically, LMs equipped with ALiBi have been shown to maintain relatively stable perplexity on sequences even ten times longer than those for training. There are also efforts like xPos to enhance the extrapolation ability of RoPE by improving the design of rotation matrix.
【推理可以跨度训练文本长度的限制，称为extrapolation，RoPE和ALiBi在这方面均展示出不错的效果】

Efficiency：In order to reduce the quadratic computational cost in attention modules, several studies design highly efficient attention computation methods that can make the memory consumption scales approximately linearly, exemplified by sparse or linear attentions . In addition to the algorithmic improvements, another important work, FlashAttention , improves the efficiency from a system-level perspective (i.e., GPU memory IO efficiency). With the same computing budget, one can train LLMs with longer context windows. 
【为了降低平方复杂度计算量，在attention算法（sparse attention），gpu效率（flash attention）等方面有很多研究，使得模型可以训练更长的文本】


Stabilizing the Training：

During the pre-training of LLMs, it often suffers from the training instability issue, which may cause the model collapse. To address this issue, weight decay and gradient clipping have been widely utilized, where existing studies commonly set the threshold of gradient clipping to 1.0 and weight decay rate to 0.1. However, with the scaling of LLMs, the training loss spike is also more likely to occur, leading to unstable training. To mitigate this problem, PaLM and OPT use a simple strategy that restarts the training process from an earlier checkpoint before the occurrence of the spike and skips over the data that may have caused the problem. Further, GLM finds that the abnormal gradients of the embedding layer usually lead to spikes, and proposes to shrink the embedding layer gradients to alleviate it.
【针对大模型训练稳定性问题，可以采用剃度裁剪（1.0），和权重衰减（0.1）的方法；另外跳过异常数据，以及embedding层梯度缩小也是两种可能的方式】


Scalable Training Techniques：
overall：当前开源库，例如DeepSpeed已经集成了3D Parmllelism、ZeRO、激活重计算、混合精度等；
￼

1）3D Parallelism：
1、Data parallelism【数据并行】：
它将模型参数和优化器状态复制到多个GPU上，然后将整个训练语料库分发到这些GPU上。这样，每个GPU只需要处理分配给它的数据，并执行前向和反向传播以获得梯度。在不同GPU上计算的梯度将进一步聚合，以获得整个批次的梯度，再新所有GPU上的模型；
2、Pipeline parallelism【模型并行】：
将LLM的不同层分布到多个GPU上。 特别是在Transformer模型的情况下，将连续的层加载到同一个GPU上，以减少在GPU之间传输计算的隐藏状态或梯度的成本。然而，一个简单的Pipeline parallelism实现可能会导致较低的GPU利用率，因为每个GPU都必须等待前一个GPU完成计算，从而导致不必要的bubbles overhead。为了减少Pipeline parallelism 中的这些bubbles，GPipe 和PipeDream 提出了填充多个数据批次和异步梯度更新的技术，以提高管道效率；
3、Tensor parallelism：
tensor parallelism focuses on decomposing the tensors (the parameter matrices) of LLMs. For a matrix multiplication operation Y = XA in the LLM, the parameter matrix A can be split into two submatrices, A1 and A2, by column, which can be expressed as Y = [XA1, XA2]. By placing matrices A1 and A2 on different GPUs, the matrix multiplication operation would be invoked at two GPUs in parallel, and the final result can be obtained by combining the outputs from the two GPUs through across-GPU communication；
【简单来讲就是对transformer中的矩阵计算，分成几个较小矩阵计算，这样就可以并行拆分到不同的gpu上】

2）ZeRO：
ZeRO技术是由DeepSpeed库提出的，专注于数据并行中的内存冗余问题。如前所述，数据并行要求每个GPU存储LLM的相同副本，包括模型参数、模型梯度和优化器参数。然而，并不是所有的数据都需要在每个GPU上保留，这会导致内存冗余问题。为了解决这个问题，ZeRO技术旨在在每个GPU上只保留部分数据，其余数据可以在需要时从其他GPU中检索。具体而言，ZeRO提供了三种解决方案，取决于数据的三个部分是如何存储的，即优化器状态分区、梯度分区和参数分区。实证结果表明，前两种解决方案不会增加通信开销，而第三种解决方案增加了约50%的通信开销，但节省了与GPU数量成比例的内存。PyTorch已经实现了类似于ZeRO的技术，称为FSDP。
【从方案上看，ZeRO的带来的计算能力的提升和GPU数量成正比，是一个非常靠谱的技术；ZoRO提出三种方式，即优化器分区、梯度分区、参数分区；前两者不会带来额外通讯开销，但第三者会增加50%通讯开销】

3）Mixed Precision Training：
在以前的PLM（例如BERT）中，主要使用32位浮点数，也称为FP32，进行预训练。近年来，为了预训练非常大的语言模型，一些研究开始使用16位浮点数（FP16），这可以减少内存使用和通信开销。此外，由于流行的NVIDIA GPU（例如A100）的FP16计算单元数量是FP32的两倍，FP16的计算效率可以进一步提高。然而，现有的研究发现FP16可能会导致计算精度的损失，从而影响最终的模型性能。为了缓解这个问题，一种叫做Brain Floating Point (BF16)的替代方法已经被用于训练，它分配了比FP16更多的指数位和较少的有效位。对于预训练来说，BF16通常在表示准确性方面优于FP16。
【目前LLM训练主要采用FP16精度，但可能会带来精度损失，影响模型性能的同时，甚至可能使训练不稳定（在训练中计算产生了溢出或者下溢，导致产生异常梯度）；为此可以采用BF16替换，但BF16在不同平台适配性较差】

在实践中，上述训练技术，特别是3D并行技术，通常会联合使用以提高训练吞吐量和大模型加载速度。例如，研究人员已经将8路数据并行、4路张量并行和12路流水线并行等技术应用到BLOOM 的训练中，使其能够在384个A100 GPU上进行训练。目前，像DeepSpeed 、Colossal-AI 和Alpa 这样的开源库可以很好地支持这三种并行训练方法。为了减少内存冗余，还可以使用ZeRO、FSDP和激活重计算技术 来训练LLM，这些技术已经集成到DeepSpeed、PyTorch和Megatron-LM中。此外，还可以利用BF16等混合精度训练技术来提高训练效率和减少GPU内存使用，但这需要硬件的必要支持（例如A100 GPU）。由于训练大模型是一个耗时的过程，因此在早期阶段预测模型性能并检测异常问题将非常有用。为此，GPT-4 最近引入了一种称为可预测扩展的新机制，该机制建立在深度学习堆栈上，可以使用一个更小的模型对大模型的性能进行预测，这对于开发LLM可能非常有用。在实践中，可以进一步利用主流深度学习框架的支持训练技术。例如，PyTorch支持数据并行训练算法FSDP （即完全分片数据并行），如果需要，可以将部分训练计算卸载到CPU上。


Instruction Tuning：

本质上，指令调优是在以自然语言形式的格式化实例集合上对预训练的LLM进行微调的方法，这与监督微调和多任务提示训练密切相关。为了进行指令调优，我们首先需要收集或构建格式化的指令实例。然后，我们使用这些格式化的实例以监督学习的方式对LLM进行微调（例如，使用sequence to sequence loss进行训练）。经过指令调优后，LLM可以展示出在未见任务上的优越泛化能力，甚至在多语言环境下也是如此；与此相比，我们主要关注指令调优对LLM的影响，并提供详细的实例收集和调优指南或策略。此外，我们还讨论了使用指令调优来满足用户的实际需求，这在现有的LLM中已经广泛应用，例如InstructGPT和GPT-4。
【采用自然语言形式，在LLM上以监督学习的方式进行微调（采用sequence to sequence loss），由此让LLM理解人类自然语言，在未见任务和多语言环境下，获取优越的泛化能力；】

在指令构造上遵循如下原则：
1、指令描述足够清晰：除了“背景、目标、约束”等之外，还可以增加“身份”信息，提升LLM对任务的理解；另外增加清晰的“示例”，可以改善模型性能，减少对prompt engineering的敏感性；
2、指令数据要多样性：除了在开源数据集上转化得到数据，还需要加入更多样性，更符合人类真实需求的指令，例如“开放式对话、开放式问答、头脑风暴、对话”等；
3、增加指令数量：尤其是一些多样性、创造性，带有推理的任务（COT指令），有助于提升模型在各种任务上性能，但需要控制好不同任务下指令数量（不超过几百），以防止过拟合；
5、高质量指令数据：采用现有LLM模型，对指令数据进行打分，过滤掉低分指令数据；
6、控制任务间数据比例：增加高质量数据集比例， 有利于提升模型性能；单数据集设置数据量限制，一般控制在几千～几万之间；
7、分阶段微调：微调阶段指令，可以分阶段逐步增加难度和复杂度；先微调通用场景下指令，再微调特定场景（对话）下指令数据，为防止模型遗忘，二阶段可以加入部分一阶段指令；
8、微调loss：通常为sequence-to-sequence loss；在对话场景下，考虑生成对话重复性，可以采用只在对话respose上，进行DAE目标学习；
9、预训练阶段：将微调指令数据和预训练数据结果，进行多目标训练，提升预训练LLM模型的基本能力；


基于较小数据量的指令微调数据，可以明显提升模型的性能，这比增加预训练数据要有效；
指令微调，赋予了模型理解人类自然语言指令的能力，使之可以遵循人类指令执行特定任务；同时对于缓解特定弱点（生成重复，任务未完成时进行补充输入等），同时指令微调具备语言间任务迁移能力；
指令微调，是将通用LLM转向特定领域专家LLM的有效方法；
提升指令的多样性和质量，比增加实例的数量更重要，因为表现良好的InstructGPT 和Alpaca 比Flan系列的LLM 使用更少但更多样化的指令（或实例）；
训练目标（sequence-to-sequence loss）和优化配置（如较小的batch_size和learning rate）；
GPT-4对于潜在风险指令，通过监督学习微调，引导模型拒绝回答哦这些指令，以确保安全性；

1）Formatted Instance Construction：
￼
￼

上图给了三种构造格式化示例指令的主要方法。
1、Formatting Task Datasets：
基于早期的研究收集了来自各种任务（如文本摘要、文本分类和翻译）的实例，创建了监督多任务训练数据集，将这些多任务训练数据集与自然语言任务描述进行格式化，即人工撰写的任务描述来扩充带标签的数据集，这些描述通过解释任务目标来指导LLM理解任务。例如，在图9(a)中，针对问答任务的每个示例都添加了一个任务描述“请回答这个问题”。经过指令调优后，LLM可以通过遵循任务描述很好地推广到其他未见任务；特别是已经证明指令是LLM任务泛化能力的关键因素；
【通过增加自然语言任务描述的方式，来指导LLM理解任务；微调之后，LLM可以遵循任务描述很好的推广到其他未见的任务】

2、Formatting Daily Chat Data：
尽管已经使用指令对大量的训练实例进行了格式化，但它们主要来自公共的自然语言处理数据集，要么缺乏指令多样性，要么与真实人类需求不匹配。为了解决这个问题，InstructGPT 提出使用用户提交给OpenAI API的查询作为任务描述。用户查询用自然语言表达，特别适合用于引发LLM的指令跟随能力。此外，为了丰富任务多样性，还要求人工标注者为真实生活任务编写指令，包括开放式生成、开放式问题回答、头脑风暴和聊天。然后，他们让另一组标注者直接回答这些指令作为输出。最后，将一个指令（即收集到的用户查询）和预期输出（即人工编写的答案）配对作为一个训练实例。需要注意的是，InstructGPT还使用这些以自然语言格式化的真实世界任务进行了对齐调优。此外，GPT-4 设计了潜在高风险的指令，并通过监督微调引导模型拒绝这些指令，以确保安全性。最近，研究人员还收集了用户的聊天请求作为输入数据，并使用ChatGPT或GPT-4作为输出数据来回应这些请求。这样的数据集代表是来自ShareGPT的对话数据。

【为了提升指令的多样性，更符合人类真是需求，除了在开源数据集上，还采用open API和人工标注获取更为丰富多样性指令，包括一些开放性生成、开放性问答、头脑风暴、聊天等】
【将一个指令（openAPI或者人工编写）和预期输出（人工编写）构成一个训练数据，采用类似sequence-to-sequence loss的方式进行微调训练】
【GPT-4对于潜在风险指令，通过监督学习微调，引导模型拒绝回答哦这些指令，以确保安全性】

3、Formatting Synthetic Data：
为了减轻人工注释或手动收集的负担，已经提出了几种半自动化的方法，通过将现有实例输入到LLM中来合成多样化的任务描述和实例。如图9(c)所示，Self-Instruct方法只需要大约100个实例作为初始任务池。然后，他们从任务池中随机选择几个实例作为示范，并提示LLM生成新的指令和相应的输入-输出对。经过质量和多样性过滤后，新生成的实例将被添加到任务池中。因此，合成方法是为LLM生成大规模指令数据的一种有效和经济的方式；
【采用self-instruct，结合LLM进行自动化多样化指令（任务描述 + 输入 + 输出）生成，结合质量过滤和多样性过滤后，添加到任务池中；】


一些结论：
1、Scaling the instructions：
已经广泛证明，扩展任务数量可以大大增强LLM的泛化能力，随着任务数量的增加，模型性能最初呈现连续增长的趋势，但当达到一定水平时，增益变得微不足道。一个合理的推测是，一定数量的代表性任务可以提供相对充足的知识，添加更多任务可能不会带来额外的收益。此外，从长度、结构和创造力等多个方面增强任务描述的多样性也是有益的。至于每个任务的实例数量，已经发现少量实例通常可以饱和模型的泛化性能。然而，将某些任务的实例数量增加到很大（例如几百个）可能会导致过拟合问题并损害模型性能；
【增加不同类型的任务，可以带来模型性能提升，尤其是一些多样性、创造性、带有推理的任务（包括COT），不同任务的实例不需要太多，太多（几百个）就有可能会造成过拟】

2、Formatting design：
自然语言格式的设计也极大地影响了LLM的泛化性能；通常情况下，我们可以将任务描述和可选示范添加到现有数据集的输入-输出对中，其中任务描述是LLM理解任务的关键部分。此外，通过使用适当数量的示范，还可以显著改善模型的性能，这也减轻了模型对指令工程的敏感性。然而，将其他组件（例如要避免的事情、原因和建议）纳入指令中可能对LLM的性能产生微不足道甚至逆向的影响。最近，为了引出LLM的逐步推理能力，一些工作提议在一些推理数据集（如算术推理）中包含思维链（CoT）示例。已经证明，使用CoT和非CoT示例对LLM进行微调可以在各种推理任务中取得良好的性能，包括那些需要多跳推理能力的任务（如常识问题回答和算术推理），以及那些不需要这种推理方式的任务（如情感分析和抽取式问题回答）。
【自然语言输入的格式也很重要，准确的输入“任务描述”和“相关实例”，可以改善模型性能，减轻对指令工程的敏感性；另外增加COT指令，可以让模型取得非常好的性能，包括在推理任务和其他各种任务上】

总结一下，提升指令的多样性和质量，比增加实例的数量更重要，因为表现良好的InstructGPT 和Alpaca 比Flan系列的LLM 使用更少但更多样化的指令（或实例）。此外，邀请标注者来组成人类需求任务比使用特定于数据集的任务更有用。然而，目前还缺乏关于注释人类需求实例的通用指导方针，使得任务组成在某种程度上具有启发式。为了减少人力投入，我们可以重复使用现有的格式化数据集（表6），或者使用现有的LLM 自动构建指令。


2）Instruction Tuning Strategies：

与预训练不同，指令微调通常更高效，因为只使用了适量的实例进行训练。由于指令微调可以被视为一种监督训练过程，其优化在几个方面与预训练不同，例如训练目标（sequence-to-sequence loss）和优化配置（如较小的batch_size和learning rate），这些在实践中需要特别注意。除了这些优化配置外，还有四个重要方面需要考虑指令微调：

1、Balancing the Data Distribution：
由于指令微调涉及到不同任务的混合，因此在微调过程中平衡不同任务的比例非常重要。一种广泛使用的方法是例子比例混合策略，即将所有数据集组合起来，并从混合数据集中均匀采样每个实例。此外，根据最近的研究结果，增加高质量数据集（如FLAN和P3）的采样比例通常可以提高性能。此外，在指令微调过程中通常会设置一个最大限制，用于控制数据集在指令微调过程中可以包含的最大示例数，这是为了防止较大的数据集对整个分布造成过大影响。在实践中，根据不同的数据集，最大限制通常设置为几千或几万个示例；
【控制不同任务之间比例，增加高质量数据集比例，有利于性能提升；对于单个数据集一般设置数量限制，例如几千或者几万便可；】

2、Combining Instruction Tuning and Pre-Training：
为了使微调过程更加有效和稳定，OPT-IML在指令微调过程中引入了预训练数据，可以看作是模型微调的正则化。此外，一些研究尝试使用多任务学习的方法，而不是使用独立的两阶段过程（预训练然后指令微调），从头开始训练一个模型，使用预训练数据（即纯文本）和指令微调数据（即格式化数据集）的混合。具体而言，GLM-130B 和Galactica 将指令格式化的数据集作为预训练语料库的一小部分，用于预训练LLM，这可能同时实现了预训练和指令微调的优势；
【在预训练时，在训练数据集中，加入指令微调数据，进行多任务学习，前者为LM目标，后者为sequence-to-sequence目标】

3、Multi-stage Instruction Tuning：
对于指令微调，有两种重要的指令数据，即任务格式化指令和日常聊天指令。通常，前者的数量要远远大于后者。在训练过程中平衡这两种指令数据非常重要。除了仔细混合不同的指令数据之外，我们还可以采用多阶段的指令微调策略，其中LLMs首先使用大规模的任务格式化指令进行微调，然后再微调日常聊天指令。为了避免容量遗忘问题，在第二阶段还可以添加一定数量的任务格式化指令。实际上，这种多阶段微调策略也可以应用于指令微调的其他设置。例如，我们可以安排不同的微调阶段，逐渐增加难度和复杂度，并逐步提高LLMs跟随复杂指令的能力；
【分阶段：LLM首先使用大规模的任务格式化指令进行微调，再在日常聊天指令上进行微调；为避免“遗忘”，在第二个阶段可以加入一定数量的任务格式化指令】
【微调阶段的指令，可以分阶段逐步增加难度和复杂度】

———— 由此引出，在垂直领域指令数据，需要和通用领域格式化指令数据进行融合，后者和前者比例2:1或者3:1；并且在通用领域指令先微调，再在垂直领域进行微调（加入部分通用领域指令）；
———— 一般指令微调采用sequence-to-sequence的训练目标，在chat多轮对话语料上，则采用MLM目标，以减少在重叠话术计算成本；

4、Other Practical Tricks：
Efficient training for multi-turn chat data：
给定一个多轮对话示例（用户和聊天机器人之间的对话），一种直接的微调方法是将其拆分为多个上下文-回复对进行训练：LLM根据相应的上下文对所有拆分（即每个用户的话语）进行微调以生成回复。在这种微调方式中，显然在对话的拆分示例中存在重叠的话语。为了节省训练成本，Vicuna 采用了一种高效的方式，将整个对话输入LLM，但依赖于仅在训练时计算聊天机器人回复的损失掩码。这可以显著减少由重叠话语导致的计算成本。
【对于多轮对话的指令数据，为存在避免重叠的话语，只在respose上进行 MLM loss进行训练，由此来减少重叠话术计算成本】

Filtering low-quality instructions using LLMs：
在指令数据收集之后，往往会包含低质量的指令，这可能会降低模型性能并增加训练成本。为了解决这个问题，现有的工作通常采用强大的LLMs（例如ChatGPT和GPT-4）来注释一部分指令。它利用像“确定对于一个目标是学习世界知识的学生来说，它的教育价值是多少”这样的提示来指导LLM对指令的质量进行注释，例如高、中和低。然后，这些由LLM注释的指令将用于训练一个分类器来预测所有剩余指令的质量，并最终过滤掉被预测为低质量的指令；
【通过现在强大的LLMs，对微调指令数据进行评分，并过滤掉低质量分数的指令】

Establishing self-identification for LLM：
为了将LLMs部署到实际应用中，有必要建立其身份并让LLMs意识到这些身份信息，例如名称、开发者和所属机构。一种实际的方法是为微调LLM创建与身份相关的指令。还可以在输入前加上自我识别提示，例如“以下是一个人类和一个名为CHATBOTNAME的AI助手之间的对话，由DEVELOPER开发。”其中CHATBOTNAME和DEVELOPER分别指代聊天机器人的名称和开发者；
【在微调指令数据中，在任务描述部分，除了背景、目标、约束等信息之外，还可以增加“身份”类信息，来提升LLM对指令的理解识别；】


3）The Effect of Instruction Tuning：
1、Performance Improvement：
指令微调已成为改进或发挥LLMs能力的重要方式，最近的研究在多个规模的语言模型（从77M到540B）上进行了实验，表明不同规模的模型都可以从指令微调中受益，随着参数规模的增加，性能得到了改善。此外，经过指令微调的较小模型甚至可以比没有微调的较大模型表现更好。除了模型规模，指令微调还在各种模型架构、预训练目标和模型适应方法中展示了一致的改进。在实践中，指令微调提供了一种增强现有语言模型能力的通用方法（包括小型PLMs）。而且，与预训练相比，指令微调成本更低，因为LLMs所需的指令数据量明显小于预训练数据量；
【基于较小数据量的指令微调数据，可以明显提升模型的性能，这比增加预训练数据要有效】

2、Task Generalization：
指令微调鼓励模型理解自然语言指令以完成任务。它赋予LLMs遵循人类指令执行特定任务的能力（通常被认为是一种新兴的能力），即使在未见过的任务上，也无需演示。大量研究已证实指令微调在实现在已知和未知任务上的卓越性能方面的有效性。此外，指令微调已被证明对缓解LLMs的几个弱点（例如重复生成或补充输入而未完成特定任务）非常有用，从而使LLMs具备解决实际任务的优越能力。此外，通过指令微调训练的LLMs可以在不同语言之间推广到相关任务。
例如，BLOOMZ-P3 基于BLOOM 使用仅英语的任务集合P3 进行微调的。有趣的是，与BLOOM相比，BLOOMZ-P3在多语言句子完成任务上的表现提高了50%以上，这表明指令微调可以帮助LLMs从仅有英语数据集中获得一般任务技能，并将这些技能转移到其他语言。此外，研究发现，使用仅有英语的指令在多语言任务上可以产生令人满意的结果，这有助于减少特定语言的指令工程的工作量。
【指令微调，赋予了模型理解人类自然语言指令的能力，使之可以遵循人类指令执行特定任务；同时对于缓解特定弱点（生成重复，任务未完成时进行补充输入等），同时指令微调具备语言间任务迁移能力】

3、Domain Specialization：
现有的LLMs在传统的自然语言处理任务（如生成和推理）和日常问题中展示了卓越的能力。然而，它们可能仍然缺乏完成特定任务所需的领域知识，例如医学、法律和金融。指令微调是将现有的通用LLMs调整为特定领域专家的有效方法。例如，研究人员提出使用医学数据集对Flan-PaLM进行微调，创建了医学知识助手Med-PaLM ，其性能水平可与专业临床医生相媲美。此外，最近的一项研究对FLAN-T5进行微调，以支持基于自然语言指令的电子商务推荐系统，在各种推荐任务中展现出强大的性能。还有一些基于LLaMA 的开源医学模型进行了指令微调，例如BenTsao 。此外，研究人员还探索了在法律、金融和算术计算方面的指令微调。
【指令微调，是将通用LLM转向特定领域专家LLM的有效方法】

【pretrain，就像在海纳百川般的接受各种知识，但LLM实际上可能并不是很理解这些知识；或者没有通，只是貌似记住了知识，可以做一些机器式交互】
【Instruction Tuning，就像一个teacher，以人类自然语言的形式，结合指令实例，告诉模型应该如何回答，如何处理；这种激发出来的能力，可以使得模型在其他一些未知任务具备了处理能力】【有点类似举一反三】


4）Improvement Strategies：
1、Enhancing the instruction complexity；
2、Increasing the topic diversity；
3、Scaling the instruction number；
4、Balancing the instruction difficulty；

5）Results and Analysis：
1、Task-formatted instructions are more proper for the QA setting, but may not be useful for the chat setting；
2、A mixture of different kinds of instructions are helpful to improve the comprehensive abilities of LLMs；
3、Enhancing the complexity and diversity of instructions leads to an improved model performance；
4、Simply increasing the number of instructions may not be that useful, and balancing the difficulty is not always helpful；
5、A larger model scale leads to a better instruction following performance；


Alignment Tuning：

LLMs在各种自然语言处理任务中展示了卓越的能力。然而，这些模型有时可能会出现意想不到的行为，例如编造虚假信息、追求不准确的目标以及产生有害、误导性和偏见的表达。对于LLMs，语言建模目标通过单词预测来预训练模型参数，但缺乏对人类价值观或偏好的考虑。为了避免这些意外行为，提出了人类对齐的方法，使LLMs与人类期望一致行动。然而，与原始的预训练和调整微调（例如指令微调）不同，这种对齐需要考虑非常不同的标准（例如有益性、诚实性和无害性）。研究表明，对齐可能在一定程度上损害LLMs的通用能力，这在相关文献中被称为对齐税；

1、Helpfulness：为了具有helpful，LLMs应该以尽可能简洁高效的方式，清晰地尝试帮助用户解决任务或回答问题。在更高的层次上，当需要进一步澄清时，LLMs应该展示通过相关的询问获取额外相关信息的能力，并展示适当的敏感度、洞察力和谨慎性。实现helpful行为的对齐对LLMs来说是具有挑战性的，因为很难准确定义和衡量用户的意图；

2、Honesty：在基本层面上，一个对齐于honest的LLM应该向用户呈现准确的内容，而不是编造信息。此外，LLM在输出中传达适当的不确定度程度非常重要，以避免任何形式的欺骗或误传信息。这要求模型了解自己的能力和知识水平（例如，“知道未知”）。诚实是一个相对更客观的标准，与有益性和无害性相比，诚实对齐可能在较少依赖人力的情况下得以发展；

3、Harmlessness：模型产生的语言不应该具有冒犯性或歧视性。在其能力范围内，模型应该能够检测到旨在诱使恶意请求的隐秘企图。理想情况下，当模型被诱导执行危险行为（例如犯罪行为）时，LLM应该礼貌地拒绝。然而，被认为有害的行为及其程度在个人或社会之间存在差异，这在很大程度上取决于谁在使用LLM、提出的问题类型以及LLM的使用背景（例如时间）。


【pretrain，以LM为目标，进行文本生成训练，就像在海纳百川般的接受各种知识，但LLM实际上可能并不是很理解这些知识；或者没有通，只是貌似记住了知识，可以做一些机器式交互；】
———— 海纳百川各种学；
【Instruction Tuning，以sequence-to-sequence为目标，以人类自然语言的指令实例，让模型理解人类自然语言，以获取强大泛化能力；这种激发出来的能力，可以使得模型在其他一些未知任务具备了处理能力】【有点类似举一反三】
———— 理解人类自然语言，具备完成人类自然语言形式的任务，包括对话；
【Alignment Tuning，对齐人类价值观或者偏好，考虑不同的标准（例如helpful、honesty、harmless），以使LLM返回结果与人类期望一致，从目标上看，Alignment标准看上去更难，不论是数据的构建，还是训练微调方式】
———— 进一步实现人类价值偏好的对齐；

【Instruction tuning指的是在传统自然语言任务上，构造指令-示例数据，微调得到的LLM；而SFT则是在开放式对话场景下，通过构造对应的指令-示例数据，微调得到的LLM；】

￼

RLHF vs SFT：
从实现复杂度上看RLHF整体明显高于SFT，但从结果有效性和稳定性上看，RLHF也是优于SFT；
从方式上看，RLHF流程包括prompt指令选择，SFT微调，RM模型训练，PPO训练，超参数选择等，其中RL部分还存在训练不稳定、受参数初始化影响等；整体复杂度明显超过纯SFT，但训练完成后，整体效果明显好于纯SFT训练效果；

训练数据：
SFT，完全依赖监督学习标注“高质量数据”，但不同标注者存在个体偏好、风格等差异，导致数据间存在差异性；要求的数据量也比较高； 
RLHF，其SFT阶段采用Instruction tuning方式（13K训练数据），RM阶段采用偏好排序的方式，来降低不同标注者之间由于存在个体偏好、风格等差异，带来的差异性影响（33k训练数据）； 

训练目标：
SFT，采用监督学习方式，去学习“高质量数据集”中的对齐信息，一般来讲采用sequence-to-sequence loss + 辅助目标的方式，所以在输出结果上，依然是token level局部优化（step by step解码）； 
RLHF，人类反馈强化学习，通过构建初始化较好SFT模型和RM模型，基于PPO算法训练LLM，目标上时最大化RM的reward，最小化文本差异性，以及LM，三重目标；在输出结果上，是一个text-level的全局最优（step by step reward）； 

训练方式：
SFT，采用监督学习，能学习达到的性能上限，就是标注“高质量数据集”，基本不具备探索标注数据之外的更高质量的文本，有可能会带来LLM幻觉问题； 
RLHF，强化学习，本质上就是在reward指导下，去学习正确的文本生成策略，而不是模仿标注数据；在训练过程中具备探索发现更多高质量文本的机会；前者有助于减少幻觉问题，后者可以提升harmless和helpful能力； 

对LLM影响：
SFT，带来一定的人类价值对齐和偏好的提升，同时会有可能带来更严重幻觉问题；————有研究发现SFT并不给LLM注入新的能力，只是通过指令微调让LLM的能力展现出来；那么当我们通过SFT希望让LLM具备非内生能力的时候（超出LLM知识范围），有可能会产生幻觉问题，从而影响LLM honesty的准确性；为此通过指令微调方式，获得的领域LLM，本质上也存在幻觉问题； 
RLHF，以及被各方面证实，在提升helpful、harmless、honesty均有明显的帮助； 

总体而言，SFT可以看成是对预训练LLM性能优化，而RLHF则是对SFT的进一步性能优化；对应的RLHF训练难度明显高于SFT；

 Reinforcement Learning from Human Feedback：
￼
￼

RLHF System：
RLHF系统主要由三个关键组件组成：一个用于对齐的预训练语言模型（LM），从人类反馈中学习的奖励模型（RM），以及用于训练LM的强化学习（RL）算法。具体而言，预训练的LM通常是一个生成模型，其参数是使用现有的预训练LM参数进行初始化的。例如，OpenAI在其第一个流行的RLHF模型InstructGPT 中使用了1750亿参数的GPT-3，而DeepMind在其GopherCite模型中使用了2800亿参数的Gopher 。此外，奖励模型（RM）提供（学习到的）指导信号，反映了人类对LM生成的文本的偏好，通常以标量值的形式呈现。奖励模型可以采用两种形式：微调的LM或使用人类偏好数据进行全新训练的LM。现有的工作通常使用具有与对齐LM不同参数规模的奖励模型。例如，OpenAI分别使用6B GPT-3和DeepMind使用7B Gopher作为奖励模型。最后，为了利用奖励模型的信号优化预训练的LM，设计了特定的RL算法用于大规模模型调整。具体而言，Proximal Policy Optimization (PPO)是现有工作中广泛使用的RL算法，用于对齐。

Key Steps for RLHF：
1）Supervised fine-tuning：
为了使LM最初表现出所期望的行为，通常需要收集一个包含输入提示（指令）和期望输出的监督数据集，用于对LM进行微调。这些提示和输出可以由人类标注者为某些特定任务编写，同时确保任务的多样性。例如，InstructGPT 要求人类标注者撰写提示（例如，“列出五种恢复对职业的热情的方法”）和几个生成任务的期望输出，例如开放式问答、头脑风暴、聊天和改写。请注意，在特定设置或场景中，第一步是可选的；
【先做自然语言下任务指令的微调，类似于Instruction tuning，以使LLM具备理解人类自然语言的能力，由此得到的模型称为SFT；】

2）Reward model training：
第二步是使用人类反馈数据来训练奖励模型（RM）；具体而言，我们使用LM生成一定数量的输出文本，将采样的提示（来自监督数据集或人类生成的提示）作为输入。然后，我们邀请人类标注者对这些文本对进行偏好标注。标注过程可以采用多种形式，常见的方法是通过对生成的候选文本进行排名来进行标注，这可以减少标注者之间的不一致性。然后，RM被训练以预测人类偏好的输出。在InstructGPT中，标注者将模型生成的输出从最好到最差进行排名，而RM（即6B GPT-3）被训练以预测排名。请注意，在最近的工作中，对响应对的偏好标注是由一个AI代理（通常是对齐的LLM）而不是人类进行的，这被称为“从AI反馈中进行强化学习（RLAIF）”；
【采用LLM训练一个RM模型，以预测人类偏好分数；一般而言RM模型不宜过大，否则可能会训练不稳定，出现过拟合现象；】

3）RL fine-tuning：
在这一步中，将LM的对齐（即微调）形式化为强化学习问题。在这个设置中，预训练的LM充当策略，接受一个提示作为输入，并返回一个输出文本，其动作空间是词汇表，状态是当前生成的令牌序列，奖励由RM提供。为了避免显著偏离初始（调整之前）的LM，通常在奖励函数中加入惩罚项。例如，InstructGPT使用PPO算法优化LM与RM之间的关系。对于每个输入提示，InstructGPT计算当前LM生成结果与初始LM之间的KL散度作为惩罚项。值得注意的是，第二步和最后一步可以进行多次迭代，以更好地对齐LLM。由于强化学习算法的不稳定性，最近的工作通过重复使用具有较高奖励的最佳排名样本，将强化学习调整替换为另一种监督微调方法。
【由于强化学习算法的不稳定性且对超参数敏感，最近的工作通过重复使用具有较高奖励的最佳排名样本，将强化学习调整替换为另一种监督微调方法；】

	Practical Strategies for RLHF：
1）Effective reward model training：
尽管InstructGPT使用了一个较小的奖励模型（6B GPT模型），但越来越多的研究[90]表明，使用一个较大的奖励模型（例如与原始模型大小相等或更大）通常更有效，因为较大的奖励模型在判断LLM生成输出的质量方面通常表现更好。在LLaMa 2 中，预训练的聊天模型检查点用于初始化奖励模型，作者认为这种方法可以通过共享相同的预训练知识有效减少待对齐模型与奖励模型之间的信息不匹配。然而，训练大规模奖励模型时常常会遇到过拟合问题。作为一种简单而有效的解决方案，现有的工作引入了来自人工标注的对齐数据集中输入提示的优选回复的LM损失作为正则化器，从而缓解了奖励模型在二分类任务上的过拟合问题。此外，由于对齐存在多个标准（例如有用性和诚实性），往往很难训练一个能够满足所有对齐标准的单一奖励模型。因此，有用的做法是训练多个关注不同对齐标准的奖励模型，并通过特殊的组合策略（例如均值池化和加权求和）基于它们产生的奖励计算最终奖励。这种方式可以在多个标准上实现更灵活的规则或标准，例如在有用性要求上放宽同时对有害性设置更严格的限制。
【使用更大的RM效果会更好，同时对齐SFT模型和RM模型的基模型很重要，可以减少信息不匹配性，但更大的RM模型同样存在过拟合风险；】
【由于对齐多个标准（helpful、honesty、harmless）在一个单一RM中比较难训练，为此可以训练多个关注不同对齐标准的RM模型，通过组合策略来生成最终奖励分数；同时这样在后续利用上也更加的灵活】

2）Effective RL training：
由于RL训练过程往往不稳定且对超参数敏感，建议在RL训练之前对语言模型进行良好的有监督微调，以达到良好的模型能力。一种常用的方法是使用对齐数据集中的提示，选择最优的output进行微调，直到收敛为止（在这个收敛模型的基础上进行后续RL）。
给定一个提示，LLM首先通过采样算法生成N个输出，然后由奖励模型选择最佳候选者进行学习。在将LLM在最佳样本上微调至收敛后，将进行RL过程以进一步提高性能。LLaMA 2 已经成功训练了五个版本的RLHF模型，其中LLM随着奖励模型的改进而逐步提升。通过这种方式，人类偏好数据的收集提示和注释可以更好地反映当前模型检查点的问题，从而进行特殊调整以解决这些问题。此外，LLaMA 2还将先前迭代的样本添加到后续迭代中，以缓解迭代优化过程中可能出现的容量回归问题
【RL训练存在不稳定，且超参数敏感问题，为此可以在SFT模型上，对最优Alignment输出数据进行微调，提升RL训练的稳定性】
【训练多个版本的RLHF（针对不同的标准），可以更好进行后续的迭代优化，及问题点的风险和解决】

3）Efficient RL training：
由于强化学习训练要求迭代LLM和奖励模型的推理过程，这将极大增加总体内存和计算成本，特别是对于较大的奖励模型和LLM而言。作为一个实用的技巧，我们可以将奖励模型部署在一个独立的服务器上，并调用相应的API与LLM在其自己的服务器上进行交互。此外，由于RLHF要求LLM生成多个候选输出，而不是多次调用样本解码过程，更高效的做法是利用beam search decoding。它只需要进行一次解码来生成响应，同时这种策略还可以增加生成的候选响应的多样性。
【为了提升RL训练效率，每次训练RL时，可以对RM采用部署调用方式，来降低训练服务器计算成本和内存成本。】
【由于RLHF需要生成多个候选输出，在decoding时候，可以采用beam search decoding方法，提升效率的同时，也可以提升生成候选的多样性；】


Alignment without RLHF：
尽管RLHF在将LLM的行为与人类价值和偏好对齐方面取得了巨大成功，但它也存在明显的局限性。首先，RLHF需要同时训练多个语言模型，包括SFT模型、RM模型和RL模型，这在算法过程中很繁琐，并且在实践中消耗大量内存。此外，RLHF中常用的PPO算法相当复杂，对超参数也很敏感。作为一种替代方法，越来越多的研究探索直接优化LLM以符合人类偏好，使用监督微调而不是强化学习。

Overview：
非强化学习对齐方法的基本思想是通过在高质量对齐数据集上进行监督学习，直接对LLM进行微调。它基本上假设响应反馈或避免不安全行为的黄金规则已经被注入或包含在经过特别策划的对齐数据集中，以便LLM可以通过适当的微调策略直接从这些示范数据中学习对齐行为。因此，要实施这种方法，两个关键问题是对齐数据集的构建和微调损失的设计。对于第一个问题，可以通过一个对齐的LLM根据人类编写的安全原则自动构建对齐数据集，或者使用编辑操作改进现有示例。此外，还可以重用现有的奖励模型从现有的人类反馈数据中选择评分较高的响应。对于第二个问题，非强化学习对齐方法主要通过在高质量对齐数据集上以监督学习的方式（与原始指令微调损失相同）对LLM进行微调，同时可以使用辅助学习目标来增强对齐性能，例如对响应进行排序或对比指令-响应对；
【核心问题在于1）如何构建监督学习对齐数据集；2）如何设计微调损失函数；】

Alignment Data Collection：
对齐数据的构建对于有效地将LLM的行为与人类偏好对齐非常重要。为了收集高质量的对齐数据，一些研究尝试重用现有的奖励模型来选择评分较高的响应，而其他研究则探索利用强大的LLM（例如ChatGPT）或构建一个模拟环境来生成合成的对齐示例。接下来，我们将讨论这三个研究方向。
1）Reward model based approaches：
RLHF中的奖励模型已经训练用于衡量LLM的响应的对齐程度。利用现有的奖励模型选择高质量的响应作为后续微调的对齐数据是直接的做法。基于这个思想，RAFT 采用在人类偏好数据上训练的奖励模型对LLM的响应进行排名，并收集那些具有较高奖励的响应进行监督微调。此外，奖励模型还可以用于评分模型的响应并将其分配到不同的质量组中。根据奖励分数将LLM的响应分为不同的分位数。每个分位数都附有一个特殊的奖励标记，表示该分位数的奖励水平。基于最高奖励标记，随后会提示LLM生成高质量的响应。作为对齐LLM的有价值资源，已经发布了几个奖励模型，包括来自OpenAssistant的DeBERTa-base/large/xxlarge，来自Fudan的Moss-7B，以及来自Stanford的Flan-T5-xl。
【利用现有的RM模型选择高质量的响应作为后续微调的对齐数据；】

2）LLM based generative approaches：
RM模型有助于从模型的响应中选择对齐的数据。然而，训练RM模型本身需要大量高质量的人工标注数据，这通常昂贵且供应不足。此外，尽管现有的RM模型可以被重用，但它们可能无法准确捕捉另一个单独训练的LLM中的非对齐行为。因此，一些研究探索利用强大的LLM自动生成与人类对齐的数据。作为代表性的工作，Constitutional AI 提出，人类监督来自一组规定AI行为的原则（即自然语言指令）。基于这些原则，LLM将批评其自己有害的响应，并反复修订它们，直到最终得到对齐的响应。类似地，Self-Align 首先采用自我指导生成侧重于涵盖多样主题的指令。然后，模型还会提示多个人工编写的原则，描述了预期模型行为的规则（还包括一些上下文示例），以生成有帮助、符合道德和可靠的响应作为对齐数据。
【利用LLM自动生成与人类对齐的数据？？听着好像可以，但要是能这样做了，为什么不直接固定呢？】

	3）LLM based interactive approaches：
	大多数现有方法在孤立环境中训练LLM，LLM并不在实际环境中通过外部反馈信号来改进自己。相比之下，人类通过与社会环境中的他人互动来学习社会规范和价值观。为了模仿这种学习方	法，稳定对齐构建了一个模拟的互动环境，其中包含多个LLM代理，AI代理相互交互，并接收改进的反馈信号。一旦中央代理接收到指令，它会生成一个响应并与附近的代理共享。这些批		评代理生成包含对响应的评分和修订建议的反馈。然后，中央代理会根据这些建议修订原始响应。这种对齐方法也可以扩展到与人类一起在真实环境中进行。
【还是这个问题，这样生成的数据靠谱吗？】

Supervised Alignment Tuning：
	在获得对齐数据之后，设计适当的微调策略以进行直接对齐也是关键。一种直接的方法是使用基于对齐数据的sequence-to-sequence目标来优化LLM。除了传统的优化目标外，还有几项研究进一步探索了增强从对齐数据中学习的辅助损失。

1）Primary training objective：
由于对齐数据通常包含输入指令和输出响应，主要的训练损失仍然是传统的sequence-to-sequence学习的cross-entropy-loss；基于这个损失，许多研究提出了一些改进的变体来增强监督对齐调优。例如，CoH 通过在注释的好和坏响应之前分别添加“一个有帮助的答案:”和“一个无用的答案:”来构建训练数据，并且只计算具有特殊掩码的响应标记的损失。Quark 将模型的响应按照不同的对齐质量排序，它在每个模型的响应前面添加一个特殊的奖励标记，表示响应的奖励水平。此外，为了通过最大似然目标实现偏好建模，DPO 首先使用策略模型（即被优化的语言模型）对响应奖励进行重新参数化，然后原始的奖励建模目标可以仅基于策略模型重新制定。通过这种方式，DPO消除了显式的奖励建模步骤，并且仅优化涉及策略模型的新学习目标等效于优化奖励；
【主要loss依然是sequence-to-sequence学习的cross-entropy-loss，其他的。。。欧米】

2）Auxiliary optimization objectives：
除了主要的交叉熵损失外，还有几项研究提出了辅助训练损失来增强从对齐数据中的学习。首先，由于每个指令的响应可以由RM模型评分，因此可以使用排序损失来训练模型以保持这些响应的排序顺序。例如，研究从多个来源中对响应进行抽样，包括模型生成的响应（例如来自模型本身、ChatGPT和GPT-4的响应）以及人工编写的响应，涵盖了高质量和低质量的实例。为了与RM模型的分数对齐，它进一步通过RM模型对具有较高排名的响应具有较高的条件对数概率来优化排序损失。其次，为了增强响应和指令之间的相关性，一些工作采用对比学习来提高正确指令-响应对的概率，同时降低不正确的指令-响应对的概率。特别地，对于一个输出响应，在中提出的方法将目标指令与其他不相关的指令进行对比。通过这样做，它可以使模型学习到指令和响应之间的正确关联。
【通过对比学习的方式，来增加正确的instruction-respose的概率，降低不正确&无关的instruction-respose的概率；】


Remarks on SFT and RLHF：
指令调整是使用格式化的演示数据（指令与期望输出配对）对预训练语言模型进行训练的过程；在早期的探索中，指令数据主要来自于自然语言处理任务，而现在已经扩展到更多种类的监督数据，包括输入和输出文本的配对（例如，开放式对话的话语）；在LLM（语言模型）的上下文中，使用这种配对文本进行训练也被称为有监督微调（SFT）；在本部分中，我们主要使用SFT作为讨论的缩写，而不是指令调整，这是因为它简单且常用。由于SFT和RLHF是LLM的两种主要适应调整方法，了解它们之间的联系和区别非常重要。
【简而言之Instruction tuning指的是在传统自然语言任务上，构造指令-示例数据，微调得到的LLM；而SFT则是在开放式对话场景下，通过构造对应的指令-示例数据，微调得到的LLM；】

Overall Comparison with RL Formulation：
Following the discussion in Section 5.2.3 (the part related to RL training), the text generation problem can be formulated as a decisionmaking process based on RL. Taking a prompt as input, the task of a LLM is to generate a text completion that appropriately responds to the prompt. This task would be completed step by step. At each step, an agent (i.e., LLM) will perform an action (i.e., generating a token) according to the policy (i.e., the generative probability distribution of LLM) conditioned on the current state (currently generated token sequence and other available context information). It is expected that a high-quality output text would be produced by the LLM, which can earn a large reward score based on the entire response. Overall, RLHF and SFT can be considered as two different training approaches to optimizing the above decision making process for LLMs. Specially, RLHF firstly learns the reward model, and then employs it to improve the LLM with RL training (e.g., PPO). As a comparison, SFT adopts a teacher-forcing approach, which directly optimizes the likelihood of a demonstration output. Such a token-level training way essentially does behavior cloning (a special algorithm of imitation learning [310]): it utilizes the expert’s action (i.e., the target token at each step) as the supervision label and directly learns to imitate the demonstrations from experts without specifying a reward model as in typical RL algorithms. To learn the desired policies, SFT adopts a “local” optimization way (i.e., tokenlevel loss) based on demonstration data, while RLHF takes a “global” optimization way (i.e., text-level loss) by involving human preference. More theoretical analysis about imitation learning and reinforcement learning can be referred to the related RL literature [310, 311]。
【上面是论文中的原文，大致意思是基于RL的RLHF，先学习一个RM模型，以此作为RL的reward，基于PPO算法训练LLM；而SFT则更像是一种teacher指导形式的学习，本质是一个token维度的学习，来直接学习teacher专家的指令数据；所以上这里可以看出，【SFT是一个token level的局部优化（step by step 解码算法），而RLHF是一个text-level全局的优化（step by step 下基于reward指导）；】

Pros and Cons of SFT：
SFT has been shown to be an effective approach to boosting the performance of LLMs on various benchmarks [62, 64, 123, 124], which can largely enhance the task generalization ability and flexibly endow specific functions (e.g., establishing the chatbot’s identity). More discussions about the usefulness of SFT can be found in Section 5.1.3. It has been widely recognized that SFT mainly unlocks the abilities but not inject new abilities into LLMs. Thus, it might become problematic when one tries to stimulate the non-endogenous abilities of LLMs via SFT. As a concrete scenario, it would potentially advocate the hallucination behaviors when demonstration data is beyond the knowledge or ability scope of LLMs, e.g., training a LLM to answer questions about its unknown facts. An interesting viewpoint from John Schulman’s talk on RLHF [312] is that distilling superior models to train less capable models (e.g., prompting GPT-4 to generate the response as fine-tuning data) might increase the possibilities of generating the hallucinated texts, thus likely affecting the factual accuracy of LLMs. Furthermore, as a behavior cloning method, SFT aims to imitate the behaviors (without explorations) of the experts who construct the demonstration data. However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT. Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage [90].
	
	翻译一下大概如下意思：SFT已被证明是提升LLM在各种基准测试中性能的有效方法，它可以大大增强任务的泛化能力，并灵活赋予特定功能（例如，建立聊天机器人的身份）；人们普遍认为，SFT主要是释放LLM的能力，而不是给LLM注入新的能力。因此，当通过SFT来激发LLM的非内生能力时，可能会出现问题。作为一个具体的场景，当演示数据超出LLM的知识或能力范围时，例如训练LLM回答关于其未知事实的问题，SFT可能会倡导产生幻觉行为。关于RLHF的John Schulman的演讲提出了一个有趣的观点，即提炼优秀模型来训练能力较弱的模型（例如，通过微调数据来引导GPT-4生成响应）可能会增加生成幻觉文本的可能性，从而可能影响LLM的事实准确性。此外，作为一种行为克隆方法，SFT旨在模仿构建演示数据的专家的行为（不进行探索）。同时不同注释者在演示数据的写作风格、质量和偏好方面往往存在差异，这往往会影响SFT的学习性能。因此，在SFT阶段，高质量的指导数据（而不是数量）是有效训练LLM的主要因素。
总结一下大概这么几个观点：
【SFT并不给LLM注入新的能力，只是通过指令微调让LLM的能力展现出来；那么当我们通过SFT希望让LLM具备非内生能力的时候（超出LLM知识范围），有可能会产生幻觉问题，从而影响LLM honesty的准确性；同时SFT的监督学习，就是在模仿teacher 专家数据，但这个数据在不同的标注者下面可能由于个体偏好、风格等存在一定的差异，从而影响SFT的性能；为此在SFT阶段，高质量的指令数据（而非数量），是训练LLM的关键；】
 Pros and Cons of RLHF：
RLHF was early explored in the literature of deep RL [70], then borrowed to improve the capacity of language models (e.g., summarization [116]), and subsequently adopted as the fundamental technique to develop InstructGPT [61]. Recently, increasing evidence [90, 298] has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity. Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores [90], and attributed this to a better human-LLM synergy for data annotation. They explain this reason in two major aspects as follows. First, since human annotators mainly provide preference annotations for RLHF, it can largely alleviate the discrepancies of annotators as that in SFT. Secondly, preference annotation is much easier than writing the demonstration data, and annotators can even judge the quality of more superior generations than those they create, making it possible to explore a broader state space beyond what can be demonstrated by human annotators. Another key point is that RLHF essentially encourages LLMs to learn correct policies by contrasting the self-generated responses (discriminating between good and bad responses). It no longer forces the model to imitate external demonstration data, and thus can mitigate the hallucination issues with SFT as discussed above36. Actually, RLHF has been demonstrated to be an important approach to reduce the hallucination behaviors in GPT-4 [46]. However, RLHF inherits the drawbacks of classic RL algorithms, e.g., sample inefficiency and training instability. When adapted to LLMs, RLHF further relies on a strong SFT model as initial model checkpoint for efficiently achieving good performance. In addition, human annotators are involved in a complex iterative optimization process, in which a number of important details (e.g., the prompt selection, the schedule of reward model training and PPO training, and the settings of hyper-parameters) have important impact on the whole model performance.

	翻译如下大致如下：RLHF有助于减轻harmful，提升helpful，提升模型性能；其中LLaMA将此解释为：
	1）标注样本采用偏好排序的方式，这样可以减少由于标注者偏好、风格等原来带来的偏差；
	2）采用偏好排序标注，标注者可以对那些生成更高质量的文本进行判断，使得探索更多的状态空间存在可能。

	另一方面，RLHF通过对生成文本进行对比，学习正确的生成策略，而不是模范标注数据，这样可以减少SFT那样的幻觉问题；实际上在GPT-4中，RLHF是减少幻觉问题关键方法；
同样的RLHF同样具备RL的确定，例如训练不稳定，受参数初始化影响，为此RLHF需要一个性能较好的初始化SFT模型；RLHF 另一个缺点是需要一个复杂的迭代过程，包括prompt指令选择，RM模型训练，PPO训练，超参数选择等，这些都对训练结果的性能有影响。
【RLHF已经被证明有助于提升helpful、harmless、honesty，对减少幻觉有明显的帮助；但同样RLHF训练难度大，整个过程包括prompt指令选择，RM模型训练，PPO训练，超参数选择等，其中RL部分还存在训练不稳定、受参数初始化影响等；为此先进行SFT，获得一个较好的初始化模型，是非常重要的。】 【RLHF学习的是如何正确的生成文本，而SFT是监督学习，是一种对训练数据中专家进行学习，为此没有办法去探索标注数据之外的更高质量的文本生成，也更容易产生幻觉；】

Overall：
Overall, SFT is particularly useful to increase the model capacity of pre-trained model checkpoints right after pretraining, while RLHF is promising to further improve the model capacity of SFT models. However, RLHF has been difficult to implement, and far from well explored (according to public literature), and more improvements (e.g., efficient and reliable annotation [298] and simplified optimization [307]) are still needed for further research.
【总体而言，SFT可以看成是对预训练LLM性能优化，而RLHF则是对SFT的进一步性能优化；对应的RLHF训练难度明显高于SFT；】  




