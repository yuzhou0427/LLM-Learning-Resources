# Prompt经验


<h4 align="left">
    <p>
        <b>业界数据集及处理方式总结参考</b> :
        <a href="9.%D2%B5%BD%E7%CA%FD%BE%DD%BC%AF%BC%B0%B4%A6%C0%ED%B7%BD%CA%BD%D7%DC%BD%E1.md">MD</a>
    <p>
</h4>


## 目录

- [训练数据的prompt](#训练数据的prompt)
- [prompt调优经验](#prompt调优经验)

<br>

## 训练数据的Prompt:

|开源模型| 训练数据输入格式 |训练数据Prompt构建代码|备注|
|-|---------|--|--|
|chatglm-6b和chatglm2-6b|多轮对话输入demo： <br> [chatglm-prompt-demo.json](prompt-demo%2Fchatglm-prompt-demo.json)|[chatglm-prompt-code.py](prompt-demo%2Fchatglm-prompt-code.py)|训练代码中会传入prefix, 可以是区分不同数据集的学习任务目标。|
|Belle|对话数据输入 demo： <br> [belle-prompt-demo.json](prompt-demo%2Fbelle-prompt-demo.json)|[belle-prompt-code.py](prompt-demo%2Fbelle-prompt-code.py)||


<br>

## Prompt调优经验:

**单轮对话:**
1. 约束条件:
   - **在开头给出机器人的角色是一个很好的写法**，其作用在于给机器人的整个回复建立一个“基调”，使模型能够在遇到约束外问题时，尽可能的正确回答。
   - **引号是一种很强的指示**，“如果对方不是店长，则回复抱歉”和“如果对方不是店长，则回复‘抱歉’”，前者模型会以抱歉为核心内容自由发挥，后者大概率只会回复“抱歉”两个字。



2. *知识库解决对话中遇到的bad case:*
   - **<font color="#dd0000">知识库的内容可以根据测试bad case进行更新。有些bad case通过Prompt进行解决，只能起到减少出现的概率问题，而通过知识库去解决，这能够极大减少case出现的情况 。覆盖大模型在垂类知识的盲区，增加垂类场景的特殊知识，通过当前query进行检索触发。</font>**

**多轮对话：**

&nbsp;&nbsp;&nbsp;&nbsp;**1）采用message形式**，整体输入是一个list，其元素是字典类型可以将指令Prompt分为多个部分，**通用指令，历史对话，以及动态指令和当前输入query**。
- 通用的指令可以放入一个固定的system message消息里，比如任务定义，角色定义，固定约束和流程等；
- 历史对话内容则需要通过构造AI message和user message来存储，按照对话顺序保存；
- 动态指令主要是指需要根据当前query检索相应的参考知识放入system message中，以提高大模型回答的准确性和合理性；
- 当前输入query主要是用户的提问，需要放入user message里存储。


    message构造demo：[gpt-message-code.py](prompt-demo%2Fgpt-message-code.py)


&nbsp;&nbsp;&nbsp;&nbsp;**2）固定输出格式**，**比如json格式可以将当前query的输入content也改成json格式**，这样在AI message的输出content中，也会输出相应json格式内容。

&nbsp;&nbsp;&nbsp;&nbsp;**3）动态指令** 的设计如果担心对话会遗忘掉通用指令，可以在**动态指令中再次简单提及通用指令中的关键名词**，来确保模型的指令遵从性。**尽量不要对对话输出内容做明确的字数限制**，会导致句子生硬的截断，语义不通顺。

&nbsp;&nbsp;&nbsp;&nbsp;**4）“不吐不快”bug**：对话历史格式对话历史的格式写成“ assistant：xxx user：xxx assistant：”是一个很糟糕的格式，很容易诱发模型续写多个回合，也就是模型会替用户答题。

    一种解决方法是使用openai推荐的message格式，另一个种缓解（但不根治）的方法是不用“assistant：”来作为结尾，而是用“作为一名xxx，你应该说”来提示模型进行输出。


&nbsp;&nbsp;&nbsp;&nbsp;**5）多轮次引导**：延迟输出特定情况下，模型虽然掌握了所有的信息，但是需要它延迟输出，如果不做限制，模型会通过猜测用户回答的方式把内容一次性说完，比如“您好，请问您是xxx的店长吗？如果是的话，这里有两个活动，一个是xxx，一个是xxx，如果您感兴趣的话，感谢您的接听，再见”，其实也是一种“不吐不快”bug的表现。延迟输出的场景有：a，电销中招呼语和活动内容的分离，一般打招呼时不适合把活动内容也一并介绍；b，酒旅助手，应当每次问一个小问题来补全整个需求，比如“您要做火车、飞机还是汽车”“您的出发地是哪里”“您的目的地是哪里”。从目前的情况来看，模型已经习得了酒旅助手的能力，所以即使不加约束，也能分段提问，但是对于自定义的问题，比如“您要订橡果优选还是普通民宿”就很难分段延迟输出。延迟输出的基本写法是“条件，动作”模式，比如“会话开始时，询问是否是店长”，“如果对方是店长，则介绍活动”，如果想要强化该模式，可以使用状态机的写法，即“会话开始时，询问是否是店长，并将状态置为‘询问’”，“如果状态为‘询问’，且对方是店长，则介绍活动，并将状态更新为‘介绍活动’”。状态机的写法来自于用chatgpt来完成一个文字游戏的灵感。

&nbsp;&nbsp;&nbsp;&nbsp;**6）跷跷板问题**：约束与约束泄露大模型的幻觉现象一般需要增加约束来减少，但增加约束后又会出现跷跷板现象。比如大模型幻觉说“我可以帮你修改差评”，如果在约束中加入“你没有修改差评的能力”，则大模型在回答毫不相关的问题时，也会突然说“我没有修改差评的能力”。如果可以话，尽量减少约束的加入，或者依赖知识库只在必要的时候做动态插入。

&nbsp;&nbsp;&nbsp;&nbsp;**7）大模型的局限**：伪状态机当前大模型并不真正具备逻辑推理能力，和状态机写法结合时，其缺陷表现为：a，无法从后面的状态跳回前面的状态；b，如果对话超出了状态机的条件设置，则会自己编造新的状态。






